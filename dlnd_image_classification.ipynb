{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2935ace80>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 6:\n",
      "Image - Min Value: 7 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2e538a5c0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 6\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 4 Max Value: 231\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTNADQGGCHgAKE/QAUNh/A+J+ZwfDyI5sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd28fc5cf28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 20:\n",
      "Image - Min Value: 26 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV+l1VAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd28fc5fbe0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 20\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are all possible labels? numbers between 0 and 9, \n",
    "* what is the range of values for the image data? 0-255, \n",
    "* Are the labels in order or random? Random.\n",
    "* Classes seem balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    normalized_data = (x-np.min(x))/np.ptp(x)#(x - min(x)) / (max(x)-min(x))\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    from sklearn import preprocessing\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    \n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.54901961,  0.49019608,  0.45098039],\n",
       "         [ 0.57254902,  0.50980392,  0.47843137],\n",
       "         [ 0.56078431,  0.49803922,  0.47843137],\n",
       "         ..., \n",
       "         [ 0.66666667,  0.56862745,  0.51372549],\n",
       "         [ 0.69019608,  0.58823529,  0.5254902 ],\n",
       "         [ 0.66666667,  0.57647059,  0.52156863]],\n",
       "\n",
       "        [[ 0.4745098 ,  0.42352941,  0.50588235],\n",
       "         [ 0.50980392,  0.4627451 ,  0.54509804],\n",
       "         [ 0.5254902 ,  0.4745098 ,  0.56078431],\n",
       "         ..., \n",
       "         [ 0.63921569,  0.55294118,  0.61568627],\n",
       "         [ 0.66666667,  0.57254902,  0.63137255],\n",
       "         [ 0.66666667,  0.58039216,  0.63137255]],\n",
       "\n",
       "        [[ 0.59607843,  0.54509804,  0.68235294],\n",
       "         [ 0.61568627,  0.56862745,  0.70196078],\n",
       "         [ 0.60784314,  0.56078431,  0.68627451],\n",
       "         ..., \n",
       "         [ 0.69411765,  0.60392157,  0.75686275],\n",
       "         [ 0.70980392,  0.61176471,  0.76078431],\n",
       "         [ 0.71764706,  0.62745098,  0.76078431]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.49019608,  0.43137255,  0.4       ],\n",
       "         [ 0.50588235,  0.43921569,  0.40392157],\n",
       "         [ 0.29803922,  0.2627451 ,  0.18431373],\n",
       "         ..., \n",
       "         [ 0.65882353,  0.5372549 ,  0.47058824],\n",
       "         [ 0.61960784,  0.49411765,  0.40392157],\n",
       "         [ 0.57254902,  0.45490196,  0.34117647]],\n",
       "\n",
       "        [[ 0.33333333,  0.30196078,  0.2745098 ],\n",
       "         [ 0.36862745,  0.31764706,  0.27843137],\n",
       "         [ 0.29019608,  0.25490196,  0.17647059],\n",
       "         ..., \n",
       "         [ 0.63529412,  0.51764706,  0.41568627],\n",
       "         [ 0.65098039,  0.5254902 ,  0.39215686],\n",
       "         [ 0.61960784,  0.50196078,  0.36078431]],\n",
       "\n",
       "        [[ 0.49019608,  0.43921569,  0.43529412],\n",
       "         [ 0.50980392,  0.44313725,  0.43529412],\n",
       "         [ 0.41176471,  0.35686275,  0.29411765],\n",
       "         ..., \n",
       "         [ 0.51764706,  0.41568627,  0.30588235],\n",
       "         [ 0.50980392,  0.39607843,  0.25098039],\n",
       "         [ 0.55686275,  0.45098039,  0.30588235]]],\n",
       "\n",
       "\n",
       "       [[[ 0.39215686,  0.42745098,  0.32941176],\n",
       "         [ 0.47843137,  0.49411765,  0.42745098],\n",
       "         [ 0.34117647,  0.34117647,  0.29803922],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.30588235,  0.27058824],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039]],\n",
       "\n",
       "        [[ 0.3372549 ,  0.38823529,  0.27843137],\n",
       "         [ 0.29803922,  0.32941176,  0.25882353],\n",
       "         [ 0.23529412,  0.25098039,  0.21176471],\n",
       "         ..., \n",
       "         [ 0.30588235,  0.31764706,  0.28235294],\n",
       "         [ 0.29803922,  0.30980392,  0.2745098 ],\n",
       "         [ 0.32156863,  0.33333333,  0.29803922]],\n",
       "\n",
       "        [[ 0.32941176,  0.39215686,  0.28627451],\n",
       "         [ 0.3254902 ,  0.37254902,  0.29411765],\n",
       "         [ 0.30196078,  0.3372549 ,  0.28235294],\n",
       "         ..., \n",
       "         [ 0.29019608,  0.30196078,  0.26666667],\n",
       "         [ 0.28627451,  0.29803922,  0.2627451 ],\n",
       "         [ 0.3254902 ,  0.3372549 ,  0.30196078]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.25098039,  0.30196078,  0.30980392],\n",
       "         [ 0.47843137,  0.52156863,  0.56470588],\n",
       "         [ 0.5254902 ,  0.56862745,  0.61176471],\n",
       "         ..., \n",
       "         [ 0.41176471,  0.48235294,  0.47058824],\n",
       "         [ 0.32941176,  0.40392157,  0.35686275],\n",
       "         [ 0.23529412,  0.34509804,  0.24705882]],\n",
       "\n",
       "        [[ 0.17254902,  0.2       ,  0.21960784],\n",
       "         [ 0.30588235,  0.32941176,  0.36862745],\n",
       "         [ 0.37647059,  0.39607843,  0.43137255],\n",
       "         ..., \n",
       "         [ 0.57647059,  0.64705882,  0.69803922],\n",
       "         [ 0.49411765,  0.56078431,  0.58431373],\n",
       "         [ 0.36862745,  0.45882353,  0.44313725]],\n",
       "\n",
       "        [[ 0.14117647,  0.1372549 ,  0.15294118],\n",
       "         [ 0.23137255,  0.22745098,  0.25882353],\n",
       "         [ 0.32156863,  0.31764706,  0.33333333],\n",
       "         ..., \n",
       "         [ 0.5254902 ,  0.6       ,  0.62745098],\n",
       "         [ 0.54117647,  0.59607843,  0.61960784],\n",
       "         [ 0.50980392,  0.58039216,  0.58823529]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0745098 ,  0.1254902 ,  0.05882353],\n",
       "         [ 0.08235294,  0.14901961,  0.08235294],\n",
       "         [ 0.10588235,  0.19215686,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.48627451,  0.51372549],\n",
       "         [ 0.29803922,  0.48627451,  0.50980392],\n",
       "         [ 0.27843137,  0.4627451 ,  0.48627451]],\n",
       "\n",
       "        [[ 0.09019608,  0.12156863,  0.05490196],\n",
       "         [ 0.08235294,  0.11764706,  0.04705882],\n",
       "         [ 0.09019608,  0.1372549 ,  0.05490196],\n",
       "         ..., \n",
       "         [ 0.28235294,  0.4627451 ,  0.49411765],\n",
       "         [ 0.29411765,  0.46666667,  0.48627451],\n",
       "         [ 0.26666667,  0.43529412,  0.44705882]],\n",
       "\n",
       "        [[ 0.09411765,  0.14509804,  0.06666667],\n",
       "         [ 0.08627451,  0.1372549 ,  0.0627451 ],\n",
       "         [ 0.09411765,  0.14117647,  0.07058824],\n",
       "         ..., \n",
       "         [ 0.25098039,  0.42745098,  0.43921569],\n",
       "         [ 0.2627451 ,  0.42745098,  0.43529412],\n",
       "         [ 0.25098039,  0.41176471,  0.41176471]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.24313725,  0.18039216,  0.09019608],\n",
       "         [ 0.23529412,  0.18039216,  0.10588235],\n",
       "         [ 0.21568627,  0.18823529,  0.10980392],\n",
       "         ..., \n",
       "         [ 0.05098039,  0.02352941,  0.01568627],\n",
       "         [ 0.04705882,  0.05490196,  0.03137255],\n",
       "         [ 0.09803922,  0.15686275,  0.11764706]],\n",
       "\n",
       "        [[ 0.24705882,  0.20784314,  0.11764706],\n",
       "         [ 0.19215686,  0.17647059,  0.08627451],\n",
       "         [ 0.17647059,  0.18039216,  0.09019608],\n",
       "         ..., \n",
       "         [ 0.11372549,  0.1372549 ,  0.12156863],\n",
       "         [ 0.11764706,  0.16470588,  0.14509804],\n",
       "         [ 0.10588235,  0.19607843,  0.16862745]],\n",
       "\n",
       "        [[ 0.27058824,  0.20392157,  0.11372549],\n",
       "         [ 0.19215686,  0.14901961,  0.07843137],\n",
       "         [ 0.21176471,  0.18039216,  0.10588235],\n",
       "         ..., \n",
       "         [ 0.25882353,  0.34509804,  0.33333333],\n",
       "         [ 0.15686275,  0.26666667,  0.25098039],\n",
       "         [ 0.11372549,  0.24313725,  0.22745098]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.1372549 ,  0.69803922,  0.92156863],\n",
       "         [ 0.15686275,  0.69019608,  0.9372549 ],\n",
       "         [ 0.16470588,  0.69019608,  0.94509804],\n",
       "         ..., \n",
       "         [ 0.38823529,  0.69411765,  0.85882353],\n",
       "         [ 0.30980392,  0.57647059,  0.77254902],\n",
       "         [ 0.34901961,  0.58039216,  0.74117647]],\n",
       "\n",
       "        [[ 0.22352941,  0.71372549,  0.91764706],\n",
       "         [ 0.17254902,  0.72156863,  0.98039216],\n",
       "         [ 0.19607843,  0.71764706,  0.94117647],\n",
       "         ..., \n",
       "         [ 0.61176471,  0.71372549,  0.78431373],\n",
       "         [ 0.55294118,  0.69411765,  0.80784314],\n",
       "         [ 0.45490196,  0.58431373,  0.68627451]],\n",
       "\n",
       "        [[ 0.38431373,  0.77254902,  0.92941176],\n",
       "         [ 0.25098039,  0.74117647,  0.98823529],\n",
       "         [ 0.27058824,  0.75294118,  0.96078431],\n",
       "         ..., \n",
       "         [ 0.7372549 ,  0.76470588,  0.80784314],\n",
       "         [ 0.46666667,  0.52941176,  0.57647059],\n",
       "         [ 0.23921569,  0.30980392,  0.35294118]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.28627451,  0.30980392,  0.30196078],\n",
       "         [ 0.20784314,  0.24705882,  0.26666667],\n",
       "         [ 0.21176471,  0.26666667,  0.31372549],\n",
       "         ..., \n",
       "         [ 0.06666667,  0.15686275,  0.25098039],\n",
       "         [ 0.08235294,  0.14117647,  0.2       ],\n",
       "         [ 0.12941176,  0.18823529,  0.19215686]],\n",
       "\n",
       "        [[ 0.23921569,  0.26666667,  0.29411765],\n",
       "         [ 0.21568627,  0.2745098 ,  0.3372549 ],\n",
       "         [ 0.22352941,  0.30980392,  0.40392157],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.18823529,  0.28235294],\n",
       "         [ 0.06666667,  0.1372549 ,  0.20784314],\n",
       "         [ 0.02745098,  0.09019608,  0.1254902 ]],\n",
       "\n",
       "        [[ 0.17254902,  0.21960784,  0.28627451],\n",
       "         [ 0.18039216,  0.25882353,  0.34509804],\n",
       "         [ 0.19215686,  0.30196078,  0.41176471],\n",
       "         ..., \n",
       "         [ 0.10588235,  0.20392157,  0.30196078],\n",
       "         [ 0.08235294,  0.16862745,  0.25882353],\n",
       "         [ 0.04705882,  0.12156863,  0.19607843]]],\n",
       "\n",
       "\n",
       "       [[[ 0.74117647,  0.82745098,  0.94117647],\n",
       "         [ 0.72941176,  0.81568627,  0.9254902 ],\n",
       "         [ 0.7254902 ,  0.81176471,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.76470588,  0.87843137],\n",
       "         [ 0.6745098 ,  0.76078431,  0.87058824],\n",
       "         [ 0.6627451 ,  0.76078431,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.76078431,  0.82352941,  0.9372549 ],\n",
       "         [ 0.74901961,  0.81176471,  0.9254902 ],\n",
       "         [ 0.74509804,  0.80784314,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.67843137,  0.75294118,  0.8627451 ],\n",
       "         [ 0.67058824,  0.74901961,  0.85490196],\n",
       "         [ 0.65490196,  0.74509804,  0.84705882]],\n",
       "\n",
       "        [[ 0.81568627,  0.85882353,  0.95686275],\n",
       "         [ 0.80392157,  0.84705882,  0.94117647],\n",
       "         [ 0.8       ,  0.84313725,  0.9372549 ],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.74901961,  0.85098039],\n",
       "         [ 0.6745098 ,  0.74509804,  0.84705882],\n",
       "         [ 0.6627451 ,  0.74901961,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.81176471,  0.78039216,  0.70980392],\n",
       "         [ 0.79607843,  0.76470588,  0.68627451],\n",
       "         [ 0.79607843,  0.76862745,  0.67843137],\n",
       "         ..., \n",
       "         [ 0.52941176,  0.51764706,  0.49803922],\n",
       "         [ 0.63529412,  0.61960784,  0.58823529],\n",
       "         [ 0.65882353,  0.63921569,  0.59215686]],\n",
       "\n",
       "        [[ 0.77647059,  0.74509804,  0.66666667],\n",
       "         [ 0.74117647,  0.70980392,  0.62352941],\n",
       "         [ 0.70588235,  0.6745098 ,  0.57647059],\n",
       "         ..., \n",
       "         [ 0.69803922,  0.67058824,  0.62745098],\n",
       "         [ 0.68627451,  0.6627451 ,  0.61176471],\n",
       "         [ 0.68627451,  0.6627451 ,  0.60392157]],\n",
       "\n",
       "        [[ 0.77647059,  0.74117647,  0.67843137],\n",
       "         [ 0.74117647,  0.70980392,  0.63529412],\n",
       "         [ 0.69803922,  0.66666667,  0.58431373],\n",
       "         ..., \n",
       "         [ 0.76470588,  0.72156863,  0.6627451 ],\n",
       "         [ 0.76862745,  0.74117647,  0.67058824],\n",
       "         [ 0.76470588,  0.74509804,  0.67058824]]],\n",
       "\n",
       "\n",
       "       [[[ 0.89803922,  0.89803922,  0.9372549 ],\n",
       "         [ 0.9254902 ,  0.92941176,  0.96862745],\n",
       "         [ 0.91764706,  0.9254902 ,  0.96862745],\n",
       "         ..., \n",
       "         [ 0.85098039,  0.85882353,  0.91372549],\n",
       "         [ 0.86666667,  0.8745098 ,  0.91764706],\n",
       "         [ 0.87058824,  0.8745098 ,  0.91372549]],\n",
       "\n",
       "        [[ 0.87058824,  0.86666667,  0.89803922],\n",
       "         [ 0.9372549 ,  0.9372549 ,  0.97647059],\n",
       "         [ 0.91372549,  0.91764706,  0.96470588],\n",
       "         ..., \n",
       "         [ 0.8745098 ,  0.8745098 ,  0.9254902 ],\n",
       "         [ 0.89019608,  0.89411765,  0.93333333],\n",
       "         [ 0.82352941,  0.82745098,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.83529412,  0.80784314,  0.82745098],\n",
       "         [ 0.91764706,  0.90980392,  0.9372549 ],\n",
       "         [ 0.90588235,  0.91372549,  0.95686275],\n",
       "         ..., \n",
       "         [ 0.8627451 ,  0.8627451 ,  0.90980392],\n",
       "         [ 0.8627451 ,  0.85882353,  0.90980392],\n",
       "         [ 0.79215686,  0.79607843,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.58823529,  0.56078431,  0.52941176],\n",
       "         [ 0.54901961,  0.52941176,  0.49803922],\n",
       "         [ 0.51764706,  0.49803922,  0.47058824],\n",
       "         ..., \n",
       "         [ 0.87843137,  0.87058824,  0.85490196],\n",
       "         [ 0.90196078,  0.89411765,  0.88235294],\n",
       "         [ 0.94509804,  0.94509804,  0.93333333]],\n",
       "\n",
       "        [[ 0.5372549 ,  0.51764706,  0.49411765],\n",
       "         [ 0.50980392,  0.49803922,  0.47058824],\n",
       "         [ 0.49019608,  0.4745098 ,  0.45098039],\n",
       "         ..., \n",
       "         [ 0.70980392,  0.70588235,  0.69803922],\n",
       "         [ 0.79215686,  0.78823529,  0.77647059],\n",
       "         [ 0.83137255,  0.82745098,  0.81176471]],\n",
       "\n",
       "        [[ 0.47843137,  0.46666667,  0.44705882],\n",
       "         [ 0.4627451 ,  0.45490196,  0.43137255],\n",
       "         [ 0.47058824,  0.45490196,  0.43529412],\n",
       "         ..., \n",
       "         [ 0.70196078,  0.69411765,  0.67843137],\n",
       "         [ 0.64313725,  0.64313725,  0.63529412],\n",
       "         [ 0.63921569,  0.63921569,  0.63137255]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview features\n",
    "valid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview labels\n",
    "valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = (None,image_shape[0],image_shape[1],image_shape[2]),name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,shape = (None,n_classes), name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    convolution_output_depth = conv_num_outputs\n",
    "    convolution_input_depth = x_tensor.get_shape().as_list()[3]\n",
    "    convolution_kernel_height = conv_ksize[0]\n",
    "    convolution_kernel_width = conv_ksize[1]\n",
    "    convolution_height_stride = conv_strides[0]\n",
    "    convolution_width_stride = conv_strides[1]\n",
    "    pool_kernel_height = pool_ksize[0]\n",
    "    pool_kernel_width = pool_ksize[1]\n",
    "    pool_height_stride = pool_strides[0]\n",
    "    pool_width_stride = pool_strides[1]\n",
    "\n",
    "    convolution_weights = tf.Variable(tf.truncated_normal([convolution_kernel_height,convolution_kernel_width,convolution_input_depth,convolution_output_depth],stddev=0.01))\n",
    "    convolution_bias = tf.Variable(tf.zeros([convolution_output_depth]))\n",
    "    convolution = tf.nn.conv2d(x_tensor,  convolution_weights,strides = [1,convolution_height_stride,convolution_width_stride,1],padding = \"SAME\")\n",
    "    convolution = tf.nn.bias_add(convolution,convolution_bias)\n",
    "    convolution = tf.nn.relu(convolution)\n",
    "    max_pool = tf.nn.max_pool(convolution,ksize=[1,pool_kernel_height,pool_kernel_width,1],strides=[1,pool_height_stride,pool_width_stride ,1],padding=\"SAME\")\n",
    "    return max_pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor,(-1,tensor_shape[1]*tensor_shape[2]*tensor_shape[3]))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    fully_connected_weights  = tf.Variable(tf.truncated_normal([tensor_shape[1],num_outputs],stddev=0.01))\n",
    "    fully_connected_bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    return tf.nn.relu( tf.add(tf.matmul(x_tensor,fully_connected_weights),fully_connected_bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    fully_connected_weights  = tf.Variable(tf.truncated_normal([tensor_shape[1],num_outputs]))\n",
    "    fully_connected_bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor,fully_connected_weights),fully_connected_bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        \"\"\"\n",
    "        Create a convolutional neural network model\n",
    "        : x: Placeholder tensor that holds image data.\n",
    "        : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "        : return: Tensor that represents logits\n",
    "        \"\"\"\n",
    "        # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "        #    Play around with different number of outputs, kernel size and stride\n",
    "        # Function Definition from Above:\n",
    "        #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "        network = conv2d_maxpool(x,32,[4,4],[1,1],[4,4],[2,2])\n",
    "        network = tf.nn.dropout(network,keep_prob)\n",
    "        network = conv2d_maxpool(network,128,[6,6],[1,1],[4,4],[2,2])\n",
    "        network = tf.nn.dropout(network,keep_prob)\n",
    "        network = conv2d_maxpool(network,256,[8,8],[1,1],[4,4],[2,2])\n",
    "        network = tf.nn.dropout(network,keep_prob)\n",
    "        # TODO: Apply a Flatten Layer\n",
    "        # Function Definition from Above:\n",
    "        #   flatten(x_tensor)\n",
    "        network = flatten(network)\n",
    "        #network = tf.nn.dropout(network,keep_prob)\n",
    "\n",
    "        # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "        #    Play around with different number of outputs\n",
    "        # Function Definition from Above:\n",
    "        #   fully_conn(x_tensor, num_outputs)\n",
    "        network = fully_conn(network,10)\n",
    "        #network = tf.nn.dropout(network,keep_prob)\n",
    "        network = fully_conn(network,10)\n",
    "        #network = tf.nn.dropout(network,keep_prob)\n",
    "        #network = fully_conn(network,256)\n",
    "        #network = fully_conn(network,128)\n",
    "        #network = tf.nn.dropout(network,keep_prob)\n",
    "\n",
    "        # TODO: Apply an Output Layer\n",
    "        #    Set this to the number of classes\n",
    "        # Function Definition from Above:\n",
    "        #   output(x_tensor, num_outputs)\n",
    "        network = output(network,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return network\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    feed_dict = {x:feature_batch,y:label_batch,keep_prob:keep_probability}\n",
    "    optimizer = session.run(optimizer,feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    val_feed_dict = {x:valid_features,y:valid_labels,keep_prob:1.0}\n",
    "    train_feed_dict = {x:feature_batch,y:label_batch,keep_prob:1.0}\n",
    "    val_cost = session.run(cost,feed_dict=val_feed_dict)\n",
    "    val_accuracy = session.run(accuracy,feed_dict=val_feed_dict)\n",
    "    train_cost = session.run(cost,feed_dict=train_feed_dict)\n",
    "    train_accuracy = session.run(accuracy,feed_dict=train_feed_dict)\n",
    "    print(\"train Cost\",train_cost,\"train accuracy\",train_accuracy)\n",
    "    print(\"val Cost\",val_cost,\"val accuracy\",val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 150\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  train Cost 2.28548 train accuracy 0.149752\n",
      "val Cost 2.2855 val accuracy 0.1436\n",
      "Epoch  2, CIFAR-10 Batch 1:  train Cost 2.25918 train accuracy 0.141089\n",
      "val Cost 2.26268 val accuracy 0.1348\n",
      "Epoch  3, CIFAR-10 Batch 1:  train Cost 2.23327 train accuracy 0.138614\n",
      "val Cost 2.23924 val accuracy 0.1306\n",
      "Epoch  4, CIFAR-10 Batch 1:  train Cost 2.20395 train accuracy 0.153465\n",
      "val Cost 2.20716 val accuracy 0.15\n",
      "Epoch  5, CIFAR-10 Batch 1:  train Cost 2.19917 train accuracy 0.15099\n",
      "val Cost 2.19146 val accuracy 0.1612\n",
      "Epoch  6, CIFAR-10 Batch 1:  train Cost 2.18378 train accuracy 0.183168\n",
      "val Cost 2.14987 val accuracy 0.185\n",
      "Epoch  7, CIFAR-10 Batch 1:  train Cost 2.13988 train accuracy 0.206683\n",
      "val Cost 2.1203 val accuracy 0.1956\n",
      "Epoch  8, CIFAR-10 Batch 1:  train Cost 2.11667 train accuracy 0.201733\n",
      "val Cost 2.10697 val accuracy 0.191\n",
      "Epoch  9, CIFAR-10 Batch 1:  train Cost 2.08661 train accuracy 0.210396\n",
      "val Cost 2.07872 val accuracy 0.1996\n",
      "Epoch 10, CIFAR-10 Batch 1:  train Cost 2.05948 train accuracy 0.230198\n",
      "val Cost 2.04108 val accuracy 0.2142\n",
      "Epoch 11, CIFAR-10 Batch 1:  train Cost 2.02747 train accuracy 0.219059\n",
      "val Cost 2.01 val accuracy 0.2232\n",
      "Epoch 12, CIFAR-10 Batch 1:  train Cost 2.0006 train accuracy 0.237624\n",
      "val Cost 1.97852 val accuracy 0.2342\n",
      "Epoch 13, CIFAR-10 Batch 1:  train Cost 1.98055 train accuracy 0.252475\n",
      "val Cost 1.96402 val accuracy 0.2482\n",
      "Epoch 14, CIFAR-10 Batch 1:  train Cost 1.94708 train accuracy 0.267327\n",
      "val Cost 1.92653 val accuracy 0.2622\n",
      "Epoch 15, CIFAR-10 Batch 1:  train Cost 1.91985 train accuracy 0.272277\n",
      "val Cost 1.90325 val accuracy 0.2686\n",
      "Epoch 16, CIFAR-10 Batch 1:  train Cost 1.89145 train accuracy 0.282178\n",
      "val Cost 1.87531 val accuracy 0.2672\n",
      "Epoch 17, CIFAR-10 Batch 1:  train Cost 1.84248 train accuracy 0.287129\n",
      "val Cost 1.82714 val accuracy 0.281\n",
      "Epoch 18, CIFAR-10 Batch 1:  train Cost 1.79361 train accuracy 0.32797\n",
      "val Cost 1.78162 val accuracy 0.3152\n",
      "Epoch 19, CIFAR-10 Batch 1:  train Cost 1.7594 train accuracy 0.335396\n",
      "val Cost 1.76246 val accuracy 0.3212\n",
      "Epoch 20, CIFAR-10 Batch 1:  train Cost 1.71718 train accuracy 0.334158\n",
      "val Cost 1.72742 val accuracy 0.3258\n",
      "Epoch 21, CIFAR-10 Batch 1:  train Cost 1.6943 train accuracy 0.32797\n",
      "val Cost 1.71087 val accuracy 0.322\n",
      "Epoch 22, CIFAR-10 Batch 1:  train Cost 1.66323 train accuracy 0.34901\n",
      "val Cost 1.68229 val accuracy 0.3384\n",
      "Epoch 23, CIFAR-10 Batch 1:  train Cost 1.62685 train accuracy 0.373762\n",
      "val Cost 1.65239 val accuracy 0.3452\n",
      "Epoch 24, CIFAR-10 Batch 1:  train Cost 1.62467 train accuracy 0.363861\n",
      "val Cost 1.67259 val accuracy 0.3388\n",
      "Epoch 25, CIFAR-10 Batch 1:  train Cost 1.57603 train accuracy 0.376238\n",
      "val Cost 1.62078 val accuracy 0.3534\n",
      "Epoch 26, CIFAR-10 Batch 1:  train Cost 1.54747 train accuracy 0.377475\n",
      "val Cost 1.59535 val accuracy 0.3644\n",
      "Epoch 27, CIFAR-10 Batch 1:  train Cost 1.51513 train accuracy 0.404703\n",
      "val Cost 1.57379 val accuracy 0.3742\n",
      "Epoch 28, CIFAR-10 Batch 1:  train Cost 1.54245 train accuracy 0.388614\n",
      "val Cost 1.62194 val accuracy 0.3606\n",
      "Epoch 29, CIFAR-10 Batch 1:  train Cost 1.50692 train accuracy 0.404703\n",
      "val Cost 1.57951 val accuracy 0.3734\n",
      "Epoch 30, CIFAR-10 Batch 1:  train Cost 1.47104 train accuracy 0.413366\n",
      "val Cost 1.54318 val accuracy 0.383\n",
      "Epoch 31, CIFAR-10 Batch 1:  train Cost 1.455 train accuracy 0.417079\n",
      "val Cost 1.53447 val accuracy 0.3932\n",
      "Epoch 32, CIFAR-10 Batch 1:  train Cost 1.42815 train accuracy 0.443069\n",
      "val Cost 1.51566 val accuracy 0.4086\n",
      "Epoch 33, CIFAR-10 Batch 1:  train Cost 1.4622 train accuracy 0.443069\n",
      "val Cost 1.56301 val accuracy 0.4006\n",
      "Epoch 34, CIFAR-10 Batch 1:  train Cost 1.39593 train accuracy 0.445545\n",
      "val Cost 1.49798 val accuracy 0.4056\n",
      "Epoch 35, CIFAR-10 Batch 1:  train Cost 1.41707 train accuracy 0.465347\n",
      "val Cost 1.5302 val accuracy 0.4126\n",
      "Epoch 36, CIFAR-10 Batch 1:  train Cost 1.38762 train accuracy 0.483911\n",
      "val Cost 1.49861 val accuracy 0.419\n",
      "Epoch 37, CIFAR-10 Batch 1:  train Cost 1.34567 train accuracy 0.457921\n",
      "val Cost 1.4689 val accuracy 0.4196\n",
      "Epoch 38, CIFAR-10 Batch 1:  train Cost 1.31926 train accuracy 0.475248\n",
      "val Cost 1.45443 val accuracy 0.4278\n",
      "Epoch 39, CIFAR-10 Batch 1:  train Cost 1.3062 train accuracy 0.491337\n",
      "val Cost 1.45275 val accuracy 0.4294\n",
      "Epoch 40, CIFAR-10 Batch 1:  train Cost 1.30423 train accuracy 0.501238\n",
      "val Cost 1.45355 val accuracy 0.4348\n",
      "Epoch 41, CIFAR-10 Batch 1:  train Cost 1.28783 train accuracy 0.52104\n",
      "val Cost 1.44163 val accuracy 0.4392\n",
      "Epoch 42, CIFAR-10 Batch 1:  train Cost 1.28753 train accuracy 0.519802\n",
      "val Cost 1.44615 val accuracy 0.4404\n",
      "Epoch 43, CIFAR-10 Batch 1:  train Cost 1.24843 train accuracy 0.540842\n",
      "val Cost 1.40847 val accuracy 0.4542\n",
      "Epoch 44, CIFAR-10 Batch 1:  train Cost 1.26612 train accuracy 0.502475\n",
      "val Cost 1.43173 val accuracy 0.4386\n",
      "Epoch 45, CIFAR-10 Batch 1:  train Cost 1.2115 train accuracy 0.528465\n",
      "val Cost 1.38645 val accuracy 0.4564\n",
      "Epoch 46, CIFAR-10 Batch 1:  train Cost 1.21187 train accuracy 0.540842\n",
      "val Cost 1.41127 val accuracy 0.4512\n",
      "Epoch 47, CIFAR-10 Batch 1:  train Cost 1.16739 train accuracy 0.556931\n",
      "val Cost 1.36846 val accuracy 0.468\n",
      "Epoch 48, CIFAR-10 Batch 1:  train Cost 1.15627 train accuracy 0.544554\n",
      "val Cost 1.37533 val accuracy 0.461\n",
      "Epoch 49, CIFAR-10 Batch 1:  train Cost 1.17316 train accuracy 0.550743\n",
      "val Cost 1.3887 val accuracy 0.4662\n",
      "Epoch 50, CIFAR-10 Batch 1:  train Cost 1.14051 train accuracy 0.556931\n",
      "val Cost 1.36289 val accuracy 0.4822\n",
      "Epoch 51, CIFAR-10 Batch 1:  train Cost 1.20108 train accuracy 0.533416\n",
      "val Cost 1.43832 val accuracy 0.4554\n",
      "Epoch 52, CIFAR-10 Batch 1:  train Cost 1.13694 train accuracy 0.57302\n",
      "val Cost 1.35447 val accuracy 0.4822\n",
      "Epoch 53, CIFAR-10 Batch 1:  train Cost 1.09914 train accuracy 0.575495\n",
      "val Cost 1.32997 val accuracy 0.4922\n",
      "Epoch 54, CIFAR-10 Batch 1:  train Cost 1.10734 train accuracy 0.564356\n",
      "val Cost 1.35019 val accuracy 0.491\n",
      "Epoch 55, CIFAR-10 Batch 1:  train Cost 1.06269 train accuracy 0.597772\n",
      "val Cost 1.31652 val accuracy 0.4962\n",
      "Epoch 56, CIFAR-10 Batch 1:  train Cost 1.04727 train accuracy 0.590347\n",
      "val Cost 1.31017 val accuracy 0.5028\n",
      "Epoch 57, CIFAR-10 Batch 1:  train Cost 1.03152 train accuracy 0.586634\n",
      "val Cost 1.31339 val accuracy 0.4952\n",
      "Epoch 58, CIFAR-10 Batch 1:  train Cost 1.01831 train accuracy 0.610148\n",
      "val Cost 1.31203 val accuracy 0.5074\n",
      "Epoch 59, CIFAR-10 Batch 1:  train Cost 1.00198 train accuracy 0.607673\n",
      "val Cost 1.30245 val accuracy 0.5054\n",
      "Epoch 60, CIFAR-10 Batch 1:  train Cost 0.991014 train accuracy 0.622525\n",
      "val Cost 1.29662 val accuracy 0.5106\n",
      "Epoch 61, CIFAR-10 Batch 1:  train Cost 0.955589 train accuracy 0.638614\n",
      "val Cost 1.27885 val accuracy 0.5238\n",
      "Epoch 62, CIFAR-10 Batch 1:  train Cost 0.935308 train accuracy 0.632426\n",
      "val Cost 1.27938 val accuracy 0.528\n",
      "Epoch 63, CIFAR-10 Batch 1:  train Cost 0.930724 train accuracy 0.654703\n",
      "val Cost 1.27407 val accuracy 0.5248\n",
      "Epoch 64, CIFAR-10 Batch 1:  train Cost 0.903695 train accuracy 0.65099\n",
      "val Cost 1.26443 val accuracy 0.5366\n",
      "Epoch 65, CIFAR-10 Batch 1:  train Cost 0.903467 train accuracy 0.657178\n",
      "val Cost 1.27356 val accuracy 0.5292\n",
      "Epoch 66, CIFAR-10 Batch 1:  train Cost 0.894845 train accuracy 0.667079\n",
      "val Cost 1.27569 val accuracy 0.537\n",
      "Epoch 67, CIFAR-10 Batch 1:  train Cost 0.891436 train accuracy 0.683168\n",
      "val Cost 1.27485 val accuracy 0.5334\n",
      "Epoch 68, CIFAR-10 Batch 1:  train Cost 0.863571 train accuracy 0.685644\n",
      "val Cost 1.26091 val accuracy 0.541\n",
      "Epoch 69, CIFAR-10 Batch 1:  train Cost 0.854346 train accuracy 0.704208\n",
      "val Cost 1.2673 val accuracy 0.5394\n",
      "Epoch 70, CIFAR-10 Batch 1:  train Cost 0.807661 train accuracy 0.706683\n",
      "val Cost 1.24706 val accuracy 0.547\n",
      "Epoch 71, CIFAR-10 Batch 1:  train Cost 0.789291 train accuracy 0.70297\n",
      "val Cost 1.24574 val accuracy 0.545\n",
      "Epoch 72, CIFAR-10 Batch 1:  train Cost 0.802637 train accuracy 0.691832\n",
      "val Cost 1.27591 val accuracy 0.5466\n",
      "Epoch 73, CIFAR-10 Batch 1:  train Cost 0.76939 train accuracy 0.727723\n",
      "val Cost 1.23506 val accuracy 0.5468\n",
      "Epoch 74, CIFAR-10 Batch 1:  train Cost 0.766591 train accuracy 0.712871\n",
      "val Cost 1.26333 val accuracy 0.5586\n",
      "Epoch 75, CIFAR-10 Batch 1:  train Cost 0.774751 train accuracy 0.725248\n",
      "val Cost 1.27633 val accuracy 0.5432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, CIFAR-10 Batch 1:  train Cost 0.733522 train accuracy 0.743812\n",
      "val Cost 1.23275 val accuracy 0.5632\n",
      "Epoch 77, CIFAR-10 Batch 1:  train Cost 0.750356 train accuracy 0.735149\n",
      "val Cost 1.26491 val accuracy 0.5578\n",
      "Epoch 78, CIFAR-10 Batch 1:  train Cost 0.689232 train accuracy 0.757426\n",
      "val Cost 1.22632 val accuracy 0.566\n",
      "Epoch 79, CIFAR-10 Batch 1:  train Cost 0.69165 train accuracy 0.766089\n",
      "val Cost 1.24449 val accuracy 0.5648\n",
      "Epoch 80, CIFAR-10 Batch 1:  train Cost 0.660031 train accuracy 0.782178\n",
      "val Cost 1.2319 val accuracy 0.5734\n",
      "Epoch 81, CIFAR-10 Batch 1:  train Cost 0.656029 train accuracy 0.77104\n",
      "val Cost 1.23766 val accuracy 0.5768\n",
      "Epoch 82, CIFAR-10 Batch 1:  train Cost 0.615874 train accuracy 0.795792\n",
      "val Cost 1.22876 val accuracy 0.578\n",
      "Epoch 83, CIFAR-10 Batch 1:  train Cost 0.610153 train accuracy 0.793317\n",
      "val Cost 1.22758 val accuracy 0.5774\n",
      "Epoch 84, CIFAR-10 Batch 1:  train Cost 0.598007 train accuracy 0.793317\n",
      "val Cost 1.23369 val accuracy 0.5782\n",
      "Epoch 85, CIFAR-10 Batch 1:  train Cost 0.589237 train accuracy 0.808168\n",
      "val Cost 1.22333 val accuracy 0.5854\n",
      "Epoch 86, CIFAR-10 Batch 1:  train Cost 0.579322 train accuracy 0.808168\n",
      "val Cost 1.24373 val accuracy 0.5798\n",
      "Epoch 87, CIFAR-10 Batch 1:  train Cost 0.564397 train accuracy 0.811881\n",
      "val Cost 1.2387 val accuracy 0.585\n",
      "Epoch 88, CIFAR-10 Batch 1:  train Cost 0.557438 train accuracy 0.806931\n",
      "val Cost 1.23239 val accuracy 0.5906\n",
      "Epoch 89, CIFAR-10 Batch 1:  train Cost 0.550107 train accuracy 0.814356\n",
      "val Cost 1.23329 val accuracy 0.5848\n",
      "Epoch 90, CIFAR-10 Batch 1:  train Cost 0.539237 train accuracy 0.821782\n",
      "val Cost 1.25902 val accuracy 0.5834\n",
      "Epoch 91, CIFAR-10 Batch 1:  train Cost 0.552909 train accuracy 0.82302\n",
      "val Cost 1.23814 val accuracy 0.5806\n",
      "Epoch 92, CIFAR-10 Batch 1:  train Cost 0.510246 train accuracy 0.839109\n",
      "val Cost 1.24482 val accuracy 0.5882\n",
      "Epoch 93, CIFAR-10 Batch 1:  train Cost 0.492932 train accuracy 0.850248\n",
      "val Cost 1.23252 val accuracy 0.5872\n",
      "Epoch 94, CIFAR-10 Batch 1:  train Cost 0.484911 train accuracy 0.841584\n",
      "val Cost 1.2451 val accuracy 0.5904\n",
      "Epoch 95, CIFAR-10 Batch 1:  train Cost 0.491912 train accuracy 0.836634\n",
      "val Cost 1.24858 val accuracy 0.5912\n",
      "Epoch 96, CIFAR-10 Batch 1:  train Cost 0.473745 train accuracy 0.844059\n",
      "val Cost 1.25546 val accuracy 0.592\n",
      "Epoch 97, CIFAR-10 Batch 1:  train Cost 0.501882 train accuracy 0.820545\n",
      "val Cost 1.28573 val accuracy 0.5806\n",
      "Epoch 98, CIFAR-10 Batch 1:  train Cost 0.540612 train accuracy 0.805693\n",
      "val Cost 1.34884 val accuracy 0.5568\n",
      "Epoch 99, CIFAR-10 Batch 1:  train Cost 0.556911 train accuracy 0.795792\n",
      "val Cost 1.32208 val accuracy 0.5636\n",
      "Epoch 100, CIFAR-10 Batch 1:  train Cost 0.609171 train accuracy 0.787129\n",
      "val Cost 1.43331 val accuracy 0.5494\n",
      "Epoch 101, CIFAR-10 Batch 1:  train Cost 0.506105 train accuracy 0.824257\n",
      "val Cost 1.29052 val accuracy 0.5796\n",
      "Epoch 102, CIFAR-10 Batch 1:  train Cost 0.461232 train accuracy 0.850248\n",
      "val Cost 1.27121 val accuracy 0.5908\n",
      "Epoch 103, CIFAR-10 Batch 1:  train Cost 0.443503 train accuracy 0.867574\n",
      "val Cost 1.25441 val accuracy 0.5854\n",
      "Epoch 104, CIFAR-10 Batch 1:  train Cost 0.446859 train accuracy 0.850248\n",
      "val Cost 1.29328 val accuracy 0.5822\n",
      "Epoch 105, CIFAR-10 Batch 1:  train Cost 0.422889 train accuracy 0.856436\n",
      "val Cost 1.26813 val accuracy 0.598\n",
      "Epoch 106, CIFAR-10 Batch 1:  train Cost 0.398941 train accuracy 0.870049\n",
      "val Cost 1.27186 val accuracy 0.5978\n",
      "Epoch 107, CIFAR-10 Batch 1:  train Cost 0.39118 train accuracy 0.878713\n",
      "val Cost 1.27054 val accuracy 0.5964\n",
      "Epoch 108, CIFAR-10 Batch 1:  train Cost 0.388446 train accuracy 0.876238\n",
      "val Cost 1.27457 val accuracy 0.5934\n",
      "Epoch 109, CIFAR-10 Batch 1:  train Cost 0.364047 train accuracy 0.872525\n",
      "val Cost 1.29722 val accuracy 0.5982\n",
      "Epoch 110, CIFAR-10 Batch 1:  train Cost 0.368483 train accuracy 0.875\n",
      "val Cost 1.33994 val accuracy 0.5934\n",
      "Epoch 111, CIFAR-10 Batch 1:  train Cost 0.368671 train accuracy 0.871287\n",
      "val Cost 1.34297 val accuracy 0.5952\n",
      "Epoch 112, CIFAR-10 Batch 1:  train Cost 0.417285 train accuracy 0.847772\n",
      "val Cost 1.43553 val accuracy 0.5814\n",
      "Epoch 113, CIFAR-10 Batch 1:  train Cost 0.405351 train accuracy 0.846535\n",
      "val Cost 1.4177 val accuracy 0.588\n",
      "Epoch 114, CIFAR-10 Batch 1:  train Cost 0.380749 train accuracy 0.862624\n",
      "val Cost 1.37226 val accuracy 0.584\n",
      "Epoch 115, CIFAR-10 Batch 1:  train Cost 0.362805 train accuracy 0.881188\n",
      "val Cost 1.38258 val accuracy 0.5852\n",
      "Epoch 116, CIFAR-10 Batch 1:  train Cost 0.35434 train accuracy 0.894802\n",
      "val Cost 1.34892 val accuracy 0.5934\n",
      "Epoch 117, CIFAR-10 Batch 1:  train Cost 0.326497 train accuracy 0.905941\n",
      "val Cost 1.32639 val accuracy 0.5974\n",
      "Epoch 118, CIFAR-10 Batch 1:  train Cost 0.319602 train accuracy 0.903465\n",
      "val Cost 1.30578 val accuracy 0.6052\n",
      "Epoch 119, CIFAR-10 Batch 1:  train Cost 0.309995 train accuracy 0.908416\n",
      "val Cost 1.26303 val accuracy 0.6152\n",
      "Epoch 120, CIFAR-10 Batch 1:  train Cost 0.306355 train accuracy 0.907178\n",
      "val Cost 1.29944 val accuracy 0.6156\n",
      "Epoch 121, CIFAR-10 Batch 1:  train Cost 0.284431 train accuracy 0.917079\n",
      "val Cost 1.29454 val accuracy 0.6114\n",
      "Epoch 122, CIFAR-10 Batch 1:  train Cost 0.274707 train accuracy 0.920792\n",
      "val Cost 1.32035 val accuracy 0.615\n",
      "Epoch 123, CIFAR-10 Batch 1:  train Cost 0.266298 train accuracy 0.923267\n",
      "val Cost 1.28443 val accuracy 0.6172\n",
      "Epoch 124, CIFAR-10 Batch 1:  train Cost 0.257414 train accuracy 0.931931\n",
      "val Cost 1.33175 val accuracy 0.6176\n",
      "Epoch 125, CIFAR-10 Batch 1:  train Cost 0.262432 train accuracy 0.92698\n",
      "val Cost 1.34584 val accuracy 0.6138\n",
      "Epoch 126, CIFAR-10 Batch 1:  train Cost 0.251923 train accuracy 0.92698\n",
      "val Cost 1.35226 val accuracy 0.6134\n",
      "Epoch 127, CIFAR-10 Batch 1:  train Cost 0.24537 train accuracy 0.92203\n",
      "val Cost 1.33314 val accuracy 0.6154\n",
      "Epoch 128, CIFAR-10 Batch 1:  train Cost 0.235299 train accuracy 0.936881\n",
      "val Cost 1.35211 val accuracy 0.6216\n",
      "Epoch 129, CIFAR-10 Batch 1:  train Cost 0.229511 train accuracy 0.934406\n",
      "val Cost 1.33367 val accuracy 0.6232\n",
      "Epoch 130, CIFAR-10 Batch 1:  train Cost 0.227224 train accuracy 0.930693\n",
      "val Cost 1.33767 val accuracy 0.6168\n",
      "Epoch 131, CIFAR-10 Batch 1:  train Cost 0.221229 train accuracy 0.943069\n",
      "val Cost 1.35485 val accuracy 0.6096\n",
      "Epoch 132, CIFAR-10 Batch 1:  train Cost 0.214067 train accuracy 0.936881\n",
      "val Cost 1.36849 val accuracy 0.6126\n",
      "Epoch 133, CIFAR-10 Batch 1:  train Cost 0.219734 train accuracy 0.939356\n",
      "val Cost 1.37141 val accuracy 0.6174\n",
      "Epoch 134, CIFAR-10 Batch 1:  train Cost 0.210108 train accuracy 0.943069\n",
      "val Cost 1.40301 val accuracy 0.6138\n",
      "Epoch 135, CIFAR-10 Batch 1:  train Cost 0.205379 train accuracy 0.95297\n",
      "val Cost 1.3749 val accuracy 0.6202\n",
      "Epoch 136, CIFAR-10 Batch 1:  train Cost 0.210769 train accuracy 0.945545\n",
      "val Cost 1.38183 val accuracy 0.6262\n",
      "Epoch 137, CIFAR-10 Batch 1:  train Cost 0.1962 train accuracy 0.94802\n",
      "val Cost 1.39689 val accuracy 0.6172\n",
      "Epoch 138, CIFAR-10 Batch 1:  train Cost 0.187958 train accuracy 0.951733\n",
      "val Cost 1.40995 val accuracy 0.6166\n",
      "Epoch 139, CIFAR-10 Batch 1:  train Cost 0.198407 train accuracy 0.94802\n",
      "val Cost 1.39472 val accuracy 0.6194\n",
      "Epoch 140, CIFAR-10 Batch 1:  train Cost 0.18488 train accuracy 0.954208\n",
      "val Cost 1.43513 val accuracy 0.6178\n",
      "Epoch 141, CIFAR-10 Batch 1:  train Cost 0.192547 train accuracy 0.949257\n",
      "val Cost 1.45966 val accuracy 0.6042\n",
      "Epoch 142, CIFAR-10 Batch 1:  train Cost 0.203121 train accuracy 0.941832\n",
      "val Cost 1.45276 val accuracy 0.6066\n",
      "Epoch 143, CIFAR-10 Batch 1:  train Cost 0.183128 train accuracy 0.949257\n",
      "val Cost 1.41726 val accuracy 0.6136\n",
      "Epoch 144, CIFAR-10 Batch 1:  train Cost 0.189855 train accuracy 0.951733\n",
      "val Cost 1.43386 val accuracy 0.6196\n",
      "Epoch 145, CIFAR-10 Batch 1:  train Cost 0.191124 train accuracy 0.938119\n",
      "val Cost 1.4363 val accuracy 0.6164\n",
      "Epoch 146, CIFAR-10 Batch 1:  train Cost 0.191955 train accuracy 0.949257\n",
      "val Cost 1.52167 val accuracy 0.6078\n",
      "Epoch 147, CIFAR-10 Batch 1:  train Cost 0.187436 train accuracy 0.936881\n",
      "val Cost 1.4563 val accuracy 0.6134\n",
      "Epoch 148, CIFAR-10 Batch 1:  train Cost 0.174503 train accuracy 0.957921\n",
      "val Cost 1.47684 val accuracy 0.6164\n",
      "Epoch 149, CIFAR-10 Batch 1:  train Cost 0.161392 train accuracy 0.960396\n",
      "val Cost 1.48919 val accuracy 0.6174\n",
      "Epoch 150, CIFAR-10 Batch 1:  train Cost 0.159801 train accuracy 0.956683\n",
      "val Cost 1.47861 val accuracy 0.6208\n",
      "time 323.35242199999993\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "print(\"time\",time.clock()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment results\n",
    "* 100 epoch,512 batch size, 2 convolutions, 2 fully connected, no dropout:train Cost 3317.09 train accuracy 0.760135 val Cost 27927.9 val accuracy 0.4018\n",
    "* 20 epoch,96 batch size, 3 convolutions, 1 fully connected 512 unnits, no dropout:train Cost 225.305 train accuracy 0.958333\n",
    "val Cost 54559.4 val accuracy 0.4438\n",
    "* 20 epoch,96 batch size, 3 convolutions, 1 fully connected 512 unnits, 0.6 dropout before last layer(after fully connected): after 10 batches it plateaus to train Cost 2.24019 train accuracy 0.125\n",
    "val Cost 151.769 val accuracy 0.106\n",
    "* same parameters as previous experiment but with dropout after last convolution and before fully connected: started with high values (35 and 30 percent accuracy on first epoch but then dropped to las than 10% for the first 10 epochs)\n",
    "* 20 epoch,96 batch size, 3 convolutions, 2 fully connected 512 unnits: it overfits, in epoch 25 train acc is 94% but validation is 41%\n",
    "* same as the last one but with  2 dropouts, one after first convolution and one between fully connected (0.5 keep prob): after 10 epochs, it didnt improved (less than 0.7)\n",
    "--------------------------------------------------------------\n",
    "    512 batch, keep_prob = 0.5, 45 epochs\n",
    "    network = conv2d_maxpool(x,32,[3,3],[1,1],[2,2],[2,2])\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = conv2d_maxpool(network,32,[3,3],[2,2],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,64,[3,3],[1,1],[2,2],[2,2])\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = flatten(network)\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    didnt improved, of 0.1\n",
    "-----------------------------------------------------------------------------------\n",
    "    512 batch, keep_prob = 0.5, 45 epochs\n",
    "    network = conv2d_maxpool(x,32,[3,3],[1,1],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,32,[3,3],[2,2],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,64,[3,3],[1,1],[2,2],[2,2])\n",
    "    network = flatten(network)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,128)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    after giving a smaller stdv to weights initialization, it got good results! 0.8 on trian and 0.55 on validation!\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  train Cost 2.3018 train accuracy 0.102723\n",
      "val Cost 2.3019 val accuracy 0.112\n",
      "Epoch  1, CIFAR-10 Batch 2:  train Cost 2.30111 train accuracy 0.131188\n",
      "val Cost 2.30118 val accuracy 0.1214\n",
      "Epoch  1, CIFAR-10 Batch 3:  train Cost 2.2875 train accuracy 0.0866337\n",
      "val Cost 2.29402 val accuracy 0.0832\n",
      "Epoch  1, CIFAR-10 Batch 4:  train Cost 2.27469 train accuracy 0.131188\n",
      "val Cost 2.27472 val accuracy 0.1384\n",
      "Epoch  1, CIFAR-10 Batch 5:  train Cost 2.23848 train accuracy 0.0990099\n",
      "val Cost 2.24318 val accuracy 0.0924\n",
      "Epoch  2, CIFAR-10 Batch 1:  train Cost 2.23632 train accuracy 0.12995\n",
      "val Cost 2.22263 val accuracy 0.123\n",
      "Epoch  2, CIFAR-10 Batch 2:  train Cost 2.21584 train accuracy 0.144802\n",
      "val Cost 2.21357 val accuracy 0.1446\n",
      "Epoch  2, CIFAR-10 Batch 3:  train Cost 2.19738 train accuracy 0.168317\n",
      "val Cost 2.19923 val accuracy 0.1604\n",
      "Epoch  2, CIFAR-10 Batch 4:  train Cost 2.19204 train accuracy 0.144802\n",
      "val Cost 2.18888 val accuracy 0.1578\n",
      "Epoch  2, CIFAR-10 Batch 5:  train Cost 2.13668 train accuracy 0.209158\n",
      "val Cost 2.14102 val accuracy 0.2004\n",
      "Epoch  3, CIFAR-10 Batch 1:  train Cost 2.16054 train accuracy 0.190594\n",
      "val Cost 2.13398 val accuracy 0.2088\n",
      "Epoch  3, CIFAR-10 Batch 2:  train Cost 2.09671 train accuracy 0.238861\n",
      "val Cost 2.09502 val accuracy 0.214\n",
      "Epoch  3, CIFAR-10 Batch 3:  train Cost 2.06077 train accuracy 0.227723\n",
      "val Cost 2.08041 val accuracy 0.2094\n",
      "Epoch  3, CIFAR-10 Batch 4:  train Cost 2.07929 train accuracy 0.209158\n",
      "val Cost 2.0698 val accuracy 0.2206\n",
      "Epoch  3, CIFAR-10 Batch 5:  train Cost 2.00696 train accuracy 0.256188\n",
      "val Cost 2.02639 val accuracy 0.246\n",
      "Epoch  4, CIFAR-10 Batch 1:  train Cost 2.05219 train accuracy 0.267327\n",
      "val Cost 2.03107 val accuracy 0.2732\n",
      "Epoch  4, CIFAR-10 Batch 2:  train Cost 2.00573 train accuracy 0.247525\n",
      "val Cost 2.00477 val accuracy 0.267\n",
      "Epoch  4, CIFAR-10 Batch 3:  train Cost 1.98167 train accuracy 0.25\n",
      "val Cost 2.00535 val accuracy 0.2538\n",
      "Epoch  4, CIFAR-10 Batch 4:  train Cost 1.94315 train accuracy 0.303218\n",
      "val Cost 1.93853 val accuracy 0.2852\n",
      "Epoch  4, CIFAR-10 Batch 5:  train Cost 1.92637 train accuracy 0.306931\n",
      "val Cost 1.94578 val accuracy 0.2754\n",
      "Epoch  5, CIFAR-10 Batch 1:  train Cost 1.93375 train accuracy 0.315594\n",
      "val Cost 1.92495 val accuracy 0.293\n",
      "Epoch  5, CIFAR-10 Batch 2:  train Cost 1.91007 train accuracy 0.289604\n",
      "val Cost 1.90272 val accuracy 0.3058\n",
      "Epoch  5, CIFAR-10 Batch 3:  train Cost 1.81182 train accuracy 0.321782\n",
      "val Cost 1.85813 val accuracy 0.3078\n",
      "Epoch  5, CIFAR-10 Batch 4:  train Cost 1.87706 train accuracy 0.293317\n",
      "val Cost 1.86866 val accuracy 0.3074\n",
      "Epoch  5, CIFAR-10 Batch 5:  train Cost 1.83242 train accuracy 0.340347\n",
      "val Cost 1.84139 val accuracy 0.3094\n",
      "Epoch  6, CIFAR-10 Batch 1:  train Cost 1.80174 train accuracy 0.350248\n",
      "val Cost 1.8054 val accuracy 0.3276\n",
      "Epoch  6, CIFAR-10 Batch 2:  train Cost 1.81054 train accuracy 0.310644\n",
      "val Cost 1.78718 val accuracy 0.327\n",
      "Epoch  6, CIFAR-10 Batch 3:  train Cost 1.81604 train accuracy 0.34901\n",
      "val Cost 1.86108 val accuracy 0.3026\n",
      "Epoch  6, CIFAR-10 Batch 4:  train Cost 1.77195 train accuracy 0.339109\n",
      "val Cost 1.77099 val accuracy 0.3312\n",
      "Epoch  6, CIFAR-10 Batch 5:  train Cost 1.71818 train accuracy 0.368812\n",
      "val Cost 1.73071 val accuracy 0.346\n",
      "Epoch  7, CIFAR-10 Batch 1:  train Cost 1.71137 train accuracy 0.363861\n",
      "val Cost 1.72021 val accuracy 0.3574\n",
      "Epoch  7, CIFAR-10 Batch 2:  train Cost 1.74307 train accuracy 0.344059\n",
      "val Cost 1.71493 val accuracy 0.3554\n",
      "Epoch  7, CIFAR-10 Batch 3:  train Cost 1.66174 train accuracy 0.373762\n",
      "val Cost 1.70854 val accuracy 0.3406\n",
      "Epoch  7, CIFAR-10 Batch 4:  train Cost 1.67301 train accuracy 0.37005\n",
      "val Cost 1.67814 val accuracy 0.37\n",
      "Epoch  7, CIFAR-10 Batch 5:  train Cost 1.67954 train accuracy 0.371287\n",
      "val Cost 1.68157 val accuracy 0.3662\n",
      "Epoch  8, CIFAR-10 Batch 1:  train Cost 1.65016 train accuracy 0.388614\n",
      "val Cost 1.67425 val accuracy 0.3846\n",
      "Epoch  8, CIFAR-10 Batch 2:  train Cost 1.66936 train accuracy 0.384901\n",
      "val Cost 1.64987 val accuracy 0.3914\n",
      "Epoch  8, CIFAR-10 Batch 3:  train Cost 1.58306 train accuracy 0.397277\n",
      "val Cost 1.63294 val accuracy 0.3836\n",
      "Epoch  8, CIFAR-10 Batch 4:  train Cost 1.5895 train accuracy 0.397277\n",
      "val Cost 1.60159 val accuracy 0.4064\n",
      "Epoch  8, CIFAR-10 Batch 5:  train Cost 1.55886 train accuracy 0.424505\n",
      "val Cost 1.57368 val accuracy 0.4258\n",
      "Epoch  9, CIFAR-10 Batch 1:  train Cost 1.57036 train accuracy 0.420792\n",
      "val Cost 1.60596 val accuracy 0.41\n",
      "Epoch  9, CIFAR-10 Batch 2:  train Cost 1.62491 train accuracy 0.403465\n",
      "val Cost 1.58827 val accuracy 0.424\n",
      "Epoch  9, CIFAR-10 Batch 3:  train Cost 1.54489 train accuracy 0.415842\n",
      "val Cost 1.59958 val accuracy 0.4008\n",
      "Epoch  9, CIFAR-10 Batch 4:  train Cost 1.51366 train accuracy 0.449257\n",
      "val Cost 1.53524 val accuracy 0.4406\n",
      "Epoch  9, CIFAR-10 Batch 5:  train Cost 1.49549 train accuracy 0.450495\n",
      "val Cost 1.51995 val accuracy 0.4438\n",
      "Epoch 10, CIFAR-10 Batch 1:  train Cost 1.52837 train accuracy 0.435644\n",
      "val Cost 1.56017 val accuracy 0.4388\n",
      "Epoch 10, CIFAR-10 Batch 2:  train Cost 1.53554 train accuracy 0.446782\n",
      "val Cost 1.50864 val accuracy 0.4596\n",
      "Epoch 10, CIFAR-10 Batch 3:  train Cost 1.50592 train accuracy 0.430693\n",
      "val Cost 1.56024 val accuracy 0.4258\n",
      "Epoch 10, CIFAR-10 Batch 4:  train Cost 1.46453 train accuracy 0.467822\n",
      "val Cost 1.49104 val accuracy 0.4662\n",
      "Epoch 10, CIFAR-10 Batch 5:  train Cost 1.44345 train accuracy 0.486386\n",
      "val Cost 1.46957 val accuracy 0.4698\n",
      "Epoch 11, CIFAR-10 Batch 1:  train Cost 1.46258 train accuracy 0.476485\n",
      "val Cost 1.49403 val accuracy 0.4664\n",
      "Epoch 11, CIFAR-10 Batch 2:  train Cost 1.47103 train accuracy 0.480198\n",
      "val Cost 1.4542 val accuracy 0.479\n",
      "Epoch 11, CIFAR-10 Batch 3:  train Cost 1.41935 train accuracy 0.491337\n",
      "val Cost 1.4897 val accuracy 0.4662\n",
      "Epoch 11, CIFAR-10 Batch 4:  train Cost 1.40096 train accuracy 0.514851\n",
      "val Cost 1.44414 val accuracy 0.4862\n",
      "Epoch 11, CIFAR-10 Batch 5:  train Cost 1.4008 train accuracy 0.487624\n",
      "val Cost 1.44956 val accuracy 0.477\n",
      "Epoch 12, CIFAR-10 Batch 1:  train Cost 1.3694 train accuracy 0.508663\n",
      "val Cost 1.41585 val accuracy 0.5006\n",
      "Epoch 12, CIFAR-10 Batch 2:  train Cost 1.45848 train accuracy 0.477723\n",
      "val Cost 1.45433 val accuracy 0.4752\n",
      "Epoch 12, CIFAR-10 Batch 3:  train Cost 1.3678 train accuracy 0.5\n",
      "val Cost 1.43126 val accuracy 0.4842\n",
      "Epoch 12, CIFAR-10 Batch 4:  train Cost 1.34921 train accuracy 0.528465\n",
      "val Cost 1.39459 val accuracy 0.5064\n",
      "Epoch 12, CIFAR-10 Batch 5:  train Cost 1.34278 train accuracy 0.522277\n",
      "val Cost 1.39052 val accuracy 0.4968\n",
      "Epoch 13, CIFAR-10 Batch 1:  train Cost 1.32515 train accuracy 0.512376\n",
      "val Cost 1.37382 val accuracy 0.5074\n",
      "Epoch 13, CIFAR-10 Batch 2:  train Cost 1.37575 train accuracy 0.517327\n",
      "val Cost 1.37514 val accuracy 0.5112\n",
      "Epoch 13, CIFAR-10 Batch 3:  train Cost 1.30939 train accuracy 0.514851\n",
      "val Cost 1.39466 val accuracy 0.5062\n",
      "Epoch 13, CIFAR-10 Batch 4:  train Cost 1.28705 train accuracy 0.554455\n",
      "val Cost 1.34059 val accuracy 0.523\n",
      "Epoch 13, CIFAR-10 Batch 5:  train Cost 1.29551 train accuracy 0.549505\n",
      "val Cost 1.35354 val accuracy 0.5108\n",
      "Epoch 14, CIFAR-10 Batch 1:  train Cost 1.27389 train accuracy 0.538366\n",
      "val Cost 1.3175 val accuracy 0.5264\n",
      "Epoch 14, CIFAR-10 Batch 2:  train Cost 1.33601 train accuracy 0.529703\n",
      "val Cost 1.34552 val accuracy 0.5202\n",
      "Epoch 14, CIFAR-10 Batch 3:  train Cost 1.26493 train accuracy 0.537129\n",
      "val Cost 1.34406 val accuracy 0.5246\n",
      "Epoch 14, CIFAR-10 Batch 4:  train Cost 1.25798 train accuracy 0.559406\n",
      "val Cost 1.31438 val accuracy 0.5322\n",
      "Epoch 14, CIFAR-10 Batch 5:  train Cost 1.23667 train accuracy 0.566832\n",
      "val Cost 1.30016 val accuracy 0.5352\n",
      "Epoch 15, CIFAR-10 Batch 1:  train Cost 1.24585 train accuracy 0.555693\n",
      "val Cost 1.29254 val accuracy 0.545\n",
      "Epoch 15, CIFAR-10 Batch 2:  train Cost 1.30225 train accuracy 0.555693\n",
      "val Cost 1.30926 val accuracy 0.5358\n",
      "Epoch 15, CIFAR-10 Batch 3:  train Cost 1.20964 train accuracy 0.563119\n",
      "val Cost 1.29602 val accuracy 0.5432\n",
      "Epoch 15, CIFAR-10 Batch 4:  train Cost 1.21035 train accuracy 0.57302\n",
      "val Cost 1.27557 val accuracy 0.5486\n",
      "Epoch 15, CIFAR-10 Batch 5:  train Cost 1.21488 train accuracy 0.579208\n",
      "val Cost 1.28638 val accuracy 0.5436\n",
      "Epoch 16, CIFAR-10 Batch 1:  train Cost 1.19981 train accuracy 0.571782\n",
      "val Cost 1.25715 val accuracy 0.5552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, CIFAR-10 Batch 2:  train Cost 1.24939 train accuracy 0.570545\n",
      "val Cost 1.26639 val accuracy 0.5454\n",
      "Epoch 16, CIFAR-10 Batch 3:  train Cost 1.24146 train accuracy 0.554455\n",
      "val Cost 1.34164 val accuracy 0.5304\n",
      "Epoch 16, CIFAR-10 Batch 4:  train Cost 1.1972 train accuracy 0.586634\n",
      "val Cost 1.26324 val accuracy 0.5502\n",
      "Epoch 16, CIFAR-10 Batch 5:  train Cost 1.15626 train accuracy 0.59901\n",
      "val Cost 1.23177 val accuracy 0.5616\n",
      "Epoch 17, CIFAR-10 Batch 1:  train Cost 1.16622 train accuracy 0.585396\n",
      "val Cost 1.23292 val accuracy 0.5668\n",
      "Epoch 17, CIFAR-10 Batch 2:  train Cost 1.20029 train accuracy 0.602723\n",
      "val Cost 1.22008 val accuracy 0.5726\n",
      "Epoch 17, CIFAR-10 Batch 3:  train Cost 1.16436 train accuracy 0.582921\n",
      "val Cost 1.26794 val accuracy 0.5536\n",
      "Epoch 17, CIFAR-10 Batch 4:  train Cost 1.15648 train accuracy 0.594059\n",
      "val Cost 1.22264 val accuracy 0.5644\n",
      "Epoch 17, CIFAR-10 Batch 5:  train Cost 1.12102 train accuracy 0.602723\n",
      "val Cost 1.21384 val accuracy 0.5668\n",
      "Epoch 18, CIFAR-10 Batch 1:  train Cost 1.11824 train accuracy 0.608911\n",
      "val Cost 1.19989 val accuracy 0.58\n",
      "Epoch 18, CIFAR-10 Batch 2:  train Cost 1.19005 train accuracy 0.60396\n",
      "val Cost 1.21235 val accuracy 0.5754\n",
      "Epoch 18, CIFAR-10 Batch 3:  train Cost 1.15207 train accuracy 0.590347\n",
      "val Cost 1.26656 val accuracy 0.5614\n",
      "Epoch 18, CIFAR-10 Batch 4:  train Cost 1.14679 train accuracy 0.607673\n",
      "val Cost 1.22168 val accuracy 0.5708\n",
      "Epoch 18, CIFAR-10 Batch 5:  train Cost 1.08883 train accuracy 0.617574\n",
      "val Cost 1.19282 val accuracy 0.5766\n",
      "Epoch 19, CIFAR-10 Batch 1:  train Cost 1.1144 train accuracy 0.600248\n",
      "val Cost 1.19544 val accuracy 0.5822\n",
      "Epoch 19, CIFAR-10 Batch 2:  train Cost 1.15016 train accuracy 0.620049\n",
      "val Cost 1.17967 val accuracy 0.5884\n",
      "Epoch 19, CIFAR-10 Batch 3:  train Cost 1.07307 train accuracy 0.627475\n",
      "val Cost 1.18361 val accuracy 0.5886\n",
      "Epoch 19, CIFAR-10 Batch 4:  train Cost 1.1056 train accuracy 0.617574\n",
      "val Cost 1.17589 val accuracy 0.5918\n",
      "Epoch 19, CIFAR-10 Batch 5:  train Cost 1.08113 train accuracy 0.605198\n",
      "val Cost 1.1902 val accuracy 0.5754\n",
      "Epoch 20, CIFAR-10 Batch 1:  train Cost 1.04568 train accuracy 0.641089\n",
      "val Cost 1.1441 val accuracy 0.6002\n",
      "Epoch 20, CIFAR-10 Batch 2:  train Cost 1.11712 train accuracy 0.626238\n",
      "val Cost 1.14537 val accuracy 0.605\n",
      "Epoch 20, CIFAR-10 Batch 3:  train Cost 1.06994 train accuracy 0.643564\n",
      "val Cost 1.18432 val accuracy 0.5936\n",
      "Epoch 20, CIFAR-10 Batch 4:  train Cost 1.06984 train accuracy 0.628713\n",
      "val Cost 1.15724 val accuracy 0.5996\n",
      "Epoch 20, CIFAR-10 Batch 5:  train Cost 1.03339 train accuracy 0.641089\n",
      "val Cost 1.14423 val accuracy 0.5944\n",
      "Epoch 21, CIFAR-10 Batch 1:  train Cost 1.02663 train accuracy 0.632426\n",
      "val Cost 1.12515 val accuracy 0.6092\n",
      "Epoch 21, CIFAR-10 Batch 2:  train Cost 1.08897 train accuracy 0.648515\n",
      "val Cost 1.1263 val accuracy 0.6154\n",
      "Epoch 21, CIFAR-10 Batch 3:  train Cost 1.06158 train accuracy 0.627475\n",
      "val Cost 1.19381 val accuracy 0.5908\n",
      "Epoch 21, CIFAR-10 Batch 4:  train Cost 1.04166 train accuracy 0.643564\n",
      "val Cost 1.13288 val accuracy 0.6062\n",
      "Epoch 21, CIFAR-10 Batch 5:  train Cost 1.00378 train accuracy 0.659653\n",
      "val Cost 1.12288 val accuracy 0.6058\n",
      "Epoch 22, CIFAR-10 Batch 1:  train Cost 1.00821 train accuracy 0.639851\n",
      "val Cost 1.11683 val accuracy 0.614\n",
      "Epoch 22, CIFAR-10 Batch 2:  train Cost 1.07679 train accuracy 0.644802\n",
      "val Cost 1.12519 val accuracy 0.6138\n",
      "Epoch 22, CIFAR-10 Batch 3:  train Cost 1.02192 train accuracy 0.637376\n",
      "val Cost 1.15884 val accuracy 0.6034\n",
      "Epoch 22, CIFAR-10 Batch 4:  train Cost 1.00839 train accuracy 0.654703\n",
      "val Cost 1.10461 val accuracy 0.617\n",
      "Epoch 22, CIFAR-10 Batch 5:  train Cost 0.971915 train accuracy 0.675743\n",
      "val Cost 1.09301 val accuracy 0.6184\n",
      "Epoch 23, CIFAR-10 Batch 1:  train Cost 1.00134 train accuracy 0.636139\n",
      "val Cost 1.10987 val accuracy 0.6166\n",
      "Epoch 23, CIFAR-10 Batch 2:  train Cost 1.04397 train accuracy 0.670792\n",
      "val Cost 1.11098 val accuracy 0.6164\n",
      "Epoch 23, CIFAR-10 Batch 3:  train Cost 1.00053 train accuracy 0.64604\n",
      "val Cost 1.13908 val accuracy 0.61\n",
      "Epoch 23, CIFAR-10 Batch 4:  train Cost 0.992329 train accuracy 0.655941\n",
      "val Cost 1.09783 val accuracy 0.6246\n",
      "Epoch 23, CIFAR-10 Batch 5:  train Cost 0.956247 train accuracy 0.685644\n",
      "val Cost 1.08407 val accuracy 0.6234\n",
      "Epoch 24, CIFAR-10 Batch 1:  train Cost 0.947525 train accuracy 0.669554\n",
      "val Cost 1.06963 val accuracy 0.6334\n",
      "Epoch 24, CIFAR-10 Batch 2:  train Cost 1.03601 train accuracy 0.667079\n",
      "val Cost 1.09426 val accuracy 0.6276\n",
      "Epoch 24, CIFAR-10 Batch 3:  train Cost 0.974773 train accuracy 0.662129\n",
      "val Cost 1.1291 val accuracy 0.615\n",
      "Epoch 24, CIFAR-10 Batch 4:  train Cost 0.969763 train accuracy 0.667079\n",
      "val Cost 1.08202 val accuracy 0.6276\n",
      "Epoch 24, CIFAR-10 Batch 5:  train Cost 0.94952 train accuracy 0.667079\n",
      "val Cost 1.08622 val accuracy 0.6208\n",
      "Epoch 25, CIFAR-10 Batch 1:  train Cost 0.927473 train accuracy 0.674505\n",
      "val Cost 1.05748 val accuracy 0.638\n",
      "Epoch 25, CIFAR-10 Batch 2:  train Cost 1.00081 train accuracy 0.679455\n",
      "val Cost 1.08359 val accuracy 0.6302\n",
      "Epoch 25, CIFAR-10 Batch 3:  train Cost 0.956825 train accuracy 0.680693\n",
      "val Cost 1.11008 val accuracy 0.6274\n",
      "Epoch 25, CIFAR-10 Batch 4:  train Cost 0.944484 train accuracy 0.669554\n",
      "val Cost 1.06161 val accuracy 0.6312\n",
      "Epoch 25, CIFAR-10 Batch 5:  train Cost 0.933864 train accuracy 0.689356\n",
      "val Cost 1.07957 val accuracy 0.6248\n",
      "Epoch 26, CIFAR-10 Batch 1:  train Cost 0.919276 train accuracy 0.668317\n",
      "val Cost 1.05746 val accuracy 0.6366\n",
      "Epoch 26, CIFAR-10 Batch 2:  train Cost 0.959977 train accuracy 0.67698\n",
      "val Cost 1.04177 val accuracy 0.6472\n",
      "Epoch 26, CIFAR-10 Batch 3:  train Cost 0.944809 train accuracy 0.665842\n",
      "val Cost 1.11393 val accuracy 0.6238\n",
      "Epoch 26, CIFAR-10 Batch 4:  train Cost 0.939334 train accuracy 0.668317\n",
      "val Cost 1.06376 val accuracy 0.6318\n",
      "Epoch 26, CIFAR-10 Batch 5:  train Cost 0.893903 train accuracy 0.712871\n",
      "val Cost 1.05744 val accuracy 0.6338\n",
      "Epoch 27, CIFAR-10 Batch 1:  train Cost 0.906233 train accuracy 0.681931\n",
      "val Cost 1.04801 val accuracy 0.6434\n",
      "Epoch 27, CIFAR-10 Batch 2:  train Cost 0.932493 train accuracy 0.700495\n",
      "val Cost 1.02182 val accuracy 0.6546\n",
      "Epoch 27, CIFAR-10 Batch 3:  train Cost 0.883358 train accuracy 0.695545\n",
      "val Cost 1.05603 val accuracy 0.6438\n",
      "Epoch 27, CIFAR-10 Batch 4:  train Cost 0.903466 train accuracy 0.695545\n",
      "val Cost 1.03063 val accuracy 0.6528\n",
      "Epoch 27, CIFAR-10 Batch 5:  train Cost 0.872547 train accuracy 0.70297\n",
      "val Cost 1.02387 val accuracy 0.651\n",
      "Epoch 28, CIFAR-10 Batch 1:  train Cost 0.867013 train accuracy 0.681931\n",
      "val Cost 1.01114 val accuracy 0.6532\n",
      "Epoch 28, CIFAR-10 Batch 2:  train Cost 0.927556 train accuracy 0.691832\n",
      "val Cost 1.0252 val accuracy 0.6526\n",
      "Epoch 28, CIFAR-10 Batch 3:  train Cost 0.887943 train accuracy 0.696782\n",
      "val Cost 1.07251 val accuracy 0.6388\n",
      "Epoch 28, CIFAR-10 Batch 4:  train Cost 0.885921 train accuracy 0.693069\n",
      "val Cost 1.0156 val accuracy 0.6562\n",
      "Epoch 28, CIFAR-10 Batch 5:  train Cost 0.852486 train accuracy 0.722772\n",
      "val Cost 1.03141 val accuracy 0.6494\n",
      "Epoch 29, CIFAR-10 Batch 1:  train Cost 0.893993 train accuracy 0.679455\n",
      "val Cost 1.03939 val accuracy 0.645\n",
      "Epoch 29, CIFAR-10 Batch 2:  train Cost 0.896347 train accuracy 0.710396\n",
      "val Cost 1.00384 val accuracy 0.6584\n",
      "Epoch 29, CIFAR-10 Batch 3:  train Cost 0.883186 train accuracy 0.694307\n",
      "val Cost 1.07235 val accuracy 0.64\n",
      "Epoch 29, CIFAR-10 Batch 4:  train Cost 0.867477 train accuracy 0.707921\n",
      "val Cost 1.00265 val accuracy 0.6566\n",
      "Epoch 29, CIFAR-10 Batch 5:  train Cost 0.850392 train accuracy 0.714109\n",
      "val Cost 1.01546 val accuracy 0.6544\n",
      "Epoch 30, CIFAR-10 Batch 1:  train Cost 0.85879 train accuracy 0.69802\n",
      "val Cost 1.01068 val accuracy 0.6512\n",
      "Epoch 30, CIFAR-10 Batch 2:  train Cost 0.867 train accuracy 0.715347\n",
      "val Cost 0.974314 val accuracy 0.6686\n",
      "Epoch 30, CIFAR-10 Batch 3:  train Cost 0.841081 train accuracy 0.704208\n",
      "val Cost 1.04549 val accuracy 0.6482\n",
      "Epoch 30, CIFAR-10 Batch 4:  train Cost 0.823803 train accuracy 0.716584\n",
      "val Cost 0.97961 val accuracy 0.6688\n",
      "Epoch 30, CIFAR-10 Batch 5:  train Cost 0.813958 train accuracy 0.719059\n",
      "val Cost 0.992865 val accuracy 0.6628\n",
      "Epoch 31, CIFAR-10 Batch 1:  train Cost 0.835946 train accuracy 0.694307\n",
      "val Cost 1.00488 val accuracy 0.6566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, CIFAR-10 Batch 2:  train Cost 0.849542 train accuracy 0.725248\n",
      "val Cost 0.981782 val accuracy 0.6672\n",
      "Epoch 31, CIFAR-10 Batch 3:  train Cost 0.816782 train accuracy 0.707921\n",
      "val Cost 1.03182 val accuracy 0.6596\n",
      "Epoch 31, CIFAR-10 Batch 4:  train Cost 0.822413 train accuracy 0.712871\n",
      "val Cost 0.984519 val accuracy 0.6676\n",
      "Epoch 31, CIFAR-10 Batch 5:  train Cost 0.808926 train accuracy 0.738861\n",
      "val Cost 1.01185 val accuracy 0.66\n",
      "Epoch 32, CIFAR-10 Batch 1:  train Cost 0.81952 train accuracy 0.69802\n",
      "val Cost 1.00429 val accuracy 0.6516\n",
      "Epoch 32, CIFAR-10 Batch 2:  train Cost 0.837586 train accuracy 0.72896\n",
      "val Cost 0.973931 val accuracy 0.6694\n",
      "Epoch 32, CIFAR-10 Batch 3:  train Cost 0.823633 train accuracy 0.721535\n",
      "val Cost 1.03995 val accuracy 0.653\n",
      "Epoch 32, CIFAR-10 Batch 4:  train Cost 0.801719 train accuracy 0.720297\n",
      "val Cost 0.967511 val accuracy 0.6716\n",
      "Epoch 32, CIFAR-10 Batch 5:  train Cost 0.785236 train accuracy 0.743812\n",
      "val Cost 0.984425 val accuracy 0.6716\n",
      "Epoch 33, CIFAR-10 Batch 1:  train Cost 0.8015 train accuracy 0.704208\n",
      "val Cost 0.99036 val accuracy 0.661\n",
      "Epoch 33, CIFAR-10 Batch 2:  train Cost 0.807484 train accuracy 0.732673\n",
      "val Cost 0.952576 val accuracy 0.6772\n",
      "Epoch 33, CIFAR-10 Batch 3:  train Cost 0.827689 train accuracy 0.715347\n",
      "val Cost 1.04139 val accuracy 0.6556\n",
      "Epoch 33, CIFAR-10 Batch 4:  train Cost 0.823377 train accuracy 0.72401\n",
      "val Cost 0.99707 val accuracy 0.662\n",
      "Epoch 33, CIFAR-10 Batch 5:  train Cost 0.774701 train accuracy 0.751238\n",
      "val Cost 0.980125 val accuracy 0.671\n",
      "Epoch 34, CIFAR-10 Batch 1:  train Cost 0.767301 train accuracy 0.731436\n",
      "val Cost 0.960203 val accuracy 0.673\n",
      "Epoch 34, CIFAR-10 Batch 2:  train Cost 0.855653 train accuracy 0.736386\n",
      "val Cost 1.03051 val accuracy 0.654\n",
      "Epoch 34, CIFAR-10 Batch 3:  train Cost 0.812589 train accuracy 0.722772\n",
      "val Cost 1.04984 val accuracy 0.6498\n",
      "Epoch 34, CIFAR-10 Batch 4:  train Cost 0.809876 train accuracy 0.709158\n",
      "val Cost 0.985194 val accuracy 0.6712\n",
      "Epoch 34, CIFAR-10 Batch 5:  train Cost 0.741734 train accuracy 0.761139\n",
      "val Cost 0.950153 val accuracy 0.6774\n",
      "Epoch 35, CIFAR-10 Batch 1:  train Cost 0.754756 train accuracy 0.736386\n",
      "val Cost 0.95881 val accuracy 0.6792\n",
      "Epoch 35, CIFAR-10 Batch 2:  train Cost 0.792586 train accuracy 0.742574\n",
      "val Cost 0.957936 val accuracy 0.6766\n",
      "Epoch 35, CIFAR-10 Batch 3:  train Cost 0.753822 train accuracy 0.741337\n",
      "val Cost 0.987849 val accuracy 0.6748\n",
      "Epoch 35, CIFAR-10 Batch 4:  train Cost 0.76732 train accuracy 0.733911\n",
      "val Cost 0.953869 val accuracy 0.674\n",
      "Epoch 35, CIFAR-10 Batch 5:  train Cost 0.730374 train accuracy 0.754951\n",
      "val Cost 0.954506 val accuracy 0.68\n",
      "Epoch 36, CIFAR-10 Batch 1:  train Cost 0.723099 train accuracy 0.746287\n",
      "val Cost 0.942144 val accuracy 0.6814\n",
      "Epoch 36, CIFAR-10 Batch 2:  train Cost 0.767789 train accuracy 0.745049\n",
      "val Cost 0.94439 val accuracy 0.682\n",
      "Epoch 36, CIFAR-10 Batch 3:  train Cost 0.754953 train accuracy 0.748762\n",
      "val Cost 1.01224 val accuracy 0.663\n",
      "Epoch 36, CIFAR-10 Batch 4:  train Cost 0.75322 train accuracy 0.748762\n",
      "val Cost 0.951117 val accuracy 0.6824\n",
      "Epoch 36, CIFAR-10 Batch 5:  train Cost 0.708786 train accuracy 0.766089\n",
      "val Cost 0.940173 val accuracy 0.6808\n",
      "Epoch 37, CIFAR-10 Batch 1:  train Cost 0.709326 train accuracy 0.751238\n",
      "val Cost 0.934987 val accuracy 0.6788\n",
      "Epoch 37, CIFAR-10 Batch 2:  train Cost 0.765755 train accuracy 0.753713\n",
      "val Cost 0.96874 val accuracy 0.6788\n",
      "Epoch 37, CIFAR-10 Batch 3:  train Cost 0.738564 train accuracy 0.748762\n",
      "val Cost 0.996873 val accuracy 0.6728\n",
      "Epoch 37, CIFAR-10 Batch 4:  train Cost 0.756048 train accuracy 0.737624\n",
      "val Cost 0.953336 val accuracy 0.6796\n",
      "Epoch 37, CIFAR-10 Batch 5:  train Cost 0.689004 train accuracy 0.772277\n",
      "val Cost 0.928076 val accuracy 0.6858\n",
      "Epoch 38, CIFAR-10 Batch 1:  train Cost 0.692403 train accuracy 0.753713\n",
      "val Cost 0.923113 val accuracy 0.6888\n",
      "Epoch 38, CIFAR-10 Batch 2:  train Cost 0.77532 train accuracy 0.756188\n",
      "val Cost 0.964447 val accuracy 0.6792\n",
      "Epoch 38, CIFAR-10 Batch 3:  train Cost 0.71085 train accuracy 0.756188\n",
      "val Cost 0.980815 val accuracy 0.6774\n",
      "Epoch 38, CIFAR-10 Batch 4:  train Cost 0.726445 train accuracy 0.757426\n",
      "val Cost 0.942731 val accuracy 0.6832\n",
      "Epoch 38, CIFAR-10 Batch 5:  train Cost 0.671326 train accuracy 0.785891\n",
      "val Cost 0.922111 val accuracy 0.6862\n",
      "Epoch 39, CIFAR-10 Batch 1:  train Cost 0.665902 train accuracy 0.763614\n",
      "val Cost 0.910036 val accuracy 0.6878\n",
      "Epoch 39, CIFAR-10 Batch 2:  train Cost 0.736165 train accuracy 0.766089\n",
      "val Cost 0.954001 val accuracy 0.6762\n",
      "Epoch 39, CIFAR-10 Batch 3:  train Cost 0.701038 train accuracy 0.768564\n",
      "val Cost 0.984818 val accuracy 0.6802\n",
      "Epoch 39, CIFAR-10 Batch 4:  train Cost 0.738944 train accuracy 0.752475\n",
      "val Cost 0.956648 val accuracy 0.678\n",
      "Epoch 39, CIFAR-10 Batch 5:  train Cost 0.650308 train accuracy 0.784653\n",
      "val Cost 0.904329 val accuracy 0.69\n",
      "Epoch 40, CIFAR-10 Batch 1:  train Cost 0.660021 train accuracy 0.774752\n",
      "val Cost 0.911235 val accuracy 0.6938\n",
      "Epoch 40, CIFAR-10 Batch 2:  train Cost 0.706059 train accuracy 0.772277\n",
      "val Cost 0.936714 val accuracy 0.6874\n",
      "Epoch 40, CIFAR-10 Batch 3:  train Cost 0.665998 train accuracy 0.77599\n",
      "val Cost 0.951997 val accuracy 0.6868\n",
      "Epoch 40, CIFAR-10 Batch 4:  train Cost 0.722462 train accuracy 0.746287\n",
      "val Cost 0.953335 val accuracy 0.679\n",
      "Epoch 40, CIFAR-10 Batch 5:  train Cost 0.658942 train accuracy 0.777228\n",
      "val Cost 0.917262 val accuracy 0.692\n",
      "Epoch 41, CIFAR-10 Batch 1:  train Cost 0.650725 train accuracy 0.779703\n",
      "val Cost 0.906363 val accuracy 0.6938\n",
      "Epoch 41, CIFAR-10 Batch 2:  train Cost 0.695186 train accuracy 0.782178\n",
      "val Cost 0.928496 val accuracy 0.6856\n",
      "Epoch 41, CIFAR-10 Batch 3:  train Cost 0.662978 train accuracy 0.785891\n",
      "val Cost 0.960197 val accuracy 0.6882\n",
      "Epoch 41, CIFAR-10 Batch 4:  train Cost 0.714071 train accuracy 0.767327\n",
      "val Cost 0.944175 val accuracy 0.684\n",
      "Epoch 41, CIFAR-10 Batch 5:  train Cost 0.635364 train accuracy 0.794554\n",
      "val Cost 0.905502 val accuracy 0.6962\n",
      "Epoch 42, CIFAR-10 Batch 1:  train Cost 0.640689 train accuracy 0.779703\n",
      "val Cost 0.902949 val accuracy 0.694\n",
      "Epoch 42, CIFAR-10 Batch 2:  train Cost 0.68805 train accuracy 0.790842\n",
      "val Cost 0.922776 val accuracy 0.6882\n",
      "Epoch 42, CIFAR-10 Batch 3:  train Cost 0.697492 train accuracy 0.754951\n",
      "val Cost 1.00836 val accuracy 0.6786\n",
      "Epoch 42, CIFAR-10 Batch 4:  train Cost 0.697456 train accuracy 0.764851\n",
      "val Cost 0.949077 val accuracy 0.6762\n",
      "Epoch 42, CIFAR-10 Batch 5:  train Cost 0.650584 train accuracy 0.782178\n",
      "val Cost 0.912196 val accuracy 0.6968\n",
      "Epoch 43, CIFAR-10 Batch 1:  train Cost 0.614757 train accuracy 0.795792\n",
      "val Cost 0.895605 val accuracy 0.6952\n",
      "Epoch 43, CIFAR-10 Batch 2:  train Cost 0.66824 train accuracy 0.799505\n",
      "val Cost 0.906159 val accuracy 0.7\n",
      "Epoch 43, CIFAR-10 Batch 3:  train Cost 0.653551 train accuracy 0.777228\n",
      "val Cost 0.965789 val accuracy 0.6892\n",
      "Epoch 43, CIFAR-10 Batch 4:  train Cost 0.657156 train accuracy 0.789604\n",
      "val Cost 0.903987 val accuracy 0.6996\n",
      "Epoch 43, CIFAR-10 Batch 5:  train Cost 0.630042 train accuracy 0.795792\n",
      "val Cost 0.912713 val accuracy 0.693\n",
      "Epoch 44, CIFAR-10 Batch 1:  train Cost 0.608422 train accuracy 0.790842\n",
      "val Cost 0.893717 val accuracy 0.7056\n",
      "Epoch 44, CIFAR-10 Batch 2:  train Cost 0.658244 train accuracy 0.79703\n",
      "val Cost 0.905591 val accuracy 0.6994\n",
      "Epoch 44, CIFAR-10 Batch 3:  train Cost 0.617615 train accuracy 0.788366\n",
      "val Cost 0.929903 val accuracy 0.6928\n",
      "Epoch 44, CIFAR-10 Batch 4:  train Cost 0.638298 train accuracy 0.782178\n",
      "val Cost 0.900279 val accuracy 0.6962\n",
      "Epoch 44, CIFAR-10 Batch 5:  train Cost 0.66797 train accuracy 0.780941\n",
      "val Cost 0.966979 val accuracy 0.6828\n",
      "Epoch 45, CIFAR-10 Batch 1:  train Cost 0.588179 train accuracy 0.800743\n",
      "val Cost 0.884333 val accuracy 0.7018\n",
      "Epoch 45, CIFAR-10 Batch 2:  train Cost 0.627769 train accuracy 0.805693\n",
      "val Cost 0.890966 val accuracy 0.6982\n",
      "Epoch 45, CIFAR-10 Batch 3:  train Cost 0.622282 train accuracy 0.798267\n",
      "val Cost 0.945723 val accuracy 0.6892\n",
      "Epoch 45, CIFAR-10 Batch 4:  train Cost 0.632205 train accuracy 0.787129\n",
      "val Cost 0.899658 val accuracy 0.6968\n",
      "Epoch 45, CIFAR-10 Batch 5:  train Cost 0.617608 train accuracy 0.80198\n",
      "val Cost 0.918521 val accuracy 0.6974\n",
      "Epoch 46, CIFAR-10 Batch 1:  train Cost 0.575647 train accuracy 0.804455\n",
      "val Cost 0.882282 val accuracy 0.7112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, CIFAR-10 Batch 2:  train Cost 0.588282 train accuracy 0.820545\n",
      "val Cost 0.862953 val accuracy 0.709\n",
      "Epoch 46, CIFAR-10 Batch 3:  train Cost 0.606404 train accuracy 0.789604\n",
      "val Cost 0.927181 val accuracy 0.6962\n",
      "Epoch 46, CIFAR-10 Batch 4:  train Cost 0.601644 train accuracy 0.804455\n",
      "val Cost 0.871599 val accuracy 0.7084\n",
      "Epoch 46, CIFAR-10 Batch 5:  train Cost 0.568754 train accuracy 0.810644\n",
      "val Cost 0.882799 val accuracy 0.7072\n",
      "Epoch 47, CIFAR-10 Batch 1:  train Cost 0.562857 train accuracy 0.810644\n",
      "val Cost 0.887741 val accuracy 0.704\n",
      "Epoch 47, CIFAR-10 Batch 2:  train Cost 0.608194 train accuracy 0.79703\n",
      "val Cost 0.883233 val accuracy 0.7036\n",
      "Epoch 47, CIFAR-10 Batch 3:  train Cost 0.578711 train accuracy 0.79703\n",
      "val Cost 0.897747 val accuracy 0.7036\n",
      "Epoch 47, CIFAR-10 Batch 4:  train Cost 0.578822 train accuracy 0.805693\n",
      "val Cost 0.850795 val accuracy 0.7096\n",
      "Epoch 47, CIFAR-10 Batch 5:  train Cost 0.571956 train accuracy 0.805693\n",
      "val Cost 0.879084 val accuracy 0.7046\n",
      "Epoch 48, CIFAR-10 Batch 1:  train Cost 0.571045 train accuracy 0.794554\n",
      "val Cost 0.883698 val accuracy 0.7078\n",
      "Epoch 48, CIFAR-10 Batch 2:  train Cost 0.578246 train accuracy 0.804455\n",
      "val Cost 0.853935 val accuracy 0.7152\n",
      "Epoch 48, CIFAR-10 Batch 3:  train Cost 0.606913 train accuracy 0.792079\n",
      "val Cost 0.954283 val accuracy 0.686\n",
      "Epoch 48, CIFAR-10 Batch 4:  train Cost 0.562692 train accuracy 0.818069\n",
      "val Cost 0.848897 val accuracy 0.7086\n",
      "Epoch 48, CIFAR-10 Batch 5:  train Cost 0.541316 train accuracy 0.808168\n",
      "val Cost 0.860856 val accuracy 0.7114\n",
      "Epoch 49, CIFAR-10 Batch 1:  train Cost 0.545522 train accuracy 0.809406\n",
      "val Cost 0.862404 val accuracy 0.7134\n",
      "Epoch 49, CIFAR-10 Batch 2:  train Cost 0.546028 train accuracy 0.824257\n",
      "val Cost 0.840787 val accuracy 0.7206\n",
      "Epoch 49, CIFAR-10 Batch 3:  train Cost 0.592309 train accuracy 0.789604\n",
      "val Cost 0.92849 val accuracy 0.6952\n",
      "Epoch 49, CIFAR-10 Batch 4:  train Cost 0.557772 train accuracy 0.806931\n",
      "val Cost 0.843209 val accuracy 0.714\n",
      "Epoch 49, CIFAR-10 Batch 5:  train Cost 0.52447 train accuracy 0.815594\n",
      "val Cost 0.833234 val accuracy 0.723\n",
      "Epoch 50, CIFAR-10 Batch 1:  train Cost 0.54865 train accuracy 0.800743\n",
      "val Cost 0.874146 val accuracy 0.7116\n",
      "Epoch 50, CIFAR-10 Batch 2:  train Cost 0.539111 train accuracy 0.831683\n",
      "val Cost 0.831241 val accuracy 0.725\n",
      "Epoch 50, CIFAR-10 Batch 3:  train Cost 0.599025 train accuracy 0.787129\n",
      "val Cost 0.94548 val accuracy 0.6952\n",
      "Epoch 50, CIFAR-10 Batch 4:  train Cost 0.52654 train accuracy 0.82302\n",
      "val Cost 0.822845 val accuracy 0.7246\n",
      "Epoch 50, CIFAR-10 Batch 5:  train Cost 0.514885 train accuracy 0.814356\n",
      "val Cost 0.833198 val accuracy 0.7192\n",
      "Epoch 51, CIFAR-10 Batch 1:  train Cost 0.520222 train accuracy 0.836634\n",
      "val Cost 0.865271 val accuracy 0.7138\n",
      "Epoch 51, CIFAR-10 Batch 2:  train Cost 0.533644 train accuracy 0.829208\n",
      "val Cost 0.840046 val accuracy 0.7178\n",
      "Epoch 51, CIFAR-10 Batch 3:  train Cost 0.56582 train accuracy 0.814356\n",
      "val Cost 0.90413 val accuracy 0.7068\n",
      "Epoch 51, CIFAR-10 Batch 4:  train Cost 0.524105 train accuracy 0.82302\n",
      "val Cost 0.837547 val accuracy 0.7184\n",
      "Epoch 51, CIFAR-10 Batch 5:  train Cost 0.498792 train accuracy 0.834158\n",
      "val Cost 0.828292 val accuracy 0.7272\n",
      "Epoch 52, CIFAR-10 Batch 1:  train Cost 0.52595 train accuracy 0.825495\n",
      "val Cost 0.851952 val accuracy 0.7184\n",
      "Epoch 52, CIFAR-10 Batch 2:  train Cost 0.500637 train accuracy 0.842822\n",
      "val Cost 0.825412 val accuracy 0.7252\n",
      "Epoch 52, CIFAR-10 Batch 3:  train Cost 0.54387 train accuracy 0.808168\n",
      "val Cost 0.906605 val accuracy 0.7088\n",
      "Epoch 52, CIFAR-10 Batch 4:  train Cost 0.50535 train accuracy 0.830446\n",
      "val Cost 0.82274 val accuracy 0.7266\n",
      "Epoch 52, CIFAR-10 Batch 5:  train Cost 0.467775 train accuracy 0.839109\n",
      "val Cost 0.833551 val accuracy 0.7284\n",
      "Epoch 53, CIFAR-10 Batch 1:  train Cost 0.496702 train accuracy 0.831683\n",
      "val Cost 0.838697 val accuracy 0.7194\n",
      "Epoch 53, CIFAR-10 Batch 2:  train Cost 0.482882 train accuracy 0.851485\n",
      "val Cost 0.811204 val accuracy 0.7246\n",
      "Epoch 53, CIFAR-10 Batch 3:  train Cost 0.572934 train accuracy 0.803218\n",
      "val Cost 0.935725 val accuracy 0.693\n",
      "Epoch 53, CIFAR-10 Batch 4:  train Cost 0.509748 train accuracy 0.829208\n",
      "val Cost 0.836913 val accuracy 0.72\n",
      "Epoch 53, CIFAR-10 Batch 5:  train Cost 0.472429 train accuracy 0.835396\n",
      "val Cost 0.830098 val accuracy 0.7264\n",
      "Epoch 54, CIFAR-10 Batch 1:  train Cost 0.478072 train accuracy 0.845297\n",
      "val Cost 0.830816 val accuracy 0.727\n",
      "Epoch 54, CIFAR-10 Batch 2:  train Cost 0.501949 train accuracy 0.835396\n",
      "val Cost 0.836523 val accuracy 0.7214\n",
      "Epoch 54, CIFAR-10 Batch 3:  train Cost 0.485101 train accuracy 0.832921\n",
      "val Cost 0.852227 val accuracy 0.723\n",
      "Epoch 54, CIFAR-10 Batch 4:  train Cost 0.496775 train accuracy 0.834158\n",
      "val Cost 0.836214 val accuracy 0.7222\n",
      "Epoch 54, CIFAR-10 Batch 5:  train Cost 0.442634 train accuracy 0.845297\n",
      "val Cost 0.794919 val accuracy 0.7358\n",
      "Epoch 55, CIFAR-10 Batch 1:  train Cost 0.467383 train accuracy 0.844059\n",
      "val Cost 0.832485 val accuracy 0.729\n",
      "Epoch 55, CIFAR-10 Batch 2:  train Cost 0.484034 train accuracy 0.844059\n",
      "val Cost 0.815301 val accuracy 0.7346\n",
      "Epoch 55, CIFAR-10 Batch 3:  train Cost 0.489816 train accuracy 0.831683\n",
      "val Cost 0.858471 val accuracy 0.723\n",
      "Epoch 55, CIFAR-10 Batch 4:  train Cost 0.476956 train accuracy 0.845297\n",
      "val Cost 0.82865 val accuracy 0.7274\n",
      "Epoch 55, CIFAR-10 Batch 5:  train Cost 0.425506 train accuracy 0.860149\n",
      "val Cost 0.797873 val accuracy 0.7342\n",
      "Epoch 56, CIFAR-10 Batch 1:  train Cost 0.478707 train accuracy 0.831683\n",
      "val Cost 0.859485 val accuracy 0.7216\n",
      "Epoch 56, CIFAR-10 Batch 2:  train Cost 0.453587 train accuracy 0.85396\n",
      "val Cost 0.796289 val accuracy 0.735\n",
      "Epoch 56, CIFAR-10 Batch 3:  train Cost 0.480126 train accuracy 0.836634\n",
      "val Cost 0.849563 val accuracy 0.7228\n",
      "Epoch 56, CIFAR-10 Batch 4:  train Cost 0.465439 train accuracy 0.840347\n",
      "val Cost 0.804808 val accuracy 0.738\n",
      "Epoch 56, CIFAR-10 Batch 5:  train Cost 0.417864 train accuracy 0.868812\n",
      "val Cost 0.792478 val accuracy 0.74\n",
      "Epoch 57, CIFAR-10 Batch 1:  train Cost 0.463078 train accuracy 0.834158\n",
      "val Cost 0.846673 val accuracy 0.7226\n",
      "Epoch 57, CIFAR-10 Batch 2:  train Cost 0.460687 train accuracy 0.85396\n",
      "val Cost 0.799859 val accuracy 0.7362\n",
      "Epoch 57, CIFAR-10 Batch 3:  train Cost 0.491452 train accuracy 0.841584\n",
      "val Cost 0.866585 val accuracy 0.7154\n",
      "Epoch 57, CIFAR-10 Batch 4:  train Cost 0.441783 train accuracy 0.858911\n",
      "val Cost 0.804579 val accuracy 0.7364\n",
      "Epoch 57, CIFAR-10 Batch 5:  train Cost 0.406686 train accuracy 0.868812\n",
      "val Cost 0.797043 val accuracy 0.7428\n",
      "Epoch 58, CIFAR-10 Batch 1:  train Cost 0.437433 train accuracy 0.844059\n",
      "val Cost 0.831992 val accuracy 0.7312\n",
      "Epoch 58, CIFAR-10 Batch 2:  train Cost 0.437278 train accuracy 0.861386\n",
      "val Cost 0.792224 val accuracy 0.7364\n",
      "Epoch 58, CIFAR-10 Batch 3:  train Cost 0.463618 train accuracy 0.84901\n",
      "val Cost 0.845905 val accuracy 0.7196\n",
      "Epoch 58, CIFAR-10 Batch 4:  train Cost 0.442557 train accuracy 0.844059\n",
      "val Cost 0.801012 val accuracy 0.7396\n",
      "Epoch 58, CIFAR-10 Batch 5:  train Cost 0.38874 train accuracy 0.87995\n",
      "val Cost 0.784117 val accuracy 0.7404\n",
      "Epoch 59, CIFAR-10 Batch 1:  train Cost 0.451456 train accuracy 0.841584\n",
      "val Cost 0.852972 val accuracy 0.7282\n",
      "Epoch 59, CIFAR-10 Batch 2:  train Cost 0.448144 train accuracy 0.85396\n",
      "val Cost 0.830551 val accuracy 0.7266\n",
      "Epoch 59, CIFAR-10 Batch 3:  train Cost 0.434659 train accuracy 0.85396\n",
      "val Cost 0.830853 val accuracy 0.7332\n",
      "Epoch 59, CIFAR-10 Batch 4:  train Cost 0.439975 train accuracy 0.85396\n",
      "val Cost 0.817158 val accuracy 0.7332\n",
      "Epoch 59, CIFAR-10 Batch 5:  train Cost 0.38594 train accuracy 0.872525\n",
      "val Cost 0.786291 val accuracy 0.7468\n",
      "Epoch 60, CIFAR-10 Batch 1:  train Cost 0.442184 train accuracy 0.851485\n",
      "val Cost 0.842215 val accuracy 0.7274\n",
      "Epoch 60, CIFAR-10 Batch 2:  train Cost 0.433925 train accuracy 0.863861\n",
      "val Cost 0.803603 val accuracy 0.7342\n",
      "Epoch 60, CIFAR-10 Batch 3:  train Cost 0.438341 train accuracy 0.852723\n",
      "val Cost 0.820906 val accuracy 0.733\n",
      "Epoch 60, CIFAR-10 Batch 4:  train Cost 0.411365 train accuracy 0.867574\n",
      "val Cost 0.788267 val accuracy 0.7438\n",
      "Epoch 60, CIFAR-10 Batch 5:  train Cost 0.364107 train accuracy 0.893564\n",
      "val Cost 0.769494 val accuracy 0.7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, CIFAR-10 Batch 1:  train Cost 0.431037 train accuracy 0.85396\n",
      "val Cost 0.842791 val accuracy 0.7276\n",
      "Epoch 61, CIFAR-10 Batch 2:  train Cost 0.413239 train accuracy 0.868812\n",
      "val Cost 0.796847 val accuracy 0.7328\n",
      "Epoch 61, CIFAR-10 Batch 3:  train Cost 0.437455 train accuracy 0.856436\n",
      "val Cost 0.830032 val accuracy 0.7274\n",
      "Epoch 61, CIFAR-10 Batch 4:  train Cost 0.407941 train accuracy 0.862624\n",
      "val Cost 0.813843 val accuracy 0.7388\n",
      "Epoch 61, CIFAR-10 Batch 5:  train Cost 0.376853 train accuracy 0.886139\n",
      "val Cost 0.783506 val accuracy 0.749\n",
      "Epoch 62, CIFAR-10 Batch 1:  train Cost 0.385723 train accuracy 0.876238\n",
      "val Cost 0.817586 val accuracy 0.7392\n",
      "Epoch 62, CIFAR-10 Batch 2:  train Cost 0.405001 train accuracy 0.872525\n",
      "val Cost 0.797614 val accuracy 0.7356\n",
      "Epoch 62, CIFAR-10 Batch 3:  train Cost 0.436482 train accuracy 0.84901\n",
      "val Cost 0.847492 val accuracy 0.7324\n",
      "Epoch 62, CIFAR-10 Batch 4:  train Cost 0.417054 train accuracy 0.857673\n",
      "val Cost 0.813822 val accuracy 0.7336\n",
      "Epoch 62, CIFAR-10 Batch 5:  train Cost 0.356633 train accuracy 0.891089\n",
      "val Cost 0.77735 val accuracy 0.743\n",
      "Epoch 63, CIFAR-10 Batch 1:  train Cost 0.389402 train accuracy 0.876238\n",
      "val Cost 0.823082 val accuracy 0.7382\n",
      "Epoch 63, CIFAR-10 Batch 2:  train Cost 0.399613 train accuracy 0.877475\n",
      "val Cost 0.791054 val accuracy 0.7416\n",
      "Epoch 63, CIFAR-10 Batch 3:  train Cost 0.411705 train accuracy 0.871287\n",
      "val Cost 0.812629 val accuracy 0.7378\n",
      "Epoch 63, CIFAR-10 Batch 4:  train Cost 0.40828 train accuracy 0.861386\n",
      "val Cost 0.815095 val accuracy 0.7384\n",
      "Epoch 63, CIFAR-10 Batch 5:  train Cost 0.380011 train accuracy 0.871287\n",
      "val Cost 0.796072 val accuracy 0.7422\n",
      "Epoch 64, CIFAR-10 Batch 1:  train Cost 0.364298 train accuracy 0.882426\n",
      "val Cost 0.810771 val accuracy 0.7364\n",
      "Epoch 64, CIFAR-10 Batch 2:  train Cost 0.380553 train accuracy 0.892327\n",
      "val Cost 0.776333 val accuracy 0.748\n",
      "Epoch 64, CIFAR-10 Batch 3:  train Cost 0.401661 train accuracy 0.862624\n",
      "val Cost 0.831063 val accuracy 0.7322\n",
      "Epoch 64, CIFAR-10 Batch 4:  train Cost 0.394819 train accuracy 0.872525\n",
      "val Cost 0.800576 val accuracy 0.7398\n",
      "Epoch 64, CIFAR-10 Batch 5:  train Cost 0.343983 train accuracy 0.898515\n",
      "val Cost 0.765524 val accuracy 0.7502\n",
      "Epoch 65, CIFAR-10 Batch 1:  train Cost 0.361865 train accuracy 0.881188\n",
      "val Cost 0.821314 val accuracy 0.7406\n",
      "Epoch 65, CIFAR-10 Batch 2:  train Cost 0.381313 train accuracy 0.887376\n",
      "val Cost 0.789806 val accuracy 0.7422\n",
      "Epoch 65, CIFAR-10 Batch 3:  train Cost 0.386459 train accuracy 0.870049\n",
      "val Cost 0.823443 val accuracy 0.7328\n",
      "Epoch 65, CIFAR-10 Batch 4:  train Cost 0.379243 train accuracy 0.876238\n",
      "val Cost 0.819578 val accuracy 0.7366\n",
      "Epoch 65, CIFAR-10 Batch 5:  train Cost 0.338206 train accuracy 0.904703\n",
      "val Cost 0.778683 val accuracy 0.7492\n",
      "Epoch 66, CIFAR-10 Batch 1:  train Cost 0.351403 train accuracy 0.884901\n",
      "val Cost 0.81545 val accuracy 0.7382\n",
      "Epoch 66, CIFAR-10 Batch 2:  train Cost 0.352373 train accuracy 0.898515\n",
      "val Cost 0.789514 val accuracy 0.744\n",
      "Epoch 66, CIFAR-10 Batch 3:  train Cost 0.400744 train accuracy 0.871287\n",
      "val Cost 0.829915 val accuracy 0.7384\n",
      "Epoch 66, CIFAR-10 Batch 4:  train Cost 0.386255 train accuracy 0.865099\n",
      "val Cost 0.830836 val accuracy 0.7376\n",
      "Epoch 66, CIFAR-10 Batch 5:  train Cost 0.337658 train accuracy 0.892327\n",
      "val Cost 0.765268 val accuracy 0.7494\n",
      "Epoch 67, CIFAR-10 Batch 1:  train Cost 0.357011 train accuracy 0.881188\n",
      "val Cost 0.830652 val accuracy 0.7456\n",
      "Epoch 67, CIFAR-10 Batch 2:  train Cost 0.351712 train accuracy 0.908416\n",
      "val Cost 0.787144 val accuracy 0.7496\n",
      "Epoch 67, CIFAR-10 Batch 3:  train Cost 0.340848 train accuracy 0.893564\n",
      "val Cost 0.797943 val accuracy 0.7444\n",
      "Epoch 67, CIFAR-10 Batch 4:  train Cost 0.352071 train accuracy 0.883663\n",
      "val Cost 0.796669 val accuracy 0.7422\n",
      "Epoch 67, CIFAR-10 Batch 5:  train Cost 0.364186 train accuracy 0.886139\n",
      "val Cost 0.801324 val accuracy 0.7428\n",
      "Epoch 68, CIFAR-10 Batch 1:  train Cost 0.384925 train accuracy 0.876238\n",
      "val Cost 0.855739 val accuracy 0.734\n",
      "Epoch 68, CIFAR-10 Batch 2:  train Cost 0.375207 train accuracy 0.882426\n",
      "val Cost 0.797079 val accuracy 0.7412\n",
      "Epoch 68, CIFAR-10 Batch 3:  train Cost 0.352024 train accuracy 0.884901\n",
      "val Cost 0.803988 val accuracy 0.7446\n",
      "Epoch 68, CIFAR-10 Batch 4:  train Cost 0.346787 train accuracy 0.893564\n",
      "val Cost 0.797449 val accuracy 0.7434\n",
      "Epoch 68, CIFAR-10 Batch 5:  train Cost 0.331578 train accuracy 0.903465\n",
      "val Cost 0.781413 val accuracy 0.7506\n",
      "Epoch 69, CIFAR-10 Batch 1:  train Cost 0.335582 train accuracy 0.891089\n",
      "val Cost 0.809501 val accuracy 0.7464\n",
      "Epoch 69, CIFAR-10 Batch 2:  train Cost 0.345446 train accuracy 0.887376\n",
      "val Cost 0.796974 val accuracy 0.7418\n",
      "Epoch 69, CIFAR-10 Batch 3:  train Cost 0.345032 train accuracy 0.893564\n",
      "val Cost 0.808693 val accuracy 0.7438\n",
      "Epoch 69, CIFAR-10 Batch 4:  train Cost 0.35578 train accuracy 0.887376\n",
      "val Cost 0.798929 val accuracy 0.7466\n",
      "Epoch 69, CIFAR-10 Batch 5:  train Cost 0.347443 train accuracy 0.893564\n",
      "val Cost 0.789221 val accuracy 0.7456\n",
      "Epoch 70, CIFAR-10 Batch 1:  train Cost 0.322557 train accuracy 0.902228\n",
      "val Cost 0.804204 val accuracy 0.7502\n",
      "Epoch 70, CIFAR-10 Batch 2:  train Cost 0.335689 train accuracy 0.908416\n",
      "val Cost 0.789716 val accuracy 0.7432\n",
      "Epoch 70, CIFAR-10 Batch 3:  train Cost 0.335014 train accuracy 0.89604\n",
      "val Cost 0.804637 val accuracy 0.7456\n",
      "Epoch 70, CIFAR-10 Batch 4:  train Cost 0.333817 train accuracy 0.897277\n",
      "val Cost 0.798284 val accuracy 0.7444\n",
      "Epoch 70, CIFAR-10 Batch 5:  train Cost 0.343278 train accuracy 0.894802\n",
      "val Cost 0.801423 val accuracy 0.7456\n",
      "Epoch 71, CIFAR-10 Batch 1:  train Cost 0.320286 train accuracy 0.898515\n",
      "val Cost 0.810891 val accuracy 0.7486\n",
      "Epoch 71, CIFAR-10 Batch 2:  train Cost 0.313913 train accuracy 0.908416\n",
      "val Cost 0.784953 val accuracy 0.7454\n",
      "Epoch 71, CIFAR-10 Batch 3:  train Cost 0.34078 train accuracy 0.893564\n",
      "val Cost 0.812948 val accuracy 0.7434\n",
      "Epoch 71, CIFAR-10 Batch 4:  train Cost 0.339093 train accuracy 0.903465\n",
      "val Cost 0.814015 val accuracy 0.739\n",
      "Epoch 71, CIFAR-10 Batch 5:  train Cost 0.361825 train accuracy 0.87995\n",
      "val Cost 0.83211 val accuracy 0.7368\n",
      "Epoch 72, CIFAR-10 Batch 1:  train Cost 0.343926 train accuracy 0.892327\n",
      "val Cost 0.83339 val accuracy 0.7426\n",
      "Epoch 72, CIFAR-10 Batch 2:  train Cost 0.330347 train accuracy 0.907178\n",
      "val Cost 0.787385 val accuracy 0.7444\n",
      "Epoch 72, CIFAR-10 Batch 3:  train Cost 0.313529 train accuracy 0.909653\n",
      "val Cost 0.799098 val accuracy 0.7498\n",
      "Epoch 72, CIFAR-10 Batch 4:  train Cost 0.326784 train accuracy 0.90099\n",
      "val Cost 0.793429 val accuracy 0.7496\n",
      "Epoch 72, CIFAR-10 Batch 5:  train Cost 0.339387 train accuracy 0.886138\n",
      "val Cost 0.793943 val accuracy 0.748\n",
      "Epoch 73, CIFAR-10 Batch 1:  train Cost 0.359109 train accuracy 0.87995\n",
      "val Cost 0.856767 val accuracy 0.7348\n",
      "Epoch 73, CIFAR-10 Batch 2:  train Cost 0.319316 train accuracy 0.90594\n",
      "val Cost 0.78213 val accuracy 0.7498\n",
      "Epoch 73, CIFAR-10 Batch 3:  train Cost 0.309828 train accuracy 0.902228\n",
      "val Cost 0.795029 val accuracy 0.743\n",
      "Epoch 73, CIFAR-10 Batch 4:  train Cost 0.322104 train accuracy 0.907178\n",
      "val Cost 0.797247 val accuracy 0.7468\n",
      "Epoch 73, CIFAR-10 Batch 5:  train Cost 0.329959 train accuracy 0.891089\n",
      "val Cost 0.798767 val accuracy 0.7442\n",
      "Epoch 74, CIFAR-10 Batch 1:  train Cost 0.342862 train accuracy 0.878713\n",
      "val Cost 0.876045 val accuracy 0.7322\n",
      "Epoch 74, CIFAR-10 Batch 2:  train Cost 0.314137 train accuracy 0.913366\n",
      "val Cost 0.770708 val accuracy 0.75\n",
      "Epoch 74, CIFAR-10 Batch 3:  train Cost 0.313176 train accuracy 0.905941\n",
      "val Cost 0.812193 val accuracy 0.7474\n",
      "Epoch 74, CIFAR-10 Batch 4:  train Cost 0.297217 train accuracy 0.913366\n",
      "val Cost 0.787111 val accuracy 0.7526\n",
      "Epoch 74, CIFAR-10 Batch 5:  train Cost 0.332235 train accuracy 0.891089\n",
      "val Cost 0.808977 val accuracy 0.7448\n",
      "Epoch 75, CIFAR-10 Batch 1:  train Cost 0.313991 train accuracy 0.90099\n",
      "val Cost 0.836338 val accuracy 0.7422\n",
      "Epoch 75, CIFAR-10 Batch 2:  train Cost 0.318343 train accuracy 0.903465\n",
      "val Cost 0.790064 val accuracy 0.75\n",
      "Epoch 75, CIFAR-10 Batch 3:  train Cost 0.301754 train accuracy 0.909653\n",
      "val Cost 0.806403 val accuracy 0.7506\n",
      "Epoch 75, CIFAR-10 Batch 4:  train Cost 0.284813 train accuracy 0.909653\n",
      "val Cost 0.781203 val accuracy 0.7548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, CIFAR-10 Batch 5:  train Cost 0.31173 train accuracy 0.903465\n",
      "val Cost 0.790037 val accuracy 0.7488\n",
      "Epoch 76, CIFAR-10 Batch 1:  train Cost 0.301928 train accuracy 0.908416\n",
      "val Cost 0.855951 val accuracy 0.738\n",
      "Epoch 76, CIFAR-10 Batch 2:  train Cost 0.333016 train accuracy 0.894802\n",
      "val Cost 0.810789 val accuracy 0.743\n",
      "Epoch 76, CIFAR-10 Batch 3:  train Cost 0.310192 train accuracy 0.899752\n",
      "val Cost 0.806894 val accuracy 0.7446\n",
      "Epoch 76, CIFAR-10 Batch 4:  train Cost 0.294685 train accuracy 0.907178\n",
      "val Cost 0.797596 val accuracy 0.7486\n",
      "Epoch 76, CIFAR-10 Batch 5:  train Cost 0.332771 train accuracy 0.89604\n",
      "val Cost 0.838571 val accuracy 0.7332\n",
      "Epoch 77, CIFAR-10 Batch 1:  train Cost 0.327921 train accuracy 0.894802\n",
      "val Cost 0.877638 val accuracy 0.7312\n",
      "Epoch 77, CIFAR-10 Batch 2:  train Cost 0.282733 train accuracy 0.910891\n",
      "val Cost 0.772461 val accuracy 0.7502\n",
      "Epoch 77, CIFAR-10 Batch 3:  train Cost 0.301591 train accuracy 0.897277\n",
      "val Cost 0.810155 val accuracy 0.7506\n",
      "Epoch 77, CIFAR-10 Batch 4:  train Cost 0.296397 train accuracy 0.923267\n",
      "val Cost 0.786089 val accuracy 0.7502\n",
      "Epoch 77, CIFAR-10 Batch 5:  train Cost 0.286116 train accuracy 0.908416\n",
      "val Cost 0.793628 val accuracy 0.7514\n",
      "Epoch 78, CIFAR-10 Batch 1:  train Cost 0.303981 train accuracy 0.898515\n",
      "val Cost 0.836618 val accuracy 0.742\n",
      "Epoch 78, CIFAR-10 Batch 2:  train Cost 0.284022 train accuracy 0.919554\n",
      "val Cost 0.792941 val accuracy 0.7456\n",
      "Epoch 78, CIFAR-10 Batch 3:  train Cost 0.277995 train accuracy 0.918317\n",
      "val Cost 0.799769 val accuracy 0.7486\n",
      "Epoch 78, CIFAR-10 Batch 4:  train Cost 0.282282 train accuracy 0.915842\n",
      "val Cost 0.796124 val accuracy 0.748\n",
      "Epoch 78, CIFAR-10 Batch 5:  train Cost 0.305258 train accuracy 0.904703\n",
      "val Cost 0.797145 val accuracy 0.7442\n",
      "Epoch 79, CIFAR-10 Batch 1:  train Cost 0.288661 train accuracy 0.917079\n",
      "val Cost 0.852058 val accuracy 0.7348\n",
      "Epoch 79, CIFAR-10 Batch 2:  train Cost 0.282521 train accuracy 0.919554\n",
      "val Cost 0.801105 val accuracy 0.75\n",
      "Epoch 79, CIFAR-10 Batch 3:  train Cost 0.27882 train accuracy 0.915842\n",
      "val Cost 0.798644 val accuracy 0.7528\n",
      "Epoch 79, CIFAR-10 Batch 4:  train Cost 0.249503 train accuracy 0.930693\n",
      "val Cost 0.767626 val accuracy 0.7592\n",
      "Epoch 79, CIFAR-10 Batch 5:  train Cost 0.310096 train accuracy 0.904703\n",
      "val Cost 0.829093 val accuracy 0.739\n",
      "Epoch 80, CIFAR-10 Batch 1:  train Cost 0.275016 train accuracy 0.918317\n",
      "val Cost 0.819544 val accuracy 0.744\n",
      "Epoch 80, CIFAR-10 Batch 2:  train Cost 0.281024 train accuracy 0.919554\n",
      "val Cost 0.797279 val accuracy 0.7482\n",
      "Epoch 80, CIFAR-10 Batch 3:  train Cost 0.279148 train accuracy 0.919554\n",
      "val Cost 0.810687 val accuracy 0.7498\n",
      "Epoch 80, CIFAR-10 Batch 4:  train Cost 0.273367 train accuracy 0.918317\n",
      "val Cost 0.80365 val accuracy 0.7494\n",
      "Epoch 80, CIFAR-10 Batch 5:  train Cost 0.286614 train accuracy 0.918317\n",
      "val Cost 0.801997 val accuracy 0.7504\n",
      "Epoch 81, CIFAR-10 Batch 1:  train Cost 0.266904 train accuracy 0.924505\n",
      "val Cost 0.812865 val accuracy 0.7494\n",
      "Epoch 81, CIFAR-10 Batch 2:  train Cost 0.272044 train accuracy 0.920792\n",
      "val Cost 0.779336 val accuracy 0.756\n",
      "Epoch 81, CIFAR-10 Batch 3:  train Cost 0.275781 train accuracy 0.914604\n",
      "val Cost 0.799604 val accuracy 0.7544\n",
      "Epoch 81, CIFAR-10 Batch 4:  train Cost 0.261378 train accuracy 0.92203\n",
      "val Cost 0.777093 val accuracy 0.7558\n",
      "Epoch 81, CIFAR-10 Batch 5:  train Cost 0.269049 train accuracy 0.917079\n",
      "val Cost 0.804812 val accuracy 0.7454\n",
      "Epoch 82, CIFAR-10 Batch 1:  train Cost 0.260451 train accuracy 0.920792\n",
      "val Cost 0.809607 val accuracy 0.7486\n",
      "Epoch 82, CIFAR-10 Batch 2:  train Cost 0.258952 train accuracy 0.930693\n",
      "val Cost 0.772138 val accuracy 0.7582\n",
      "Epoch 82, CIFAR-10 Batch 3:  train Cost 0.271856 train accuracy 0.913366\n",
      "val Cost 0.804433 val accuracy 0.756\n",
      "Epoch 82, CIFAR-10 Batch 4:  train Cost 0.228991 train accuracy 0.931931\n",
      "val Cost 0.763904 val accuracy 0.7612\n",
      "Epoch 82, CIFAR-10 Batch 5:  train Cost 0.275428 train accuracy 0.915842\n",
      "val Cost 0.815761 val accuracy 0.7486\n",
      "Epoch 83, CIFAR-10 Batch 1:  train Cost 0.271786 train accuracy 0.914604\n",
      "val Cost 0.826051 val accuracy 0.7446\n",
      "Epoch 83, CIFAR-10 Batch 2:  train Cost 0.290992 train accuracy 0.902228\n",
      "val Cost 0.835573 val accuracy 0.7426\n",
      "Epoch 83, CIFAR-10 Batch 3:  train Cost 0.273289 train accuracy 0.912129\n",
      "val Cost 0.785928 val accuracy 0.7532\n",
      "Epoch 83, CIFAR-10 Batch 4:  train Cost 0.233444 train accuracy 0.933168\n",
      "val Cost 0.784773 val accuracy 0.755\n",
      "Epoch 83, CIFAR-10 Batch 5:  train Cost 0.241043 train accuracy 0.930693\n",
      "val Cost 0.785058 val accuracy 0.7558\n",
      "Epoch 84, CIFAR-10 Batch 1:  train Cost 0.237667 train accuracy 0.925743\n",
      "val Cost 0.79093 val accuracy 0.7544\n",
      "Epoch 84, CIFAR-10 Batch 2:  train Cost 0.250401 train accuracy 0.924505\n",
      "val Cost 0.784147 val accuracy 0.7582\n",
      "Epoch 84, CIFAR-10 Batch 3:  train Cost 0.276025 train accuracy 0.92203\n",
      "val Cost 0.819641 val accuracy 0.749\n",
      "Epoch 84, CIFAR-10 Batch 4:  train Cost 0.239566 train accuracy 0.929455\n",
      "val Cost 0.775581 val accuracy 0.7568\n",
      "Epoch 84, CIFAR-10 Batch 5:  train Cost 0.266842 train accuracy 0.918317\n",
      "val Cost 0.797451 val accuracy 0.7518\n",
      "Epoch 85, CIFAR-10 Batch 1:  train Cost 0.258775 train accuracy 0.917079\n",
      "val Cost 0.820079 val accuracy 0.7462\n",
      "Epoch 85, CIFAR-10 Batch 2:  train Cost 0.240104 train accuracy 0.935643\n",
      "val Cost 0.77951 val accuracy 0.7554\n",
      "Epoch 85, CIFAR-10 Batch 3:  train Cost 0.251658 train accuracy 0.924505\n",
      "val Cost 0.80735 val accuracy 0.7548\n",
      "Epoch 85, CIFAR-10 Batch 4:  train Cost 0.206095 train accuracy 0.946782\n",
      "val Cost 0.763868 val accuracy 0.765\n",
      "Epoch 85, CIFAR-10 Batch 5:  train Cost 0.250511 train accuracy 0.931931\n",
      "val Cost 0.792869 val accuracy 0.7534\n",
      "Epoch 86, CIFAR-10 Batch 1:  train Cost 0.238993 train accuracy 0.925743\n",
      "val Cost 0.802225 val accuracy 0.7522\n",
      "Epoch 86, CIFAR-10 Batch 2:  train Cost 0.249881 train accuracy 0.924505\n",
      "val Cost 0.78602 val accuracy 0.7536\n",
      "Epoch 86, CIFAR-10 Batch 3:  train Cost 0.265849 train accuracy 0.918317\n",
      "val Cost 0.809872 val accuracy 0.7526\n",
      "Epoch 86, CIFAR-10 Batch 4:  train Cost 0.208807 train accuracy 0.94802\n",
      "val Cost 0.747525 val accuracy 0.7654\n",
      "Epoch 86, CIFAR-10 Batch 5:  train Cost 0.264622 train accuracy 0.907178\n",
      "val Cost 0.81559 val accuracy 0.75\n",
      "Epoch 87, CIFAR-10 Batch 1:  train Cost 0.24716 train accuracy 0.933168\n",
      "val Cost 0.808333 val accuracy 0.7478\n",
      "Epoch 87, CIFAR-10 Batch 2:  train Cost 0.270777 train accuracy 0.917079\n",
      "val Cost 0.816136 val accuracy 0.7494\n",
      "Epoch 87, CIFAR-10 Batch 3:  train Cost 0.257266 train accuracy 0.92698\n",
      "val Cost 0.817105 val accuracy 0.753\n",
      "Epoch 87, CIFAR-10 Batch 4:  train Cost 0.218241 train accuracy 0.945545\n",
      "val Cost 0.748843 val accuracy 0.766\n",
      "Epoch 87, CIFAR-10 Batch 5:  train Cost 0.22996 train accuracy 0.935643\n",
      "val Cost 0.791782 val accuracy 0.7564\n",
      "Epoch 88, CIFAR-10 Batch 1:  train Cost 0.23506 train accuracy 0.930693\n",
      "val Cost 0.813551 val accuracy 0.7496\n",
      "Epoch 88, CIFAR-10 Batch 2:  train Cost 0.236506 train accuracy 0.923267\n",
      "val Cost 0.795689 val accuracy 0.7584\n",
      "Epoch 88, CIFAR-10 Batch 3:  train Cost 0.232425 train accuracy 0.936881\n",
      "val Cost 0.792601 val accuracy 0.7594\n",
      "Epoch 88, CIFAR-10 Batch 4:  train Cost 0.192171 train accuracy 0.95297\n",
      "val Cost 0.76456 val accuracy 0.7606\n",
      "Epoch 88, CIFAR-10 Batch 5:  train Cost 0.232811 train accuracy 0.928218\n",
      "val Cost 0.784685 val accuracy 0.7596\n",
      "Epoch 89, CIFAR-10 Batch 1:  train Cost 0.240724 train accuracy 0.931931\n",
      "val Cost 0.821686 val accuracy 0.7494\n",
      "Epoch 89, CIFAR-10 Batch 2:  train Cost 0.222177 train accuracy 0.935644\n",
      "val Cost 0.774426 val accuracy 0.7588\n",
      "Epoch 89, CIFAR-10 Batch 3:  train Cost 0.251717 train accuracy 0.918317\n",
      "val Cost 0.841549 val accuracy 0.748\n",
      "Epoch 89, CIFAR-10 Batch 4:  train Cost 0.216506 train accuracy 0.955446\n",
      "val Cost 0.78118 val accuracy 0.7626\n",
      "Epoch 89, CIFAR-10 Batch 5:  train Cost 0.217791 train accuracy 0.943069\n",
      "val Cost 0.785875 val accuracy 0.7582\n",
      "Epoch 90, CIFAR-10 Batch 1:  train Cost 0.214206 train accuracy 0.941832\n",
      "val Cost 0.786972 val accuracy 0.7504\n",
      "Epoch 90, CIFAR-10 Batch 2:  train Cost 0.24156 train accuracy 0.928218\n",
      "val Cost 0.799183 val accuracy 0.7566\n",
      "Epoch 90, CIFAR-10 Batch 3:  train Cost 0.248006 train accuracy 0.924505\n",
      "val Cost 0.827303 val accuracy 0.7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, CIFAR-10 Batch 4:  train Cost 0.215056 train accuracy 0.949257\n",
      "val Cost 0.783105 val accuracy 0.7534\n",
      "Epoch 90, CIFAR-10 Batch 5:  train Cost 0.209959 train accuracy 0.945545\n",
      "val Cost 0.785698 val accuracy 0.7564\n",
      "Epoch 91, CIFAR-10 Batch 1:  train Cost 0.223921 train accuracy 0.930693\n",
      "val Cost 0.80249 val accuracy 0.7532\n",
      "Epoch 91, CIFAR-10 Batch 2:  train Cost 0.241911 train accuracy 0.931931\n",
      "val Cost 0.803708 val accuracy 0.756\n",
      "Epoch 91, CIFAR-10 Batch 3:  train Cost 0.224787 train accuracy 0.931931\n",
      "val Cost 0.813974 val accuracy 0.7542\n",
      "Epoch 91, CIFAR-10 Batch 4:  train Cost 0.189152 train accuracy 0.960396\n",
      "val Cost 0.760285 val accuracy 0.7658\n",
      "Epoch 91, CIFAR-10 Batch 5:  train Cost 0.242102 train accuracy 0.92698\n",
      "val Cost 0.811568 val accuracy 0.7542\n",
      "Epoch 92, CIFAR-10 Batch 1:  train Cost 0.210309 train accuracy 0.938119\n",
      "val Cost 0.801605 val accuracy 0.7586\n",
      "Epoch 92, CIFAR-10 Batch 2:  train Cost 0.224426 train accuracy 0.935643\n",
      "val Cost 0.794398 val accuracy 0.7546\n",
      "Epoch 92, CIFAR-10 Batch 3:  train Cost 0.218506 train accuracy 0.944307\n",
      "val Cost 0.791938 val accuracy 0.7572\n",
      "Epoch 92, CIFAR-10 Batch 4:  train Cost 0.192273 train accuracy 0.951733\n",
      "val Cost 0.781254 val accuracy 0.7648\n",
      "Epoch 92, CIFAR-10 Batch 5:  train Cost 0.211793 train accuracy 0.943069\n",
      "val Cost 0.786437 val accuracy 0.7568\n",
      "Epoch 93, CIFAR-10 Batch 1:  train Cost 0.218668 train accuracy 0.938119\n",
      "val Cost 0.809495 val accuracy 0.7482\n",
      "Epoch 93, CIFAR-10 Batch 2:  train Cost 0.222566 train accuracy 0.935643\n",
      "val Cost 0.784432 val accuracy 0.7558\n",
      "Epoch 93, CIFAR-10 Batch 3:  train Cost 0.247616 train accuracy 0.938119\n",
      "val Cost 0.855004 val accuracy 0.7478\n",
      "Epoch 93, CIFAR-10 Batch 4:  train Cost 0.181435 train accuracy 0.960396\n",
      "val Cost 0.759062 val accuracy 0.7652\n",
      "Epoch 93, CIFAR-10 Batch 5:  train Cost 0.207357 train accuracy 0.949257\n",
      "val Cost 0.76884 val accuracy 0.7606\n",
      "Epoch 94, CIFAR-10 Batch 1:  train Cost 0.22005 train accuracy 0.930693\n",
      "val Cost 0.816374 val accuracy 0.7492\n",
      "Epoch 94, CIFAR-10 Batch 2:  train Cost 0.219579 train accuracy 0.939356\n",
      "val Cost 0.779465 val accuracy 0.757\n",
      "Epoch 94, CIFAR-10 Batch 3:  train Cost 0.210998 train accuracy 0.943069\n",
      "val Cost 0.785716 val accuracy 0.7604\n",
      "Epoch 94, CIFAR-10 Batch 4:  train Cost 0.174963 train accuracy 0.964109\n",
      "val Cost 0.752149 val accuracy 0.7658\n",
      "Epoch 94, CIFAR-10 Batch 5:  train Cost 0.222859 train accuracy 0.935643\n",
      "val Cost 0.782359 val accuracy 0.7574\n",
      "Epoch 95, CIFAR-10 Batch 1:  train Cost 0.196054 train accuracy 0.946782\n",
      "val Cost 0.78311 val accuracy 0.7628\n",
      "Epoch 95, CIFAR-10 Batch 2:  train Cost 0.204677 train accuracy 0.94802\n",
      "val Cost 0.78057 val accuracy 0.7624\n",
      "Epoch 95, CIFAR-10 Batch 3:  train Cost 0.231275 train accuracy 0.931931\n",
      "val Cost 0.822282 val accuracy 0.7498\n",
      "Epoch 95, CIFAR-10 Batch 4:  train Cost 0.16288 train accuracy 0.967822\n",
      "val Cost 0.781685 val accuracy 0.7648\n",
      "Epoch 95, CIFAR-10 Batch 5:  train Cost 0.185878 train accuracy 0.957921\n",
      "val Cost 0.762324 val accuracy 0.7624\n",
      "Epoch 96, CIFAR-10 Batch 1:  train Cost 0.185737 train accuracy 0.949257\n",
      "val Cost 0.771659 val accuracy 0.7606\n",
      "Epoch 96, CIFAR-10 Batch 2:  train Cost 0.206087 train accuracy 0.936881\n",
      "val Cost 0.778699 val accuracy 0.7574\n",
      "Epoch 96, CIFAR-10 Batch 3:  train Cost 0.242787 train accuracy 0.930693\n",
      "val Cost 0.83977 val accuracy 0.7434\n",
      "Epoch 96, CIFAR-10 Batch 4:  train Cost 0.179746 train accuracy 0.960396\n",
      "val Cost 0.767869 val accuracy 0.7646\n",
      "Epoch 96, CIFAR-10 Batch 5:  train Cost 0.186379 train accuracy 0.95297\n",
      "val Cost 0.776226 val accuracy 0.76\n",
      "Epoch 97, CIFAR-10 Batch 1:  train Cost 0.19526 train accuracy 0.95297\n",
      "val Cost 0.789596 val accuracy 0.7568\n",
      "Epoch 97, CIFAR-10 Batch 2:  train Cost 0.199069 train accuracy 0.950495\n",
      "val Cost 0.798778 val accuracy 0.7588\n",
      "Epoch 97, CIFAR-10 Batch 3:  train Cost 0.233127 train accuracy 0.92203\n",
      "val Cost 0.840763 val accuracy 0.7434\n",
      "Epoch 97, CIFAR-10 Batch 4:  train Cost 0.1854 train accuracy 0.957921\n",
      "val Cost 0.786057 val accuracy 0.7622\n",
      "Epoch 97, CIFAR-10 Batch 5:  train Cost 0.182091 train accuracy 0.955446\n",
      "val Cost 0.761492 val accuracy 0.761\n",
      "Epoch 98, CIFAR-10 Batch 1:  train Cost 0.222529 train accuracy 0.92698\n",
      "val Cost 0.811348 val accuracy 0.7598\n",
      "Epoch 98, CIFAR-10 Batch 2:  train Cost 0.191286 train accuracy 0.949257\n",
      "val Cost 0.773427 val accuracy 0.765\n",
      "Epoch 98, CIFAR-10 Batch 3:  train Cost 0.216109 train accuracy 0.939356\n",
      "val Cost 0.816841 val accuracy 0.759\n",
      "Epoch 98, CIFAR-10 Batch 4:  train Cost 0.157047 train accuracy 0.975248\n",
      "val Cost 0.753199 val accuracy 0.7702\n",
      "Epoch 98, CIFAR-10 Batch 5:  train Cost 0.16991 train accuracy 0.965347\n",
      "val Cost 0.769467 val accuracy 0.7638\n",
      "Epoch 99, CIFAR-10 Batch 1:  train Cost 0.181239 train accuracy 0.95297\n",
      "val Cost 0.759567 val accuracy 0.765\n",
      "Epoch 99, CIFAR-10 Batch 2:  train Cost 0.194604 train accuracy 0.940594\n",
      "val Cost 0.800903 val accuracy 0.762\n",
      "Epoch 99, CIFAR-10 Batch 3:  train Cost 0.237126 train accuracy 0.928218\n",
      "val Cost 0.877537 val accuracy 0.7404\n",
      "Epoch 99, CIFAR-10 Batch 4:  train Cost 0.16725 train accuracy 0.965346\n",
      "val Cost 0.75443 val accuracy 0.77\n",
      "Epoch 99, CIFAR-10 Batch 5:  train Cost 0.175685 train accuracy 0.962871\n",
      "val Cost 0.790126 val accuracy 0.7608\n",
      "Epoch 100, CIFAR-10 Batch 1:  train Cost 0.174702 train accuracy 0.954208\n",
      "val Cost 0.770611 val accuracy 0.7664\n",
      "Epoch 100, CIFAR-10 Batch 2:  train Cost 0.18167 train accuracy 0.954208\n",
      "val Cost 0.780365 val accuracy 0.7662\n",
      "Epoch 100, CIFAR-10 Batch 3:  train Cost 0.224463 train accuracy 0.940594\n",
      "val Cost 0.849974 val accuracy 0.7474\n",
      "Epoch 100, CIFAR-10 Batch 4:  train Cost 0.155844 train accuracy 0.962871\n",
      "val Cost 0.758109 val accuracy 0.7704\n",
      "Epoch 100, CIFAR-10 Batch 5:  train Cost 0.154034 train accuracy 0.972772\n",
      "val Cost 0.752223 val accuracy 0.7664\n",
      "Epoch 101, CIFAR-10 Batch 1:  train Cost 0.177996 train accuracy 0.95297\n",
      "val Cost 0.755598 val accuracy 0.7668\n",
      "Epoch 101, CIFAR-10 Batch 2:  train Cost 0.177794 train accuracy 0.956683\n",
      "val Cost 0.771782 val accuracy 0.7594\n",
      "Epoch 101, CIFAR-10 Batch 3:  train Cost 0.215189 train accuracy 0.940594\n",
      "val Cost 0.844645 val accuracy 0.7514\n",
      "Epoch 101, CIFAR-10 Batch 4:  train Cost 0.161108 train accuracy 0.962871\n",
      "val Cost 0.781225 val accuracy 0.7634\n",
      "Epoch 101, CIFAR-10 Batch 5:  train Cost 0.155323 train accuracy 0.969059\n",
      "val Cost 0.758433 val accuracy 0.767\n",
      "Epoch 102, CIFAR-10 Batch 1:  train Cost 0.190683 train accuracy 0.945544\n",
      "val Cost 0.777377 val accuracy 0.7632\n",
      "Epoch 102, CIFAR-10 Batch 2:  train Cost 0.168932 train accuracy 0.956683\n",
      "val Cost 0.762517 val accuracy 0.7688\n",
      "Epoch 102, CIFAR-10 Batch 3:  train Cost 0.199523 train accuracy 0.95297\n",
      "val Cost 0.80836 val accuracy 0.7544\n",
      "Epoch 102, CIFAR-10 Batch 4:  train Cost 0.154577 train accuracy 0.964109\n",
      "val Cost 0.769022 val accuracy 0.7632\n",
      "Epoch 102, CIFAR-10 Batch 5:  train Cost 0.167888 train accuracy 0.965347\n",
      "val Cost 0.765512 val accuracy 0.762\n",
      "Epoch 103, CIFAR-10 Batch 1:  train Cost 0.180024 train accuracy 0.946782\n",
      "val Cost 0.789847 val accuracy 0.7574\n",
      "Epoch 103, CIFAR-10 Batch 2:  train Cost 0.177745 train accuracy 0.951733\n",
      "val Cost 0.796268 val accuracy 0.757\n",
      "Epoch 103, CIFAR-10 Batch 3:  train Cost 0.216091 train accuracy 0.941832\n",
      "val Cost 0.843738 val accuracy 0.7454\n",
      "Epoch 103, CIFAR-10 Batch 4:  train Cost 0.159536 train accuracy 0.964109\n",
      "val Cost 0.791345 val accuracy 0.7616\n",
      "Epoch 103, CIFAR-10 Batch 5:  train Cost 0.15779 train accuracy 0.965346\n",
      "val Cost 0.778442 val accuracy 0.7618\n",
      "Epoch 104, CIFAR-10 Batch 1:  train Cost 0.185863 train accuracy 0.949257\n",
      "val Cost 0.802637 val accuracy 0.7548\n",
      "Epoch 104, CIFAR-10 Batch 2:  train Cost 0.172487 train accuracy 0.960396\n",
      "val Cost 0.765119 val accuracy 0.7654\n",
      "Epoch 104, CIFAR-10 Batch 3:  train Cost 0.175272 train accuracy 0.95297\n",
      "val Cost 0.802369 val accuracy 0.7608\n",
      "Epoch 104, CIFAR-10 Batch 4:  train Cost 0.164018 train accuracy 0.965347\n",
      "val Cost 0.800945 val accuracy 0.763\n",
      "Epoch 104, CIFAR-10 Batch 5:  train Cost 0.168049 train accuracy 0.95297\n",
      "val Cost 0.80368 val accuracy 0.753\n",
      "Epoch 105, CIFAR-10 Batch 1:  train Cost 0.181384 train accuracy 0.945544\n",
      "val Cost 0.791561 val accuracy 0.7584\n",
      "Epoch 105, CIFAR-10 Batch 2:  train Cost 0.158723 train accuracy 0.959158\n",
      "val Cost 0.786081 val accuracy 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, CIFAR-10 Batch 3:  train Cost 0.220286 train accuracy 0.929455\n",
      "val Cost 0.870639 val accuracy 0.751\n",
      "Epoch 105, CIFAR-10 Batch 4:  train Cost 0.162371 train accuracy 0.964109\n",
      "val Cost 0.815278 val accuracy 0.7548\n",
      "Epoch 105, CIFAR-10 Batch 5:  train Cost 0.160912 train accuracy 0.962871\n",
      "val Cost 0.801383 val accuracy 0.7566\n",
      "Epoch 106, CIFAR-10 Batch 1:  train Cost 0.186761 train accuracy 0.941832\n",
      "val Cost 0.784872 val accuracy 0.763\n",
      "Epoch 106, CIFAR-10 Batch 2:  train Cost 0.165294 train accuracy 0.950495\n",
      "val Cost 0.796797 val accuracy 0.7644\n",
      "Epoch 106, CIFAR-10 Batch 3:  train Cost 0.206886 train accuracy 0.941832\n",
      "val Cost 0.821738 val accuracy 0.7502\n",
      "Epoch 106, CIFAR-10 Batch 4:  train Cost 0.150743 train accuracy 0.965346\n",
      "val Cost 0.780904 val accuracy 0.7662\n",
      "Epoch 106, CIFAR-10 Batch 5:  train Cost 0.158975 train accuracy 0.962871\n",
      "val Cost 0.779398 val accuracy 0.7592\n",
      "Epoch 107, CIFAR-10 Batch 1:  train Cost 0.176028 train accuracy 0.95297\n",
      "val Cost 0.766743 val accuracy 0.7662\n",
      "Epoch 107, CIFAR-10 Batch 2:  train Cost 0.180406 train accuracy 0.951733\n",
      "val Cost 0.800873 val accuracy 0.764\n",
      "Epoch 107, CIFAR-10 Batch 3:  train Cost 0.197436 train accuracy 0.941832\n",
      "val Cost 0.813428 val accuracy 0.7526\n",
      "Epoch 107, CIFAR-10 Batch 4:  train Cost 0.143758 train accuracy 0.970297\n",
      "val Cost 0.788156 val accuracy 0.7602\n",
      "Epoch 107, CIFAR-10 Batch 5:  train Cost 0.166226 train accuracy 0.961634\n",
      "val Cost 0.792755 val accuracy 0.7618\n",
      "Epoch 108, CIFAR-10 Batch 1:  train Cost 0.206961 train accuracy 0.934406\n",
      "val Cost 0.813067 val accuracy 0.7512\n",
      "Epoch 108, CIFAR-10 Batch 2:  train Cost 0.16527 train accuracy 0.960396\n",
      "val Cost 0.765909 val accuracy 0.7664\n",
      "Epoch 108, CIFAR-10 Batch 3:  train Cost 0.180942 train accuracy 0.956683\n",
      "val Cost 0.811773 val accuracy 0.7616\n",
      "Epoch 108, CIFAR-10 Batch 4:  train Cost 0.159735 train accuracy 0.965346\n",
      "val Cost 0.793723 val accuracy 0.7588\n",
      "Epoch 108, CIFAR-10 Batch 5:  train Cost 0.15901 train accuracy 0.956683\n",
      "val Cost 0.794876 val accuracy 0.761\n",
      "Epoch 109, CIFAR-10 Batch 1:  train Cost 0.172716 train accuracy 0.956683\n",
      "val Cost 0.780706 val accuracy 0.7638\n",
      "Epoch 109, CIFAR-10 Batch 2:  train Cost 0.176628 train accuracy 0.951733\n",
      "val Cost 0.791148 val accuracy 0.7598\n",
      "Epoch 109, CIFAR-10 Batch 3:  train Cost 0.225274 train accuracy 0.935643\n",
      "val Cost 0.874669 val accuracy 0.7456\n",
      "Epoch 109, CIFAR-10 Batch 4:  train Cost 0.14877 train accuracy 0.97401\n",
      "val Cost 0.785692 val accuracy 0.766\n",
      "Epoch 109, CIFAR-10 Batch 5:  train Cost 0.167643 train accuracy 0.955445\n",
      "val Cost 0.806798 val accuracy 0.755\n",
      "Epoch 110, CIFAR-10 Batch 1:  train Cost 0.164011 train accuracy 0.961634\n",
      "val Cost 0.77728 val accuracy 0.7642\n",
      "Epoch 110, CIFAR-10 Batch 2:  train Cost 0.18413 train accuracy 0.956683\n",
      "val Cost 0.80449 val accuracy 0.7548\n",
      "Epoch 110, CIFAR-10 Batch 3:  train Cost 0.235057 train accuracy 0.929455\n",
      "val Cost 0.873052 val accuracy 0.7438\n",
      "Epoch 110, CIFAR-10 Batch 4:  train Cost 0.149304 train accuracy 0.969059\n",
      "val Cost 0.795917 val accuracy 0.765\n",
      "Epoch 110, CIFAR-10 Batch 5:  train Cost 0.163763 train accuracy 0.967822\n",
      "val Cost 0.783163 val accuracy 0.76\n",
      "Epoch 111, CIFAR-10 Batch 1:  train Cost 0.167222 train accuracy 0.955446\n",
      "val Cost 0.800659 val accuracy 0.761\n",
      "Epoch 111, CIFAR-10 Batch 2:  train Cost 0.191822 train accuracy 0.945544\n",
      "val Cost 0.829595 val accuracy 0.7532\n",
      "Epoch 111, CIFAR-10 Batch 3:  train Cost 0.194461 train accuracy 0.956683\n",
      "val Cost 0.818333 val accuracy 0.7484\n",
      "Epoch 111, CIFAR-10 Batch 4:  train Cost 0.138322 train accuracy 0.967822\n",
      "val Cost 0.771967 val accuracy 0.7646\n",
      "Epoch 111, CIFAR-10 Batch 5:  train Cost 0.170407 train accuracy 0.95297\n",
      "val Cost 0.810481 val accuracy 0.7524\n",
      "Epoch 112, CIFAR-10 Batch 1:  train Cost 0.15802 train accuracy 0.962871\n",
      "val Cost 0.773349 val accuracy 0.7666\n",
      "Epoch 112, CIFAR-10 Batch 2:  train Cost 0.160544 train accuracy 0.964109\n",
      "val Cost 0.780331 val accuracy 0.7638\n",
      "Epoch 112, CIFAR-10 Batch 3:  train Cost 0.172717 train accuracy 0.957921\n",
      "val Cost 0.821151 val accuracy 0.7544\n",
      "Epoch 112, CIFAR-10 Batch 4:  train Cost 0.151293 train accuracy 0.966584\n",
      "val Cost 0.783663 val accuracy 0.7622\n",
      "Epoch 112, CIFAR-10 Batch 5:  train Cost 0.169425 train accuracy 0.95297\n",
      "val Cost 0.800346 val accuracy 0.7566\n",
      "Epoch 113, CIFAR-10 Batch 1:  train Cost 0.156295 train accuracy 0.959158\n",
      "val Cost 0.77033 val accuracy 0.7654\n",
      "Epoch 113, CIFAR-10 Batch 2:  train Cost 0.179086 train accuracy 0.955445\n",
      "val Cost 0.793414 val accuracy 0.7578\n",
      "Epoch 113, CIFAR-10 Batch 3:  train Cost 0.196892 train accuracy 0.944307\n",
      "val Cost 0.832588 val accuracy 0.7524\n",
      "Epoch 113, CIFAR-10 Batch 4:  train Cost 0.148742 train accuracy 0.966584\n",
      "val Cost 0.789035 val accuracy 0.7664\n",
      "Epoch 113, CIFAR-10 Batch 5:  train Cost 0.16443 train accuracy 0.957921\n",
      "val Cost 0.804775 val accuracy 0.7606\n",
      "Epoch 114, CIFAR-10 Batch 1:  train Cost 0.166522 train accuracy 0.960396\n",
      "val Cost 0.76125 val accuracy 0.7654\n",
      "Epoch 114, CIFAR-10 Batch 2:  train Cost 0.164635 train accuracy 0.960396\n",
      "val Cost 0.800929 val accuracy 0.7596\n",
      "Epoch 114, CIFAR-10 Batch 3:  train Cost 0.201128 train accuracy 0.94802\n",
      "val Cost 0.837283 val accuracy 0.7498\n",
      "Epoch 114, CIFAR-10 Batch 4:  train Cost 0.137743 train accuracy 0.967822\n",
      "val Cost 0.779751 val accuracy 0.7686\n",
      "Epoch 114, CIFAR-10 Batch 5:  train Cost 0.158007 train accuracy 0.957921\n",
      "val Cost 0.786794 val accuracy 0.7632\n",
      "Epoch 115, CIFAR-10 Batch 1:  train Cost 0.162107 train accuracy 0.949257\n",
      "val Cost 0.783422 val accuracy 0.7616\n",
      "Epoch 115, CIFAR-10 Batch 2:  train Cost 0.165005 train accuracy 0.960396\n",
      "val Cost 0.808997 val accuracy 0.7586\n",
      "Epoch 115, CIFAR-10 Batch 3:  train Cost 0.158525 train accuracy 0.961634\n",
      "val Cost 0.815185 val accuracy 0.7568\n",
      "Epoch 115, CIFAR-10 Batch 4:  train Cost 0.124613 train accuracy 0.97896\n",
      "val Cost 0.771291 val accuracy 0.768\n",
      "Epoch 115, CIFAR-10 Batch 5:  train Cost 0.161003 train accuracy 0.955445\n",
      "val Cost 0.801929 val accuracy 0.7578\n",
      "Epoch 116, CIFAR-10 Batch 1:  train Cost 0.14542 train accuracy 0.969059\n",
      "val Cost 0.776468 val accuracy 0.7678\n",
      "Epoch 116, CIFAR-10 Batch 2:  train Cost 0.132013 train accuracy 0.975248\n",
      "val Cost 0.76595 val accuracy 0.7708\n",
      "Epoch 116, CIFAR-10 Batch 3:  train Cost 0.164015 train accuracy 0.957921\n",
      "val Cost 0.821396 val accuracy 0.7506\n",
      "Epoch 116, CIFAR-10 Batch 4:  train Cost 0.132637 train accuracy 0.975248\n",
      "val Cost 0.776962 val accuracy 0.7626\n",
      "Epoch 116, CIFAR-10 Batch 5:  train Cost 0.144691 train accuracy 0.965347\n",
      "val Cost 0.800936 val accuracy 0.7604\n",
      "Epoch 117, CIFAR-10 Batch 1:  train Cost 0.14886 train accuracy 0.966584\n",
      "val Cost 0.762772 val accuracy 0.771\n",
      "Epoch 117, CIFAR-10 Batch 2:  train Cost 0.168444 train accuracy 0.951733\n",
      "val Cost 0.818459 val accuracy 0.7594\n",
      "Epoch 117, CIFAR-10 Batch 3:  train Cost 0.173797 train accuracy 0.95297\n",
      "val Cost 0.858276 val accuracy 0.7504\n",
      "Epoch 117, CIFAR-10 Batch 4:  train Cost 0.134504 train accuracy 0.971535\n",
      "val Cost 0.791421 val accuracy 0.7666\n",
      "Epoch 117, CIFAR-10 Batch 5:  train Cost 0.143166 train accuracy 0.966584\n",
      "val Cost 0.800028 val accuracy 0.7562\n",
      "Epoch 118, CIFAR-10 Batch 1:  train Cost 0.135073 train accuracy 0.965347\n",
      "val Cost 0.75163 val accuracy 0.77\n",
      "Epoch 118, CIFAR-10 Batch 2:  train Cost 0.142561 train accuracy 0.966584\n",
      "val Cost 0.79256 val accuracy 0.7652\n",
      "Epoch 118, CIFAR-10 Batch 3:  train Cost 0.174405 train accuracy 0.955445\n",
      "val Cost 0.825816 val accuracy 0.7536\n",
      "Epoch 118, CIFAR-10 Batch 4:  train Cost 0.130874 train accuracy 0.972772\n",
      "val Cost 0.791229 val accuracy 0.7652\n",
      "Epoch 118, CIFAR-10 Batch 5:  train Cost 0.142186 train accuracy 0.971535\n",
      "val Cost 0.788098 val accuracy 0.7646\n",
      "Epoch 119, CIFAR-10 Batch 1:  train Cost 0.144077 train accuracy 0.967822\n",
      "val Cost 0.759294 val accuracy 0.7666\n",
      "Epoch 119, CIFAR-10 Batch 2:  train Cost 0.13197 train accuracy 0.972772\n",
      "val Cost 0.800177 val accuracy 0.7658\n",
      "Epoch 119, CIFAR-10 Batch 3:  train Cost 0.173454 train accuracy 0.94802\n",
      "val Cost 0.840951 val accuracy 0.7526\n",
      "Epoch 119, CIFAR-10 Batch 4:  train Cost 0.154749 train accuracy 0.962871\n",
      "val Cost 0.803046 val accuracy 0.7556\n",
      "Epoch 119, CIFAR-10 Batch 5:  train Cost 0.139622 train accuracy 0.969059\n",
      "val Cost 0.801142 val accuracy 0.759\n",
      "Epoch 120, CIFAR-10 Batch 1:  train Cost 0.128859 train accuracy 0.971535\n",
      "val Cost 0.761124 val accuracy 0.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, CIFAR-10 Batch 2:  train Cost 0.141944 train accuracy 0.962871\n",
      "val Cost 0.779161 val accuracy 0.7694\n",
      "Epoch 120, CIFAR-10 Batch 3:  train Cost 0.143635 train accuracy 0.966584\n",
      "val Cost 0.796054 val accuracy 0.7636\n",
      "Epoch 120, CIFAR-10 Batch 4:  train Cost 0.144781 train accuracy 0.967822\n",
      "val Cost 0.811495 val accuracy 0.7638\n",
      "Epoch 120, CIFAR-10 Batch 5:  train Cost 0.149883 train accuracy 0.959158\n",
      "val Cost 0.829002 val accuracy 0.7576\n",
      "Epoch 121, CIFAR-10 Batch 1:  train Cost 0.127295 train accuracy 0.980198\n",
      "val Cost 0.746689 val accuracy 0.7772\n",
      "Epoch 121, CIFAR-10 Batch 2:  train Cost 0.144661 train accuracy 0.972772\n",
      "val Cost 0.79092 val accuracy 0.7656\n",
      "Epoch 121, CIFAR-10 Batch 3:  train Cost 0.159304 train accuracy 0.960396\n",
      "val Cost 0.802845 val accuracy 0.757\n",
      "Epoch 121, CIFAR-10 Batch 4:  train Cost 0.118947 train accuracy 0.976485\n",
      "val Cost 0.770906 val accuracy 0.7694\n",
      "Epoch 121, CIFAR-10 Batch 5:  train Cost 0.127215 train accuracy 0.97401\n",
      "val Cost 0.797874 val accuracy 0.759\n",
      "Epoch 122, CIFAR-10 Batch 1:  train Cost 0.136184 train accuracy 0.969059\n",
      "val Cost 0.755702 val accuracy 0.77\n",
      "Epoch 122, CIFAR-10 Batch 2:  train Cost 0.157067 train accuracy 0.954208\n",
      "val Cost 0.810919 val accuracy 0.7626\n",
      "Epoch 122, CIFAR-10 Batch 3:  train Cost 0.162699 train accuracy 0.964109\n",
      "val Cost 0.818124 val accuracy 0.7552\n",
      "Epoch 122, CIFAR-10 Batch 4:  train Cost 0.120968 train accuracy 0.981436\n",
      "val Cost 0.789479 val accuracy 0.7656\n",
      "Epoch 122, CIFAR-10 Batch 5:  train Cost 0.143355 train accuracy 0.957921\n",
      "val Cost 0.812057 val accuracy 0.7566\n",
      "Epoch 123, CIFAR-10 Batch 1:  train Cost 0.137616 train accuracy 0.976485\n",
      "val Cost 0.772765 val accuracy 0.7684\n",
      "Epoch 123, CIFAR-10 Batch 2:  train Cost 0.137675 train accuracy 0.975248\n",
      "val Cost 0.796904 val accuracy 0.7646\n",
      "Epoch 123, CIFAR-10 Batch 3:  train Cost 0.153512 train accuracy 0.961634\n",
      "val Cost 0.837761 val accuracy 0.7534\n",
      "Epoch 123, CIFAR-10 Batch 4:  train Cost 0.130754 train accuracy 0.97401\n",
      "val Cost 0.792272 val accuracy 0.7644\n",
      "Epoch 123, CIFAR-10 Batch 5:  train Cost 0.131084 train accuracy 0.966584\n",
      "val Cost 0.811434 val accuracy 0.763\n",
      "Epoch 124, CIFAR-10 Batch 1:  train Cost 0.125015 train accuracy 0.972772\n",
      "val Cost 0.745967 val accuracy 0.7778\n",
      "Epoch 124, CIFAR-10 Batch 2:  train Cost 0.141809 train accuracy 0.967822\n",
      "val Cost 0.805052 val accuracy 0.7642\n",
      "Epoch 124, CIFAR-10 Batch 3:  train Cost 0.167249 train accuracy 0.960396\n",
      "val Cost 0.83742 val accuracy 0.7516\n",
      "Epoch 124, CIFAR-10 Batch 4:  train Cost 0.118675 train accuracy 0.975248\n",
      "val Cost 0.790873 val accuracy 0.768\n",
      "Epoch 124, CIFAR-10 Batch 5:  train Cost 0.113971 train accuracy 0.97896\n",
      "val Cost 0.785913 val accuracy 0.7656\n",
      "Epoch 125, CIFAR-10 Batch 1:  train Cost 0.119379 train accuracy 0.977723\n",
      "val Cost 0.74149 val accuracy 0.7768\n",
      "Epoch 125, CIFAR-10 Batch 2:  train Cost 0.121398 train accuracy 0.972772\n",
      "val Cost 0.787042 val accuracy 0.7674\n",
      "Epoch 125, CIFAR-10 Batch 3:  train Cost 0.135485 train accuracy 0.965347\n",
      "val Cost 0.815288 val accuracy 0.7596\n",
      "Epoch 125, CIFAR-10 Batch 4:  train Cost 0.111203 train accuracy 0.977723\n",
      "val Cost 0.76007 val accuracy 0.7768\n",
      "Epoch 125, CIFAR-10 Batch 5:  train Cost 0.125459 train accuracy 0.971535\n",
      "val Cost 0.802038 val accuracy 0.766\n",
      "Epoch 126, CIFAR-10 Batch 1:  train Cost 0.126587 train accuracy 0.972772\n",
      "val Cost 0.746658 val accuracy 0.7758\n",
      "Epoch 126, CIFAR-10 Batch 2:  train Cost 0.123024 train accuracy 0.975248\n",
      "val Cost 0.797988 val accuracy 0.7642\n",
      "Epoch 126, CIFAR-10 Batch 3:  train Cost 0.13884 train accuracy 0.970297\n",
      "val Cost 0.790328 val accuracy 0.7672\n",
      "Epoch 126, CIFAR-10 Batch 4:  train Cost 0.107306 train accuracy 0.980198\n",
      "val Cost 0.77664 val accuracy 0.7698\n",
      "Epoch 126, CIFAR-10 Batch 5:  train Cost 0.105564 train accuracy 0.980198\n",
      "val Cost 0.768752 val accuracy 0.7696\n",
      "Epoch 127, CIFAR-10 Batch 1:  train Cost 0.118852 train accuracy 0.977723\n",
      "val Cost 0.740285 val accuracy 0.7774\n",
      "Epoch 127, CIFAR-10 Batch 2:  train Cost 0.116529 train accuracy 0.981436\n",
      "val Cost 0.762589 val accuracy 0.7688\n",
      "Epoch 127, CIFAR-10 Batch 3:  train Cost 0.137547 train accuracy 0.967822\n",
      "val Cost 0.804612 val accuracy 0.7654\n",
      "Epoch 127, CIFAR-10 Batch 4:  train Cost 0.10722 train accuracy 0.976485\n",
      "val Cost 0.761385 val accuracy 0.7744\n",
      "Epoch 127, CIFAR-10 Batch 5:  train Cost 0.100377 train accuracy 0.982673\n",
      "val Cost 0.778034 val accuracy 0.7722\n",
      "Epoch 128, CIFAR-10 Batch 1:  train Cost 0.122892 train accuracy 0.966584\n",
      "val Cost 0.756932 val accuracy 0.7774\n",
      "Epoch 128, CIFAR-10 Batch 2:  train Cost 0.121444 train accuracy 0.977723\n",
      "val Cost 0.78194 val accuracy 0.7716\n",
      "Epoch 128, CIFAR-10 Batch 3:  train Cost 0.125683 train accuracy 0.977723\n",
      "val Cost 0.791385 val accuracy 0.7644\n",
      "Epoch 128, CIFAR-10 Batch 4:  train Cost 0.0994999 train accuracy 0.982673\n",
      "val Cost 0.764429 val accuracy 0.7716\n",
      "Epoch 128, CIFAR-10 Batch 5:  train Cost 0.109254 train accuracy 0.977723\n",
      "val Cost 0.777507 val accuracy 0.7666\n",
      "Epoch 129, CIFAR-10 Batch 1:  train Cost 0.113379 train accuracy 0.972772\n",
      "val Cost 0.748971 val accuracy 0.7738\n",
      "Epoch 129, CIFAR-10 Batch 2:  train Cost 0.104842 train accuracy 0.983911\n",
      "val Cost 0.769633 val accuracy 0.7712\n",
      "Epoch 129, CIFAR-10 Batch 3:  train Cost 0.11105 train accuracy 0.980198\n",
      "val Cost 0.778895 val accuracy 0.7678\n",
      "Epoch 129, CIFAR-10 Batch 4:  train Cost 0.0904533 train accuracy 0.986386\n",
      "val Cost 0.765713 val accuracy 0.7766\n",
      "Epoch 129, CIFAR-10 Batch 5:  train Cost 0.10746 train accuracy 0.97896\n",
      "val Cost 0.7947 val accuracy 0.7704\n",
      "Epoch 130, CIFAR-10 Batch 1:  train Cost 0.12651 train accuracy 0.975248\n",
      "val Cost 0.776614 val accuracy 0.7698\n",
      "Epoch 130, CIFAR-10 Batch 2:  train Cost 0.123686 train accuracy 0.97401\n",
      "val Cost 0.797781 val accuracy 0.7676\n",
      "Epoch 130, CIFAR-10 Batch 3:  train Cost 0.132507 train accuracy 0.969059\n",
      "val Cost 0.80074 val accuracy 0.7582\n",
      "Epoch 130, CIFAR-10 Batch 4:  train Cost 0.0985633 train accuracy 0.981436\n",
      "val Cost 0.758746 val accuracy 0.7744\n",
      "Epoch 130, CIFAR-10 Batch 5:  train Cost 0.0991227 train accuracy 0.972772\n",
      "val Cost 0.788589 val accuracy 0.7696\n",
      "Epoch 131, CIFAR-10 Batch 1:  train Cost 0.111548 train accuracy 0.980198\n",
      "val Cost 0.739406 val accuracy 0.7786\n",
      "Epoch 131, CIFAR-10 Batch 2:  train Cost 0.111126 train accuracy 0.976485\n",
      "val Cost 0.790633 val accuracy 0.7754\n",
      "Epoch 131, CIFAR-10 Batch 3:  train Cost 0.112442 train accuracy 0.980198\n",
      "val Cost 0.773494 val accuracy 0.7672\n",
      "Epoch 131, CIFAR-10 Batch 4:  train Cost 0.103322 train accuracy 0.977723\n",
      "val Cost 0.766024 val accuracy 0.775\n",
      "Epoch 131, CIFAR-10 Batch 5:  train Cost 0.10611 train accuracy 0.977723\n",
      "val Cost 0.784822 val accuracy 0.7662\n",
      "Epoch 132, CIFAR-10 Batch 1:  train Cost 0.112257 train accuracy 0.976485\n",
      "val Cost 0.75454 val accuracy 0.7752\n",
      "Epoch 132, CIFAR-10 Batch 2:  train Cost 0.109039 train accuracy 0.982673\n",
      "val Cost 0.784164 val accuracy 0.7708\n",
      "Epoch 132, CIFAR-10 Batch 3:  train Cost 0.112551 train accuracy 0.975248\n",
      "val Cost 0.800195 val accuracy 0.7644\n",
      "Epoch 132, CIFAR-10 Batch 4:  train Cost 0.0946029 train accuracy 0.983911\n",
      "val Cost 0.756874 val accuracy 0.7758\n",
      "Epoch 132, CIFAR-10 Batch 5:  train Cost 0.0898782 train accuracy 0.981436\n",
      "val Cost 0.7924 val accuracy 0.7692\n",
      "Epoch 133, CIFAR-10 Batch 1:  train Cost 0.10358 train accuracy 0.980198\n",
      "val Cost 0.759953 val accuracy 0.774\n",
      "Epoch 133, CIFAR-10 Batch 2:  train Cost 0.10348 train accuracy 0.986386\n",
      "val Cost 0.770647 val accuracy 0.7698\n",
      "Epoch 133, CIFAR-10 Batch 3:  train Cost 0.125402 train accuracy 0.969059\n",
      "val Cost 0.808535 val accuracy 0.7638\n",
      "Epoch 133, CIFAR-10 Batch 4:  train Cost 0.0948844 train accuracy 0.985149\n",
      "val Cost 0.75636 val accuracy 0.7784\n",
      "Epoch 133, CIFAR-10 Batch 5:  train Cost 0.0873583 train accuracy 0.988861\n",
      "val Cost 0.788352 val accuracy 0.7702\n",
      "Epoch 134, CIFAR-10 Batch 1:  train Cost 0.107957 train accuracy 0.969059\n",
      "val Cost 0.765417 val accuracy 0.7738\n",
      "Epoch 134, CIFAR-10 Batch 2:  train Cost 0.104659 train accuracy 0.983911\n",
      "val Cost 0.772165 val accuracy 0.7734\n",
      "Epoch 134, CIFAR-10 Batch 3:  train Cost 0.111062 train accuracy 0.97896\n",
      "val Cost 0.78671 val accuracy 0.7636\n",
      "Epoch 134, CIFAR-10 Batch 4:  train Cost 0.0830569 train accuracy 0.987624\n",
      "val Cost 0.75272 val accuracy 0.7758\n",
      "Epoch 134, CIFAR-10 Batch 5:  train Cost 0.0964183 train accuracy 0.977723\n",
      "val Cost 0.810187 val accuracy 0.769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135, CIFAR-10 Batch 1:  train Cost 0.0949198 train accuracy 0.981436\n",
      "val Cost 0.749505 val accuracy 0.778\n",
      "Epoch 135, CIFAR-10 Batch 2:  train Cost 0.108766 train accuracy 0.980198\n",
      "val Cost 0.769901 val accuracy 0.7712\n",
      "Epoch 135, CIFAR-10 Batch 3:  train Cost 0.100412 train accuracy 0.977723\n",
      "val Cost 0.780993 val accuracy 0.7696\n",
      "Epoch 135, CIFAR-10 Batch 4:  train Cost 0.0844936 train accuracy 0.987624\n",
      "val Cost 0.748097 val accuracy 0.779\n",
      "Epoch 135, CIFAR-10 Batch 5:  train Cost 0.0878876 train accuracy 0.983911\n",
      "val Cost 0.784955 val accuracy 0.772\n",
      "Epoch 136, CIFAR-10 Batch 1:  train Cost 0.103026 train accuracy 0.97401\n",
      "val Cost 0.757494 val accuracy 0.773\n",
      "Epoch 136, CIFAR-10 Batch 2:  train Cost 0.103881 train accuracy 0.977723\n",
      "val Cost 0.768216 val accuracy 0.7726\n",
      "Epoch 136, CIFAR-10 Batch 3:  train Cost 0.103721 train accuracy 0.97896\n",
      "val Cost 0.808943 val accuracy 0.768\n",
      "Epoch 136, CIFAR-10 Batch 4:  train Cost 0.0865023 train accuracy 0.985149\n",
      "val Cost 0.771706 val accuracy 0.7794\n",
      "Epoch 136, CIFAR-10 Batch 5:  train Cost 0.0822116 train accuracy 0.988861\n",
      "val Cost 0.75733 val accuracy 0.7776\n",
      "Epoch 137, CIFAR-10 Batch 1:  train Cost 0.0919479 train accuracy 0.981436\n",
      "val Cost 0.745433 val accuracy 0.7796\n",
      "Epoch 137, CIFAR-10 Batch 2:  train Cost 0.0936145 train accuracy 0.985149\n",
      "val Cost 0.779554 val accuracy 0.7762\n",
      "Epoch 137, CIFAR-10 Batch 3:  train Cost 0.104483 train accuracy 0.97401\n",
      "val Cost 0.783718 val accuracy 0.7686\n",
      "Epoch 137, CIFAR-10 Batch 4:  train Cost 0.0875914 train accuracy 0.985149\n",
      "val Cost 0.761393 val accuracy 0.7764\n",
      "Epoch 137, CIFAR-10 Batch 5:  train Cost 0.0775576 train accuracy 0.983911\n",
      "val Cost 0.785273 val accuracy 0.7698\n",
      "Epoch 138, CIFAR-10 Batch 1:  train Cost 0.0941547 train accuracy 0.977723\n",
      "val Cost 0.758722 val accuracy 0.7748\n",
      "Epoch 138, CIFAR-10 Batch 2:  train Cost 0.0979125 train accuracy 0.985149\n",
      "val Cost 0.770648 val accuracy 0.779\n",
      "Epoch 138, CIFAR-10 Batch 3:  train Cost 0.101158 train accuracy 0.976485\n",
      "val Cost 0.797712 val accuracy 0.7676\n",
      "Epoch 138, CIFAR-10 Batch 4:  train Cost 0.0867881 train accuracy 0.985148\n",
      "val Cost 0.779125 val accuracy 0.7728\n",
      "Epoch 138, CIFAR-10 Batch 5:  train Cost 0.0859864 train accuracy 0.988861\n",
      "val Cost 0.778575 val accuracy 0.7732\n",
      "Epoch 139, CIFAR-10 Batch 1:  train Cost 0.0927642 train accuracy 0.982673\n",
      "val Cost 0.762414 val accuracy 0.7726\n",
      "Epoch 139, CIFAR-10 Batch 2:  train Cost 0.0927895 train accuracy 0.988861\n",
      "val Cost 0.758187 val accuracy 0.7792\n",
      "Epoch 139, CIFAR-10 Batch 3:  train Cost 0.0955926 train accuracy 0.976485\n",
      "val Cost 0.798181 val accuracy 0.7706\n",
      "Epoch 139, CIFAR-10 Batch 4:  train Cost 0.0756941 train accuracy 0.988861\n",
      "val Cost 0.755111 val accuracy 0.782\n",
      "Epoch 139, CIFAR-10 Batch 5:  train Cost 0.08448 train accuracy 0.983911\n",
      "val Cost 0.802469 val accuracy 0.7688\n",
      "Epoch 140, CIFAR-10 Batch 1:  train Cost 0.0939452 train accuracy 0.986386\n",
      "val Cost 0.744543 val accuracy 0.7752\n",
      "Epoch 140, CIFAR-10 Batch 2:  train Cost 0.0899188 train accuracy 0.987624\n",
      "val Cost 0.755332 val accuracy 0.7752\n",
      "Epoch 140, CIFAR-10 Batch 3:  train Cost 0.0926324 train accuracy 0.983911\n",
      "val Cost 0.776607 val accuracy 0.7688\n",
      "Epoch 140, CIFAR-10 Batch 4:  train Cost 0.0767719 train accuracy 0.988861\n",
      "val Cost 0.747877 val accuracy 0.7782\n",
      "Epoch 140, CIFAR-10 Batch 5:  train Cost 0.0843311 train accuracy 0.986386\n",
      "val Cost 0.77826 val accuracy 0.7698\n",
      "Epoch 141, CIFAR-10 Batch 1:  train Cost 0.0972459 train accuracy 0.980198\n",
      "val Cost 0.756684 val accuracy 0.7688\n",
      "Epoch 141, CIFAR-10 Batch 2:  train Cost 0.0905245 train accuracy 0.986386\n",
      "val Cost 0.788743 val accuracy 0.7728\n",
      "Epoch 141, CIFAR-10 Batch 3:  train Cost 0.0929558 train accuracy 0.981436\n",
      "val Cost 0.785904 val accuracy 0.7678\n",
      "Epoch 141, CIFAR-10 Batch 4:  train Cost 0.0860292 train accuracy 0.983911\n",
      "val Cost 0.764482 val accuracy 0.7776\n",
      "Epoch 141, CIFAR-10 Batch 5:  train Cost 0.0869641 train accuracy 0.986386\n",
      "val Cost 0.794624 val accuracy 0.7672\n",
      "Epoch 142, CIFAR-10 Batch 1:  train Cost 0.0921668 train accuracy 0.983911\n",
      "val Cost 0.759059 val accuracy 0.7764\n",
      "Epoch 142, CIFAR-10 Batch 2:  train Cost 0.0937737 train accuracy 0.985149\n",
      "val Cost 0.778786 val accuracy 0.7714\n",
      "Epoch 142, CIFAR-10 Batch 3:  train Cost 0.11722 train accuracy 0.970297\n",
      "val Cost 0.832105 val accuracy 0.7624\n",
      "Epoch 142, CIFAR-10 Batch 4:  train Cost 0.0839029 train accuracy 0.981436\n",
      "val Cost 0.773102 val accuracy 0.7782\n",
      "Epoch 142, CIFAR-10 Batch 5:  train Cost 0.0714794 train accuracy 0.990099\n",
      "val Cost 0.768106 val accuracy 0.7786\n",
      "Epoch 143, CIFAR-10 Batch 1:  train Cost 0.101383 train accuracy 0.982673\n",
      "val Cost 0.749134 val accuracy 0.7726\n",
      "Epoch 143, CIFAR-10 Batch 2:  train Cost 0.097772 train accuracy 0.992574\n",
      "val Cost 0.787781 val accuracy 0.7708\n",
      "Epoch 143, CIFAR-10 Batch 3:  train Cost 0.121557 train accuracy 0.972772\n",
      "val Cost 0.833282 val accuracy 0.755\n",
      "Epoch 143, CIFAR-10 Batch 4:  train Cost 0.0808428 train accuracy 0.983911\n",
      "val Cost 0.774087 val accuracy 0.774\n",
      "Epoch 143, CIFAR-10 Batch 5:  train Cost 0.075761 train accuracy 0.993812\n",
      "val Cost 0.772704 val accuracy 0.7692\n",
      "Epoch 144, CIFAR-10 Batch 1:  train Cost 0.0882109 train accuracy 0.983911\n",
      "val Cost 0.756217 val accuracy 0.7756\n",
      "Epoch 144, CIFAR-10 Batch 2:  train Cost 0.0847125 train accuracy 0.988861\n",
      "val Cost 0.781273 val accuracy 0.773\n",
      "Epoch 144, CIFAR-10 Batch 3:  train Cost 0.116194 train accuracy 0.972772\n",
      "val Cost 0.813337 val accuracy 0.7618\n",
      "Epoch 144, CIFAR-10 Batch 4:  train Cost 0.0723837 train accuracy 0.986386\n",
      "val Cost 0.764354 val accuracy 0.7808\n",
      "Epoch 144, CIFAR-10 Batch 5:  train Cost 0.0688834 train accuracy 0.991337\n",
      "val Cost 0.764978 val accuracy 0.7786\n",
      "Epoch 145, CIFAR-10 Batch 1:  train Cost 0.0889753 train accuracy 0.985148\n",
      "val Cost 0.758405 val accuracy 0.7704\n",
      "Epoch 145, CIFAR-10 Batch 2:  train Cost 0.0947486 train accuracy 0.990099\n",
      "val Cost 0.775763 val accuracy 0.7716\n",
      "Epoch 145, CIFAR-10 Batch 3:  train Cost 0.113543 train accuracy 0.972772\n",
      "val Cost 0.831057 val accuracy 0.758\n",
      "Epoch 145, CIFAR-10 Batch 4:  train Cost 0.0818922 train accuracy 0.97896\n",
      "val Cost 0.785976 val accuracy 0.7746\n",
      "Epoch 145, CIFAR-10 Batch 5:  train Cost 0.0743776 train accuracy 0.991337\n",
      "val Cost 0.761882 val accuracy 0.7788\n",
      "Epoch 146, CIFAR-10 Batch 1:  train Cost 0.0869413 train accuracy 0.982673\n",
      "val Cost 0.779842 val accuracy 0.779\n",
      "Epoch 146, CIFAR-10 Batch 2:  train Cost 0.082419 train accuracy 0.987624\n",
      "val Cost 0.774096 val accuracy 0.7792\n",
      "Epoch 146, CIFAR-10 Batch 3:  train Cost 0.107187 train accuracy 0.972772\n",
      "val Cost 0.826137 val accuracy 0.7632\n",
      "Epoch 146, CIFAR-10 Batch 4:  train Cost 0.0718386 train accuracy 0.986386\n",
      "val Cost 0.768412 val accuracy 0.7786\n",
      "Epoch 146, CIFAR-10 Batch 5:  train Cost 0.0767256 train accuracy 0.990099\n",
      "val Cost 0.790018 val accuracy 0.7764\n",
      "Epoch 147, CIFAR-10 Batch 1:  train Cost 0.0976031 train accuracy 0.977723\n",
      "val Cost 0.769682 val accuracy 0.7784\n",
      "Epoch 147, CIFAR-10 Batch 2:  train Cost 0.0936127 train accuracy 0.97896\n",
      "val Cost 0.805825 val accuracy 0.7728\n",
      "Epoch 147, CIFAR-10 Batch 3:  train Cost 0.102695 train accuracy 0.97401\n",
      "val Cost 0.825976 val accuracy 0.7628\n",
      "Epoch 147, CIFAR-10 Batch 4:  train Cost 0.0626455 train accuracy 0.990099\n",
      "val Cost 0.759155 val accuracy 0.7788\n",
      "Epoch 147, CIFAR-10 Batch 5:  train Cost 0.0720801 train accuracy 0.988861\n",
      "val Cost 0.773758 val accuracy 0.7798\n",
      "Epoch 148, CIFAR-10 Batch 1:  train Cost 0.0800922 train accuracy 0.986386\n",
      "val Cost 0.776275 val accuracy 0.7788\n",
      "Epoch 148, CIFAR-10 Batch 2:  train Cost 0.0865913 train accuracy 0.983911\n",
      "val Cost 0.76726 val accuracy 0.7786\n",
      "Epoch 148, CIFAR-10 Batch 3:  train Cost 0.0924434 train accuracy 0.976485\n",
      "val Cost 0.794972 val accuracy 0.7666\n",
      "Epoch 148, CIFAR-10 Batch 4:  train Cost 0.0669099 train accuracy 0.992574\n",
      "val Cost 0.769186 val accuracy 0.7778\n",
      "Epoch 148, CIFAR-10 Batch 5:  train Cost 0.0657221 train accuracy 0.991337\n",
      "val Cost 0.784127 val accuracy 0.7778\n",
      "Epoch 149, CIFAR-10 Batch 1:  train Cost 0.083198 train accuracy 0.982673\n",
      "val Cost 0.757489 val accuracy 0.7774\n",
      "Epoch 149, CIFAR-10 Batch 2:  train Cost 0.0869253 train accuracy 0.985149\n",
      "val Cost 0.786206 val accuracy 0.7764\n",
      "Epoch 149, CIFAR-10 Batch 3:  train Cost 0.0994658 train accuracy 0.970297\n",
      "val Cost 0.834135 val accuracy 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149, CIFAR-10 Batch 4:  train Cost 0.0745185 train accuracy 0.986386\n",
      "val Cost 0.767826 val accuracy 0.7784\n",
      "Epoch 149, CIFAR-10 Batch 5:  train Cost 0.0656253 train accuracy 0.987624\n",
      "val Cost 0.753779 val accuracy 0.7848\n",
      "Epoch 150, CIFAR-10 Batch 1:  train Cost 0.0826803 train accuracy 0.987624\n",
      "val Cost 0.762554 val accuracy 0.7782\n",
      "Epoch 150, CIFAR-10 Batch 2:  train Cost 0.0855613 train accuracy 0.987624\n",
      "val Cost 0.781481 val accuracy 0.7776\n",
      "Epoch 150, CIFAR-10 Batch 3:  train Cost 0.0914946 train accuracy 0.976485\n",
      "val Cost 0.81784 val accuracy 0.7672\n",
      "Epoch 150, CIFAR-10 Batch 4:  train Cost 0.0661637 train accuracy 0.987624\n",
      "val Cost 0.75749 val accuracy 0.7818\n",
      "Epoch 150, CIFAR-10 Batch 5:  train Cost 0.0734676 train accuracy 0.987624\n",
      "val Cost 0.752448 val accuracy 0.7816\n",
      "time: 1630.9769620000002\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "print(\"time:\",time.clock()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7668407201766968\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1ef39f400>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
