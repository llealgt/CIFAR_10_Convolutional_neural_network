{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAHF9JREFUeJzt3UmPZOl1HuAvxsyMrKzKqsqau6rYA5vNbropkjJJmYIs\nUIBXWtn+BV7YO/8Yr73wymtDNAwIggwSMEmBNMeW2Wz2VOzumquyco6M2QttzI2Bc5gChYPn2Z88\nEd+9cd+8q7ezWq0aAFBT9w/9AQCAfzyCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/T/0B/jH8l/+w79fZebGx9PwTK+f\n+3+pc/tGeGZvtJHa9faFYWruk1/+LDzznR/+PLVrbzILz/R6ybPvdFJzg7X18MylKzupXec34t/t\n83eupHb9+be+Hp6Zz+LXq7XWnu0fpeYGWxfDM+9+8NvUrr/97g/jQ8nnwNogN3dhMAjPDPuL1K5p\n4lrPZ7nfWFstU2NrvbXwzMkq/rxvrbUXp/F46eZ+Lu073/+75EH+P7t/3z8AAPzTJegBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+te3P84NddfxJuT\nBv1UUV67v5qEZ94f5yqQ3v7iK6m55TT+Ga/t5NraNlLfLXf22fa6k0n8PPZ3X6R2HXXiTWOT03Fq\n15e/+o3wzOzkNLXr2fPceVxbjzc3LqcHqV0ba/H7atlyrWtXt86l5r70ymvhmadP7qd2jceH4Zmj\no1xLYevGW/laa22tPw/P3Lx+IbVrNrwanvngV/dSu86CN3oAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZUpuPT9dScyfj/fDMsJMr92iLeKFCtzNMrXr2\n28epuZ88+Cw88+snudKS1SReSpEtp1lfX0/NzebxopnWzf0/vb4Rv4f3xrlilR+983545sblXCHI\nZJ67ZpkCo7XkE24wSHzG3NG3L7z6amruc3fuhme2t0apXY8e3gvPLGe55+K5izdSc4tBvPRotJYr\n3rm5Ey8i+rSXO/uz4I0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLLtdeNeriFrtxtvJ+ssJqldl/vx4z93/mJq1+lxvJWvtdb2DuPf7eB0ltq1\nSpz9YpFok2ut9ZKfsZ/533gWb11rrbXjafzsz61yu370i1+GZ15/7bXUrjdevZOa6w/j7V+f+1yu\nGe54OQjPPH74NLXr4HCcmmvrm+GRP/6zt1Orfv7j74VnxvN4G2VrrR3Oci1vz4/jz8ZL41zD3q3e\nYXjm9Cjb2vj780YPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAorW2qz1tlNzd0YxYsYtlu8AKO11i5d3AjPfLyKlym01trmxjI1t9aJl6SMOrnbara5Fp+Z\n58ppTie5IqJF4n/jjVGupGO4Fr+vrt++kdp186Xb4ZlnR7lCkEcHuRKXb3zj6+GZ3cePUrv+9b/5\nVnjmf/z3v07t+uEP/i41d+dLXw3PfPvtr6V2fXj/o/DMx9//cWrX/nQrNXc0jz/jvvjP42fYWmvj\n2YvwzM7OemrXWfBGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNAD\nQGGCHgAKE/QAUFjZ9rrhZu6rvbJ1NTzz8iq368Iw0Wa0/1lq12g73gzXWmvHw5PwzHKwSO364z+K\nN0lduxq/Xq219tEHH6TmPv3kfnim28u1G67m8Xa49W7u7P/kG/Gzfxq/NVprrf3oe99Nzb333p3w\nzGKc/JCbF8Mje8e5RsSjWe5964OHz8Mzx8teatfxPP4Zn+zlzmOyfi419/m7r4Rntq/dTO16+jx+\n9t/+9lupXWfBGz0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0BhZdvrjqa5xrALvc3wzOzZi9SuT/fiTWh/+uU3UrvG0+PU3K1lfGZ9tErt+uZ2/Ozf\nvLKT2nWyzH3GZ2vxFsCT/dz9sZjGZ/rTw9Suu598HJ7Z2Jundl26sp2am/39z8Iz2ebAH/7q3fDM\new8epHadznMtb/c/iTdZPnn+NLXr61/5Znjm7vbt1K7/9F//W2puOn4UnvnJj5+ldj1+/GF45qt/\nkXt2nwVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nsLKlNld666m5W60Xnjl/fiu16+cv4qUULyb7qV13r99Izf3bJy+HZwYHuQKdy+/Hz2Ptw4epXYvl\nLDX3uU58ZrBIDLXWuv34Pbzo5EpcJj/6aXjmQrKMZbkTLy9qrbXFPNGwdLBI7TrfOxeemRzn7vtL\n8UdOa6210Wocnjl49NvUrltffD08s7WZewZ//dVbqbkn+/EWqEdHJ6ldJye74ZmP3n8/tesseKMH\ngMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGx7\n3Rtbo9Tc5vNn4ZleN9Gq1Vp7/aWXwjOHj5+mdrVVrkHtVmcVnhkNc7t6iUaozjL++VprLd5z9Q8m\n3cT/xsO11K7BKv7d+pmGt9baoBtv85tt5WrXVie51rv5JH4ei5a7F69143fItzdyrXzTzjA1t7h5\nLTyzfu9eatdJ5iMmWz3feuO11NyNk/g1uzGbp3a9/urN8MxrO/FGxLPijR4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21Gb3wUepuck8XoIx7uWKRE4u\nxEsONk7i5SOttXb67oepuUVvEZ6Zb+Zuq24vXkqxlixx6bT11Nw8UQ60WOY+42owiM+kNuXm+ldf\nSe3a2su9X5wmLtn07sXUrovzo/DM5mmuKmm+lytWOXqyH545efD91K6H//sX4Znzb72e2vX8Ua64\nazq6FJ6Zj1Or2snzF+GZg0G2Suv3540eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdc+P9lJznx6fhmfmy1z71LBzPTwzuriT2vV8fJiau95b\nC89snOb+f1wcxJv5JtNcm1/byZ3j5uuvhWdOE01orbV29OwgPLO2jLfrtdZabzIJz0ye5u6ptpZr\nlOtsx9se+51cn9/yIP4c2Hgr1+bXhvHv1Vproyfx6rXj+/dTu/Z+/UF4ZvnJ49SurUtbqbnd7XhL\n5PNHud/mwyefhWdeHt5I7ToL3ugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nMEEPAIUJegAoTNADQGGCHgAKK9te9+I03j7VWmuPTuJtRrOD49SunWtXwjOr21dTu9Yu5hqh1g7i\nzXz9B09Tu6ZHJ+GZoxZvrGqttcW5jdTc4O6d8Ey/s0jt2tyOn8fsN5+kds0SLYCn3Vxz4NafvZma\nO9l7Fh9679epXW2eeAd6mPh8rbXJMte0Obh+Mzxz/V9+M7VrbaMXntn9zYepXdsn8V2ttXbhbrxp\n85NHuYa9jV68FXEwGKZ2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLKlNrdvv5Sa6358PzyzMU6taotpvBhhrTNI7XpxfJCa+8Gnn4Vnbp4epna9\n0eIHOUmUsbTW2vh+/Dq31tr0p7+K72rx69xaa51bt8Izp69fT+06mY/CM2+/miunOe6eS82NH9wL\nzwz3c+VW8/PxApLpJ8lCoce5UqzB1SfhmZNruVKswaUL4ZmLf/HV1K69Tx+m5rZ34mU4Xz13N7Xr\nb/7Xi/DM2na8xOyseKMHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAorGx73fWb11Jzh/efhWdGFzupXa2zFh4ZdHO7Hj57npr7z7/4P+GZL1zOtZP9\nx/XN8Mwo+a/q6vgoNbf7Try9bvdKvPmrtdY+msRbzabJprybr98Mz9y5mPte04ePU3PnEq1mneU0\ntasdxn9na92N1KqD8UlqbvHRR+GZ1YNHqV0vtuLPqs0v5BpEb778amru9FH8vroyij9zWmvtK196\nLTxz++XceZwFb/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAU\nJugBoLCypTb7ixepuf5qPzwz6OeOcdqLF5DszcepXbvjXNnJfBX/bgeDXLnH/cEoPLO9mqd2Tbu5\nudVqEp7ZX+ZKSz57Ei+1Od9dT+16kbhkf3X/r1K7vnDrVmru1Uvx73Z57Xpq1/G9++GZxTh+vVpr\nbbXI3YsvXjxN7Mo9B6br8VKb2X68IKy11qa/fD81N0oUOk3WB6ldd998Kzwze/Db1K6z4I0eAAoT\n9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdcPV\nMjXXX87CMzvdXAPStBdvrerPpqldJ6e587h15Up45qWXb6d23T9KNPOtcm1cw2RrVWce/8lMl/HG\nu9Zau3F5JzzTzxWhtYOnj8Izq91cK9+D57mWt/3RMDxzZxL/PbfWWvdZvL2ujXOH353n3rfG8/g5\nnixyz49VohVxNO6kdj28/1lqbtSJ7zue567Z9iQ+t/P266ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQ\nA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGypzcZ4lJp7ML8QnrnaPU3tujjeC8/0\nnzxM7ZofvkjNffHNl8Mzd77w+dSu3V+8F5650emldrVBrgxnsIr/b7xxlCtx6bf4ZxyNNlK7fvPh\nvfDMznHuPeGVz11KzX02jBfUPP4g93vZONwNz3TmuXuqs8jdw6eJUqxpN3fNpsfxXbuLw9Su0eh8\nau5wGi+POp7krtnu/cfhmf6d66ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeA\nwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+v2j+NNV6219t39eEvT/HJqVfvWchqe2XjyKLVrfXaS\nmvvK174dnrl5+7XUru/86J3wzP4k1xy46Ofuj1miLW9j1UntOv0sfq17l3LNcK9c3AnPnC72U7v6\nm8PU3Nt/+vXwzG680Owf5n7yJDwzWeaa0Jb9tdTcOHFfbW4mH1Ybm+GR8TDXyre8fDE1d9ri+x49\njbcUttba/t6z8MyLX7+f2vWXqanf5Y0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtddODB6m5D54/Ds+MZ7k2ru2X4o1hXx7kWte2+vFWvtZa\ne/n27fDM+XO5BrXJIt7mNzmJz7TW2nCwSM2druL7ht3c/TGcxq/ZeDfXxtXtxx8Fy16ure3x81wD\n44t3fxWeGa3nGtQO18/FZzZGqV2Tc1upuePj4/DMaCf329ydxlsiD+e531h3Nk7NPXx0FN+1Hm/l\na621g1n8ObB5kGt7PAve6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhQl6AChM0ANAYWVLbf7V3VxZwdPdeJnFjz8+Se36m3vxkoONV3Lfa3RuLTW31YsXdcwO4wUYrbW2\n6MRLMI4nuV3rvdytv+gl/jfu5P6fXnbjc7vH8WKP1lpbncYLdIbHubOf7eWKiFYffhKeGSXfZaaj\n8+GZd+aT1K57z56k5taX8ZnhMlcYM1iP/146s05q1+lerpjpeBUvB+qfG6R2LQbx73b34nZq11nw\nRg8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY\n2fa612/mvtq/G90Jz9xeu5/a9T/fizeN/e29WWrXH929mZo7+vDj8Mxe8v/H3jJex7U3zTUHXhnF\nm65aa22x6oVnZsvcNXu6ip/Hs1G8fbG11k778fa6rU7uN7Z5IXf2y2n8M7bnB6lda2vxlsjPTnPN\ncM8Xq9Tc9UG8eW20mbs/tjbj57Ea59oNn01z59jvxZ8Fvd3c8+NLq2F45txh7jlwFrzRA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCypbaTJJlJ5fWO+GZ\nP3l9J7Xr2XG8tOQn9/dTu959/CI19/lEUcd0mLutVsv4/52Hp5Pcrkm8lKK11gbr8e+2WuZKS1pi\nbmNtPbXqcBUvIDm4cy216/Jbb6TmevGfS3vnr7+X2nU7cV+9dPFKalebTFNj6/34gezPcoUxx8/j\nz9PryYKlmzuXU3PDbvy3OdjNPU/vHsYLyW5vb6d2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoA\nKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIWVba/r9HJfrTOPt1bd2M41hv2Lly+EZw6m\n8Zax1lq7t5dr8zvpxdv8rt6+ndrVG47CM6fzXDPc6eFhaq4/W4RnhoON1K743dHa/PHT1K7zi3l4\nZnKQu6d2Z4kautba9sWL8ZlO7l1mcBr/brc2N1O7hsn3rc7mWnxmkPuM3aN4w961fvz33FpriQLR\n1lpr3Un8t3mSfA5c6MXvj1fv5HLiLHijB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQ\nmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91qlatAWi0T7WTLeONda629eSl+/E9vnEvtOp7kPuN8\nHG/L27l8JbVr/Vy8r21vmWuvm01nqbl5Ym7SyzUOdju98Mz55L/umV6t6cF+btlp7jxWj56EZ15q\nuefAoBdv89sa587jai/Xbvgi0Ui5thVvAGytteUsfmPNT/ZSuw4muVbERHldW06OU7tuvHk1PPPy\nndxz8Sx4oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhZUttVl2cv/DLFq8SKTNcwUpF/rxwo2v3N5J7Xp+uJuamz5+GJ6ZHeeKIoab8XKP0+R1nq1yc91l\n/FovZom2jdZaZxG/P+bJ85gOMuUv8eKX1lrrzHPnsegN40PdXKnNYh7/bqtkWc/6YpCaW82m4ZlH\n67mimdla/OyXa6lVbbCZO4+Tk/h5DFfL1K4rd66HZ9b7ifv3jHijB4DCBD0AFCboAaAwQQ8AhQl6\nAChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91wYzM111sfhWeme0epXZlW\ns5vb8c/XWmv/bD/XrPXu3uPwzKMHn6R2HYwPwjNHy1z71Gk39z/uYLkKz8xXuba27ir+8zzu5Nra\nTlbxuX7yPWE5yV2z5SR+D3eS7XUtcZ1P+7nrvEw05bXW2nHmM65NUrtaN/7d1ge5+rrlIt5C11pr\nm8v4d3vt2lZq18Vh/OxPnueaA3Of8Hd5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QA\nUJigB4DCBD0AFCboAaAwQQ8AhZUttWndXmqs0xmEZ/obqVXttDsLzwwSZQqttXbnRq4M5+PP4gUT\n08lxatdiGd+1N88VYDzr5G79rV78vuqscteskyio2c/1xbRH03hpSbeTe0/oJQp0srJvMoMWv86P\nl/Hfc2ut7bdcGc5R4lrfSpb8bCcKuHq7h6ld1/rrqbmv3b4ennn1du7hPRrHi8wmybIepTYAwP+X\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhdVtr1vm\n/oeZjE/CM9k2rk6iSWo1zTVkndvcTM3tnI83Lu0+fZLadfgoPrffy13nHySbxi4miujOJxoRW2tt\nM9FeN+vmmvIO5vG502TrWra7rteNX+thom2wtdZGqU+Z29Xv5CoHR4lrvZzNU7umi/h5bCTvjwvn\ncp+xzQ7CI0cvcmd/cD7+m+7Mc8+cndTU7/JGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZ9rrFMtfitUrMdZINasP+MDyzGucakFruONrVzfhn\n/Ok7f5/a9fzB0/DMvJO7hZ8mO9QO5vE2v9Ei2U6W+IhryXtxNYxf526iTa611jqJVr7WWuv3441h\ni1WynWwR/53N57m2tlXyMw4zx59sr1sm7qtuP/fQWbbcM27vaC8801vlzmOtuxWe6Sz/cHHrjR4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21KY7iBdg\ntNbaINHD0EkWxnR6ieNf5IozFsdHqbkbW6PwzOVB7jMOTsfhmfPLXEHKaSf3P243MTfv50pLjpfx\nuXHyXmyJEpfePLeskywU6iYKhVarZLlVJ372uW/V2qDTy80lnh8byfv+XGJss5N8DuTGWmvxwcn4\nOLUp8zgddePP0rPijR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJig\nB4DCBD0AFCboAaCwuu11/dxX660S//uscu1kLdVel2vl63dz3VrnOvHGsD9762Zq1/5JfNfPPnmW\n2vVsMk/NnS7jbWiTZK/ZMnF/LJP/uy8S36ubrG3sJGveut1sNV9cL9Hy1k9+vI1u7lk16safBVv9\n3OFvdePPuMvJdBklb5BBi/+mh8l7arWI7zpNtHOeFW/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAo\nTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaCwsqU2bbieHIyXFXRWyTaLRPHOfD5LrVomL3WmvOHG\nKLWq/eWXb4Vnrg1yhUIfPD5IzT0+jp//i3mupON02QvPTJK34rwTv86rRPFLa611e/Hv1VprvcRc\nsj+nDRIlP/1kt9VmptyqtbaWOP+1Tu5Dnu8twjMXkwU6m73cfbU+iJ9jP3crttks/hw46cTP8Kx4\noweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6ACis\ns8o2rwEA/+R5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAw\nQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM\n0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/xfkBwlHN40TWAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3cdd1ad68>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 6:\n",
      "Image - Min Value: 7 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3cdda65f8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 6\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 4 Max Value: 231\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTNADQGGCHgAKE/QAUNh/A+J+ZwfDyI5sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3cdd7aeb8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 20:\n",
      "Image - Min Value: 26 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV+l1VAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3bc597f28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 20\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are all possible labels? numbers between 0 and 9, \n",
    "* what is the range of values for the image data? 0-255, \n",
    "* Are the labels in order or random? Random.\n",
    "* Classes seem balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    normalized_data = (x-np.min(x))/np.ptp(x)#(x - min(x)) / (max(x)-min(x))\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    from sklearn import preprocessing\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    \n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.54901961,  0.49019608,  0.45098039],\n",
       "         [ 0.57254902,  0.50980392,  0.47843137],\n",
       "         [ 0.56078431,  0.49803922,  0.47843137],\n",
       "         ..., \n",
       "         [ 0.66666667,  0.56862745,  0.51372549],\n",
       "         [ 0.69019608,  0.58823529,  0.5254902 ],\n",
       "         [ 0.66666667,  0.57647059,  0.52156863]],\n",
       "\n",
       "        [[ 0.4745098 ,  0.42352941,  0.50588235],\n",
       "         [ 0.50980392,  0.4627451 ,  0.54509804],\n",
       "         [ 0.5254902 ,  0.4745098 ,  0.56078431],\n",
       "         ..., \n",
       "         [ 0.63921569,  0.55294118,  0.61568627],\n",
       "         [ 0.66666667,  0.57254902,  0.63137255],\n",
       "         [ 0.66666667,  0.58039216,  0.63137255]],\n",
       "\n",
       "        [[ 0.59607843,  0.54509804,  0.68235294],\n",
       "         [ 0.61568627,  0.56862745,  0.70196078],\n",
       "         [ 0.60784314,  0.56078431,  0.68627451],\n",
       "         ..., \n",
       "         [ 0.69411765,  0.60392157,  0.75686275],\n",
       "         [ 0.70980392,  0.61176471,  0.76078431],\n",
       "         [ 0.71764706,  0.62745098,  0.76078431]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.49019608,  0.43137255,  0.4       ],\n",
       "         [ 0.50588235,  0.43921569,  0.40392157],\n",
       "         [ 0.29803922,  0.2627451 ,  0.18431373],\n",
       "         ..., \n",
       "         [ 0.65882353,  0.5372549 ,  0.47058824],\n",
       "         [ 0.61960784,  0.49411765,  0.40392157],\n",
       "         [ 0.57254902,  0.45490196,  0.34117647]],\n",
       "\n",
       "        [[ 0.33333333,  0.30196078,  0.2745098 ],\n",
       "         [ 0.36862745,  0.31764706,  0.27843137],\n",
       "         [ 0.29019608,  0.25490196,  0.17647059],\n",
       "         ..., \n",
       "         [ 0.63529412,  0.51764706,  0.41568627],\n",
       "         [ 0.65098039,  0.5254902 ,  0.39215686],\n",
       "         [ 0.61960784,  0.50196078,  0.36078431]],\n",
       "\n",
       "        [[ 0.49019608,  0.43921569,  0.43529412],\n",
       "         [ 0.50980392,  0.44313725,  0.43529412],\n",
       "         [ 0.41176471,  0.35686275,  0.29411765],\n",
       "         ..., \n",
       "         [ 0.51764706,  0.41568627,  0.30588235],\n",
       "         [ 0.50980392,  0.39607843,  0.25098039],\n",
       "         [ 0.55686275,  0.45098039,  0.30588235]]],\n",
       "\n",
       "\n",
       "       [[[ 0.39215686,  0.42745098,  0.32941176],\n",
       "         [ 0.47843137,  0.49411765,  0.42745098],\n",
       "         [ 0.34117647,  0.34117647,  0.29803922],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.30588235,  0.27058824],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039],\n",
       "         [ 0.2745098 ,  0.28627451,  0.25098039]],\n",
       "\n",
       "        [[ 0.3372549 ,  0.38823529,  0.27843137],\n",
       "         [ 0.29803922,  0.32941176,  0.25882353],\n",
       "         [ 0.23529412,  0.25098039,  0.21176471],\n",
       "         ..., \n",
       "         [ 0.30588235,  0.31764706,  0.28235294],\n",
       "         [ 0.29803922,  0.30980392,  0.2745098 ],\n",
       "         [ 0.32156863,  0.33333333,  0.29803922]],\n",
       "\n",
       "        [[ 0.32941176,  0.39215686,  0.28627451],\n",
       "         [ 0.3254902 ,  0.37254902,  0.29411765],\n",
       "         [ 0.30196078,  0.3372549 ,  0.28235294],\n",
       "         ..., \n",
       "         [ 0.29019608,  0.30196078,  0.26666667],\n",
       "         [ 0.28627451,  0.29803922,  0.2627451 ],\n",
       "         [ 0.3254902 ,  0.3372549 ,  0.30196078]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.25098039,  0.30196078,  0.30980392],\n",
       "         [ 0.47843137,  0.52156863,  0.56470588],\n",
       "         [ 0.5254902 ,  0.56862745,  0.61176471],\n",
       "         ..., \n",
       "         [ 0.41176471,  0.48235294,  0.47058824],\n",
       "         [ 0.32941176,  0.40392157,  0.35686275],\n",
       "         [ 0.23529412,  0.34509804,  0.24705882]],\n",
       "\n",
       "        [[ 0.17254902,  0.2       ,  0.21960784],\n",
       "         [ 0.30588235,  0.32941176,  0.36862745],\n",
       "         [ 0.37647059,  0.39607843,  0.43137255],\n",
       "         ..., \n",
       "         [ 0.57647059,  0.64705882,  0.69803922],\n",
       "         [ 0.49411765,  0.56078431,  0.58431373],\n",
       "         [ 0.36862745,  0.45882353,  0.44313725]],\n",
       "\n",
       "        [[ 0.14117647,  0.1372549 ,  0.15294118],\n",
       "         [ 0.23137255,  0.22745098,  0.25882353],\n",
       "         [ 0.32156863,  0.31764706,  0.33333333],\n",
       "         ..., \n",
       "         [ 0.5254902 ,  0.6       ,  0.62745098],\n",
       "         [ 0.54117647,  0.59607843,  0.61960784],\n",
       "         [ 0.50980392,  0.58039216,  0.58823529]]],\n",
       "\n",
       "\n",
       "       [[[ 0.0745098 ,  0.1254902 ,  0.05882353],\n",
       "         [ 0.08235294,  0.14901961,  0.08235294],\n",
       "         [ 0.10588235,  0.19215686,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.48627451,  0.51372549],\n",
       "         [ 0.29803922,  0.48627451,  0.50980392],\n",
       "         [ 0.27843137,  0.4627451 ,  0.48627451]],\n",
       "\n",
       "        [[ 0.09019608,  0.12156863,  0.05490196],\n",
       "         [ 0.08235294,  0.11764706,  0.04705882],\n",
       "         [ 0.09019608,  0.1372549 ,  0.05490196],\n",
       "         ..., \n",
       "         [ 0.28235294,  0.4627451 ,  0.49411765],\n",
       "         [ 0.29411765,  0.46666667,  0.48627451],\n",
       "         [ 0.26666667,  0.43529412,  0.44705882]],\n",
       "\n",
       "        [[ 0.09411765,  0.14509804,  0.06666667],\n",
       "         [ 0.08627451,  0.1372549 ,  0.0627451 ],\n",
       "         [ 0.09411765,  0.14117647,  0.07058824],\n",
       "         ..., \n",
       "         [ 0.25098039,  0.42745098,  0.43921569],\n",
       "         [ 0.2627451 ,  0.42745098,  0.43529412],\n",
       "         [ 0.25098039,  0.41176471,  0.41176471]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.24313725,  0.18039216,  0.09019608],\n",
       "         [ 0.23529412,  0.18039216,  0.10588235],\n",
       "         [ 0.21568627,  0.18823529,  0.10980392],\n",
       "         ..., \n",
       "         [ 0.05098039,  0.02352941,  0.01568627],\n",
       "         [ 0.04705882,  0.05490196,  0.03137255],\n",
       "         [ 0.09803922,  0.15686275,  0.11764706]],\n",
       "\n",
       "        [[ 0.24705882,  0.20784314,  0.11764706],\n",
       "         [ 0.19215686,  0.17647059,  0.08627451],\n",
       "         [ 0.17647059,  0.18039216,  0.09019608],\n",
       "         ..., \n",
       "         [ 0.11372549,  0.1372549 ,  0.12156863],\n",
       "         [ 0.11764706,  0.16470588,  0.14509804],\n",
       "         [ 0.10588235,  0.19607843,  0.16862745]],\n",
       "\n",
       "        [[ 0.27058824,  0.20392157,  0.11372549],\n",
       "         [ 0.19215686,  0.14901961,  0.07843137],\n",
       "         [ 0.21176471,  0.18039216,  0.10588235],\n",
       "         ..., \n",
       "         [ 0.25882353,  0.34509804,  0.33333333],\n",
       "         [ 0.15686275,  0.26666667,  0.25098039],\n",
       "         [ 0.11372549,  0.24313725,  0.22745098]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.1372549 ,  0.69803922,  0.92156863],\n",
       "         [ 0.15686275,  0.69019608,  0.9372549 ],\n",
       "         [ 0.16470588,  0.69019608,  0.94509804],\n",
       "         ..., \n",
       "         [ 0.38823529,  0.69411765,  0.85882353],\n",
       "         [ 0.30980392,  0.57647059,  0.77254902],\n",
       "         [ 0.34901961,  0.58039216,  0.74117647]],\n",
       "\n",
       "        [[ 0.22352941,  0.71372549,  0.91764706],\n",
       "         [ 0.17254902,  0.72156863,  0.98039216],\n",
       "         [ 0.19607843,  0.71764706,  0.94117647],\n",
       "         ..., \n",
       "         [ 0.61176471,  0.71372549,  0.78431373],\n",
       "         [ 0.55294118,  0.69411765,  0.80784314],\n",
       "         [ 0.45490196,  0.58431373,  0.68627451]],\n",
       "\n",
       "        [[ 0.38431373,  0.77254902,  0.92941176],\n",
       "         [ 0.25098039,  0.74117647,  0.98823529],\n",
       "         [ 0.27058824,  0.75294118,  0.96078431],\n",
       "         ..., \n",
       "         [ 0.7372549 ,  0.76470588,  0.80784314],\n",
       "         [ 0.46666667,  0.52941176,  0.57647059],\n",
       "         [ 0.23921569,  0.30980392,  0.35294118]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.28627451,  0.30980392,  0.30196078],\n",
       "         [ 0.20784314,  0.24705882,  0.26666667],\n",
       "         [ 0.21176471,  0.26666667,  0.31372549],\n",
       "         ..., \n",
       "         [ 0.06666667,  0.15686275,  0.25098039],\n",
       "         [ 0.08235294,  0.14117647,  0.2       ],\n",
       "         [ 0.12941176,  0.18823529,  0.19215686]],\n",
       "\n",
       "        [[ 0.23921569,  0.26666667,  0.29411765],\n",
       "         [ 0.21568627,  0.2745098 ,  0.3372549 ],\n",
       "         [ 0.22352941,  0.30980392,  0.40392157],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.18823529,  0.28235294],\n",
       "         [ 0.06666667,  0.1372549 ,  0.20784314],\n",
       "         [ 0.02745098,  0.09019608,  0.1254902 ]],\n",
       "\n",
       "        [[ 0.17254902,  0.21960784,  0.28627451],\n",
       "         [ 0.18039216,  0.25882353,  0.34509804],\n",
       "         [ 0.19215686,  0.30196078,  0.41176471],\n",
       "         ..., \n",
       "         [ 0.10588235,  0.20392157,  0.30196078],\n",
       "         [ 0.08235294,  0.16862745,  0.25882353],\n",
       "         [ 0.04705882,  0.12156863,  0.19607843]]],\n",
       "\n",
       "\n",
       "       [[[ 0.74117647,  0.82745098,  0.94117647],\n",
       "         [ 0.72941176,  0.81568627,  0.9254902 ],\n",
       "         [ 0.7254902 ,  0.81176471,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.76470588,  0.87843137],\n",
       "         [ 0.6745098 ,  0.76078431,  0.87058824],\n",
       "         [ 0.6627451 ,  0.76078431,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.76078431,  0.82352941,  0.9372549 ],\n",
       "         [ 0.74901961,  0.81176471,  0.9254902 ],\n",
       "         [ 0.74509804,  0.80784314,  0.92156863],\n",
       "         ..., \n",
       "         [ 0.67843137,  0.75294118,  0.8627451 ],\n",
       "         [ 0.67058824,  0.74901961,  0.85490196],\n",
       "         [ 0.65490196,  0.74509804,  0.84705882]],\n",
       "\n",
       "        [[ 0.81568627,  0.85882353,  0.95686275],\n",
       "         [ 0.80392157,  0.84705882,  0.94117647],\n",
       "         [ 0.8       ,  0.84313725,  0.9372549 ],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.74901961,  0.85098039],\n",
       "         [ 0.6745098 ,  0.74509804,  0.84705882],\n",
       "         [ 0.6627451 ,  0.74901961,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.81176471,  0.78039216,  0.70980392],\n",
       "         [ 0.79607843,  0.76470588,  0.68627451],\n",
       "         [ 0.79607843,  0.76862745,  0.67843137],\n",
       "         ..., \n",
       "         [ 0.52941176,  0.51764706,  0.49803922],\n",
       "         [ 0.63529412,  0.61960784,  0.58823529],\n",
       "         [ 0.65882353,  0.63921569,  0.59215686]],\n",
       "\n",
       "        [[ 0.77647059,  0.74509804,  0.66666667],\n",
       "         [ 0.74117647,  0.70980392,  0.62352941],\n",
       "         [ 0.70588235,  0.6745098 ,  0.57647059],\n",
       "         ..., \n",
       "         [ 0.69803922,  0.67058824,  0.62745098],\n",
       "         [ 0.68627451,  0.6627451 ,  0.61176471],\n",
       "         [ 0.68627451,  0.6627451 ,  0.60392157]],\n",
       "\n",
       "        [[ 0.77647059,  0.74117647,  0.67843137],\n",
       "         [ 0.74117647,  0.70980392,  0.63529412],\n",
       "         [ 0.69803922,  0.66666667,  0.58431373],\n",
       "         ..., \n",
       "         [ 0.76470588,  0.72156863,  0.6627451 ],\n",
       "         [ 0.76862745,  0.74117647,  0.67058824],\n",
       "         [ 0.76470588,  0.74509804,  0.67058824]]],\n",
       "\n",
       "\n",
       "       [[[ 0.89803922,  0.89803922,  0.9372549 ],\n",
       "         [ 0.9254902 ,  0.92941176,  0.96862745],\n",
       "         [ 0.91764706,  0.9254902 ,  0.96862745],\n",
       "         ..., \n",
       "         [ 0.85098039,  0.85882353,  0.91372549],\n",
       "         [ 0.86666667,  0.8745098 ,  0.91764706],\n",
       "         [ 0.87058824,  0.8745098 ,  0.91372549]],\n",
       "\n",
       "        [[ 0.87058824,  0.86666667,  0.89803922],\n",
       "         [ 0.9372549 ,  0.9372549 ,  0.97647059],\n",
       "         [ 0.91372549,  0.91764706,  0.96470588],\n",
       "         ..., \n",
       "         [ 0.8745098 ,  0.8745098 ,  0.9254902 ],\n",
       "         [ 0.89019608,  0.89411765,  0.93333333],\n",
       "         [ 0.82352941,  0.82745098,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.83529412,  0.80784314,  0.82745098],\n",
       "         [ 0.91764706,  0.90980392,  0.9372549 ],\n",
       "         [ 0.90588235,  0.91372549,  0.95686275],\n",
       "         ..., \n",
       "         [ 0.8627451 ,  0.8627451 ,  0.90980392],\n",
       "         [ 0.8627451 ,  0.85882353,  0.90980392],\n",
       "         [ 0.79215686,  0.79607843,  0.84313725]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.58823529,  0.56078431,  0.52941176],\n",
       "         [ 0.54901961,  0.52941176,  0.49803922],\n",
       "         [ 0.51764706,  0.49803922,  0.47058824],\n",
       "         ..., \n",
       "         [ 0.87843137,  0.87058824,  0.85490196],\n",
       "         [ 0.90196078,  0.89411765,  0.88235294],\n",
       "         [ 0.94509804,  0.94509804,  0.93333333]],\n",
       "\n",
       "        [[ 0.5372549 ,  0.51764706,  0.49411765],\n",
       "         [ 0.50980392,  0.49803922,  0.47058824],\n",
       "         [ 0.49019608,  0.4745098 ,  0.45098039],\n",
       "         ..., \n",
       "         [ 0.70980392,  0.70588235,  0.69803922],\n",
       "         [ 0.79215686,  0.78823529,  0.77647059],\n",
       "         [ 0.83137255,  0.82745098,  0.81176471]],\n",
       "\n",
       "        [[ 0.47843137,  0.46666667,  0.44705882],\n",
       "         [ 0.4627451 ,  0.45490196,  0.43137255],\n",
       "         [ 0.47058824,  0.45490196,  0.43529412],\n",
       "         ..., \n",
       "         [ 0.70196078,  0.69411765,  0.67843137],\n",
       "         [ 0.64313725,  0.64313725,  0.63529412],\n",
       "         [ 0.63921569,  0.63921569,  0.63137255]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview features\n",
    "valid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview labels\n",
    "valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = (None,image_shape[0],image_shape[1],image_shape[2]),name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,shape = (None,n_classes), name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    convolution_output_depth = conv_num_outputs\n",
    "    convolution_input_depth = x_tensor.get_shape().as_list()[3]\n",
    "    convolution_kernel_height = conv_ksize[0]\n",
    "    convolution_kernel_width = conv_ksize[1]\n",
    "    convolution_height_stride = conv_strides[0]\n",
    "    convolution_width_stride = conv_strides[1]\n",
    "    pool_kernel_height = pool_ksize[0]\n",
    "    pool_kernel_width = pool_ksize[1]\n",
    "    pool_height_stride = pool_strides[0]\n",
    "    pool_width_stride = pool_strides[1]\n",
    "\n",
    "    convolution_weights = tf.Variable(tf.truncated_normal([convolution_kernel_height,convolution_kernel_width,convolution_input_depth,convolution_output_depth],stddev=0.01))\n",
    "    convolution_bias = tf.Variable(tf.zeros([convolution_output_depth]))\n",
    "    convolution = tf.nn.conv2d(x_tensor,  convolution_weights,strides = [1,convolution_height_stride,convolution_width_stride,1],padding = \"SAME\")\n",
    "    convolution = tf.nn.bias_add(convolution,convolution_bias)\n",
    "    convolution = tf.nn.relu(convolution)\n",
    "    max_pool = tf.nn.max_pool(convolution,ksize=[1,pool_kernel_height,pool_kernel_width,1],strides=[1,pool_height_stride,pool_width_stride ,1],padding=\"SAME\")\n",
    "    return max_pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor,(-1,tensor_shape[1]*tensor_shape[2]*tensor_shape[3]))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    fully_connected_weights  = tf.Variable(tf.truncated_normal([tensor_shape[1],num_outputs],stddev=0.01))\n",
    "    fully_connected_bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    return tf.nn.relu( tf.add(tf.matmul(x_tensor,fully_connected_weights),fully_connected_bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    fully_connected_weights  = tf.Variable(tf.truncated_normal([tensor_shape[1],num_outputs]))\n",
    "    fully_connected_bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor,fully_connected_weights),fully_connected_bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    network = conv2d_maxpool(x,32,[3,3],[1,1],[2,2],[2,2])\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = conv2d_maxpool(network,32,[3,3],[2,2],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,64,[3,3],[1,1],[2,2],[2,2])\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    network = flatten(network)\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,128)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    #network = fully_conn(network,256)\n",
    "    #network = fully_conn(network,128)\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    network = output(network,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return network\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    feed_dict = {x:feature_batch,y:label_batch,keep_prob:keep_probability}\n",
    "    optimizer = session.run(optimizer,feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    val_feed_dict = {x:valid_features,y:valid_labels,keep_prob:1.0}\n",
    "    train_feed_dict = {x:feature_batch,y:label_batch,keep_prob:1.0}\n",
    "    val_cost = session.run(cost,feed_dict=val_feed_dict)\n",
    "    val_accuracy = session.run(accuracy,feed_dict=val_feed_dict)\n",
    "    train_cost = session.run(cost,feed_dict=train_feed_dict)\n",
    "    train_accuracy = session.run(accuracy,feed_dict=train_feed_dict)\n",
    "    print(\"train Cost\",train_cost,\"train accuracy\",train_accuracy)\n",
    "    print(\"val Cost\",val_cost,\"val accuracy\",val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  train Cost 2.28406 train accuracy 0.125\n",
      "val Cost 2.28343 val accuracy 0.1092\n",
      "Epoch  2, CIFAR-10 Batch 1:  train Cost 2.21139 train accuracy 0.182432\n",
      "val Cost 2.17206 val accuracy 0.2\n",
      "Epoch  3, CIFAR-10 Batch 1:  train Cost 2.11499 train accuracy 0.246622\n",
      "val Cost 2.08281 val accuracy 0.2022\n",
      "Epoch  4, CIFAR-10 Batch 1:  train Cost 2.0697 train accuracy 0.246622\n",
      "val Cost 2.03421 val accuracy 0.2404\n",
      "Epoch  5, CIFAR-10 Batch 1:  train Cost 2.04906 train accuracy 0.266892\n",
      "val Cost 2.02391 val accuracy 0.2336\n",
      "Epoch  6, CIFAR-10 Batch 1:  train Cost 2.06393 train accuracy 0.236486\n",
      "val Cost 2.05654 val accuracy 0.2232\n",
      "Epoch  7, CIFAR-10 Batch 1:  train Cost 2.02194 train accuracy 0.27027\n",
      "val Cost 1.99032 val accuracy 0.2458\n",
      "Epoch  8, CIFAR-10 Batch 1:  train Cost 1.9964 train accuracy 0.283784\n",
      "val Cost 1.97723 val accuracy 0.2592\n",
      "Epoch  9, CIFAR-10 Batch 1:  train Cost 1.96718 train accuracy 0.297297\n",
      "val Cost 1.94057 val accuracy 0.2894\n",
      "Epoch 10, CIFAR-10 Batch 1:  train Cost 1.93568 train accuracy 0.277027\n",
      "val Cost 1.87179 val accuracy 0.3064\n",
      "Epoch 11, CIFAR-10 Batch 1:  train Cost 1.88586 train accuracy 0.304054\n",
      "val Cost 1.84194 val accuracy 0.3174\n",
      "Epoch 12, CIFAR-10 Batch 1:  train Cost 1.84525 train accuracy 0.327703\n",
      "val Cost 1.78877 val accuracy 0.3378\n",
      "Epoch 13, CIFAR-10 Batch 1:  train Cost 1.82934 train accuracy 0.337838\n",
      "val Cost 1.75437 val accuracy 0.3478\n",
      "Epoch 14, CIFAR-10 Batch 1:  train Cost 1.79847 train accuracy 0.368243\n",
      "val Cost 1.74962 val accuracy 0.3522\n",
      "Epoch 15, CIFAR-10 Batch 1:  train Cost 1.79671 train accuracy 0.35473\n",
      "val Cost 1.72797 val accuracy 0.3622\n",
      "Epoch 16, CIFAR-10 Batch 1:  train Cost 1.74761 train accuracy 0.405405\n",
      "val Cost 1.69573 val accuracy 0.3864\n",
      "Epoch 17, CIFAR-10 Batch 1:  train Cost 1.72773 train accuracy 0.408784\n",
      "val Cost 1.68105 val accuracy 0.3826\n",
      "Epoch 18, CIFAR-10 Batch 1:  train Cost 1.70544 train accuracy 0.435811\n",
      "val Cost 1.66555 val accuracy 0.4002\n",
      "Epoch 19, CIFAR-10 Batch 1:  train Cost 1.67789 train accuracy 0.442568\n",
      "val Cost 1.64684 val accuracy 0.4012\n",
      "Epoch 20, CIFAR-10 Batch 1:  train Cost 1.66222 train accuracy 0.422297\n",
      "val Cost 1.64949 val accuracy 0.3974\n",
      "Epoch 21, CIFAR-10 Batch 1:  train Cost 1.62116 train accuracy 0.456081\n",
      "val Cost 1.61066 val accuracy 0.412\n",
      "Epoch 22, CIFAR-10 Batch 1:  train Cost 1.59932 train accuracy 0.456081\n",
      "val Cost 1.60496 val accuracy 0.4112\n",
      "Epoch 23, CIFAR-10 Batch 1:  train Cost 1.59708 train accuracy 0.459459\n",
      "val Cost 1.56661 val accuracy 0.43\n",
      "Epoch 24, CIFAR-10 Batch 1:  train Cost 1.56104 train accuracy 0.472973\n",
      "val Cost 1.55609 val accuracy 0.4278\n",
      "Epoch 25, CIFAR-10 Batch 1:  train Cost 1.53974 train accuracy 0.47973\n",
      "val Cost 1.53043 val accuracy 0.437\n",
      "Epoch 26, CIFAR-10 Batch 1:  train Cost 1.51735 train accuracy 0.476351\n",
      "val Cost 1.50559 val accuracy 0.441\n",
      "Epoch 27, CIFAR-10 Batch 1:  train Cost 1.49428 train accuracy 0.459459\n",
      "val Cost 1.52745 val accuracy 0.441\n",
      "Epoch 28, CIFAR-10 Batch 1:  train Cost 1.48572 train accuracy 0.496622\n",
      "val Cost 1.50826 val accuracy 0.4452\n",
      "Epoch 29, CIFAR-10 Batch 1:  train Cost 1.45649 train accuracy 0.496622\n",
      "val Cost 1.48411 val accuracy 0.4522\n",
      "Epoch 30, CIFAR-10 Batch 1:  train Cost 1.44711 train accuracy 0.489865\n",
      "val Cost 1.47725 val accuracy 0.4576\n",
      "Epoch 31, CIFAR-10 Batch 1:  train Cost 1.42148 train accuracy 0.516892\n",
      "val Cost 1.4696 val accuracy 0.4608\n",
      "Epoch 32, CIFAR-10 Batch 1:  train Cost 1.40729 train accuracy 0.523649\n",
      "val Cost 1.4627 val accuracy 0.4646\n",
      "Epoch 33, CIFAR-10 Batch 1:  train Cost 1.38746 train accuracy 0.533784\n",
      "val Cost 1.44248 val accuracy 0.471\n",
      "Epoch 34, CIFAR-10 Batch 1:  train Cost 1.3644 train accuracy 0.540541\n",
      "val Cost 1.43864 val accuracy 0.4646\n",
      "Epoch 35, CIFAR-10 Batch 1:  train Cost 1.36763 train accuracy 0.543919\n",
      "val Cost 1.42736 val accuracy 0.4748\n",
      "Epoch 36, CIFAR-10 Batch 1:  train Cost 1.33285 train accuracy 0.527027\n",
      "val Cost 1.41836 val accuracy 0.4766\n",
      "Epoch 37, CIFAR-10 Batch 1:  train Cost 1.32228 train accuracy 0.527027\n",
      "val Cost 1.41552 val accuracy 0.4802\n",
      "Epoch 38, CIFAR-10 Batch 1:  train Cost 1.31808 train accuracy 0.543919\n",
      "val Cost 1.40682 val accuracy 0.4828\n",
      "Epoch 39, CIFAR-10 Batch 1:  train Cost 1.28516 train accuracy 0.574324\n",
      "val Cost 1.40092 val accuracy 0.4804\n",
      "Epoch 40, CIFAR-10 Batch 1:  train Cost 1.27587 train accuracy 0.581081\n",
      "val Cost 1.39807 val accuracy 0.483\n",
      "Epoch 41, CIFAR-10 Batch 1:  train Cost 1.24653 train accuracy 0.581081\n",
      "val Cost 1.38101 val accuracy 0.495\n",
      "Epoch 42, CIFAR-10 Batch 1:  train Cost 1.21726 train accuracy 0.577703\n",
      "val Cost 1.37359 val accuracy 0.4962\n",
      "Epoch 43, CIFAR-10 Batch 1:  train Cost 1.21035 train accuracy 0.584459\n",
      "val Cost 1.3701 val accuracy 0.497\n",
      "Epoch 44, CIFAR-10 Batch 1:  train Cost 1.1798 train accuracy 0.591216\n",
      "val Cost 1.3585 val accuracy 0.5092\n",
      "Epoch 45, CIFAR-10 Batch 1:  train Cost 1.17229 train accuracy 0.591216\n",
      "val Cost 1.34215 val accuracy 0.5106\n",
      "Epoch 46, CIFAR-10 Batch 1:  train Cost 1.19429 train accuracy 0.587838\n",
      "val Cost 1.39221 val accuracy 0.5008\n",
      "Epoch 47, CIFAR-10 Batch 1:  train Cost 1.14918 train accuracy 0.581081\n",
      "val Cost 1.35114 val accuracy 0.5074\n",
      "Epoch 48, CIFAR-10 Batch 1:  train Cost 1.11711 train accuracy 0.601351\n",
      "val Cost 1.33499 val accuracy 0.5168\n",
      "Epoch 49, CIFAR-10 Batch 1:  train Cost 1.09473 train accuracy 0.618243\n",
      "val Cost 1.31771 val accuracy 0.5208\n",
      "Epoch 50, CIFAR-10 Batch 1:  train Cost 1.08229 train accuracy 0.614865\n",
      "val Cost 1.31068 val accuracy 0.5268\n",
      "Epoch 51, CIFAR-10 Batch 1:  train Cost 1.06239 train accuracy 0.618243\n",
      "val Cost 1.31239 val accuracy 0.5222\n",
      "Epoch 52, CIFAR-10 Batch 1:  train Cost 1.07101 train accuracy 0.631757\n",
      "val Cost 1.31059 val accuracy 0.526\n",
      "Epoch 53, CIFAR-10 Batch 1:  train Cost 1.07376 train accuracy 0.638514\n",
      "val Cost 1.32343 val accuracy 0.5236\n",
      "Epoch 54, CIFAR-10 Batch 1:  train Cost 1.04491 train accuracy 0.608108\n",
      "val Cost 1.31975 val accuracy 0.519\n",
      "Epoch 55, CIFAR-10 Batch 1:  train Cost 1.01074 train accuracy 0.641892\n",
      "val Cost 1.28942 val accuracy 0.5312\n",
      "Epoch 56, CIFAR-10 Batch 1:  train Cost 1.01232 train accuracy 0.64527\n",
      "val Cost 1.28893 val accuracy 0.5286\n",
      "Epoch 57, CIFAR-10 Batch 1:  train Cost 0.993335 train accuracy 0.658784\n",
      "val Cost 1.29636 val accuracy 0.5272\n",
      "Epoch 58, CIFAR-10 Batch 1:  train Cost 0.978885 train accuracy 0.655405\n",
      "val Cost 1.28573 val accuracy 0.5308\n",
      "Epoch 59, CIFAR-10 Batch 1:  train Cost 0.966081 train accuracy 0.679054\n",
      "val Cost 1.28759 val accuracy 0.5322\n",
      "Epoch 60, CIFAR-10 Batch 1:  train Cost 0.953738 train accuracy 0.672297\n",
      "val Cost 1.2794 val accuracy 0.5314\n",
      "Epoch 61, CIFAR-10 Batch 1:  train Cost 0.935124 train accuracy 0.699324\n",
      "val Cost 1.26343 val accuracy 0.5426\n",
      "Epoch 62, CIFAR-10 Batch 1:  train Cost 0.939875 train accuracy 0.692568\n",
      "val Cost 1.2794 val accuracy 0.539\n",
      "Epoch 63, CIFAR-10 Batch 1:  train Cost 0.952048 train accuracy 0.672297\n",
      "val Cost 1.2944 val accuracy 0.5282\n",
      "Epoch 64, CIFAR-10 Batch 1:  train Cost 0.903087 train accuracy 0.685811\n",
      "val Cost 1.27414 val accuracy 0.538\n",
      "Epoch 65, CIFAR-10 Batch 1:  train Cost 0.890328 train accuracy 0.709459\n",
      "val Cost 1.25517 val accuracy 0.5436\n",
      "Epoch 66, CIFAR-10 Batch 1:  train Cost 0.892006 train accuracy 0.706081\n",
      "val Cost 1.26621 val accuracy 0.5446\n",
      "Epoch 67, CIFAR-10 Batch 1:  train Cost 0.861719 train accuracy 0.692568\n",
      "val Cost 1.26467 val accuracy 0.5418\n",
      "Epoch 68, CIFAR-10 Batch 1:  train Cost 0.863486 train accuracy 0.722973\n",
      "val Cost 1.27509 val accuracy 0.5354\n",
      "Epoch 69, CIFAR-10 Batch 1:  train Cost 0.867259 train accuracy 0.716216\n",
      "val Cost 1.27671 val accuracy 0.535\n",
      "Epoch 70, CIFAR-10 Batch 1:  train Cost 0.852206 train accuracy 0.706081\n",
      "val Cost 1.26489 val accuracy 0.5394\n",
      "Epoch 71, CIFAR-10 Batch 1:  train Cost 0.832994 train accuracy 0.726351\n",
      "val Cost 1.25695 val accuracy 0.5424\n",
      "Epoch 72, CIFAR-10 Batch 1:  train Cost 0.84202 train accuracy 0.716216\n",
      "val Cost 1.2606 val accuracy 0.5422\n",
      "Epoch 73, CIFAR-10 Batch 1:  train Cost 0.817952 train accuracy 0.722973\n",
      "val Cost 1.25932 val accuracy 0.5442\n",
      "Epoch 74, CIFAR-10 Batch 1:  train Cost 0.806445 train accuracy 0.726351\n",
      "val Cost 1.26209 val accuracy 0.5454\n",
      "Epoch 75, CIFAR-10 Batch 1:  train Cost 0.808378 train accuracy 0.716216\n",
      "val Cost 1.26315 val accuracy 0.5492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, CIFAR-10 Batch 1:  train Cost 0.809242 train accuracy 0.72973\n",
      "val Cost 1.29449 val accuracy 0.536\n",
      "Epoch 77, CIFAR-10 Batch 1:  train Cost 0.750843 train accuracy 0.736486\n",
      "val Cost 1.25403 val accuracy 0.5504\n",
      "Epoch 78, CIFAR-10 Batch 1:  train Cost 0.759099 train accuracy 0.743243\n",
      "val Cost 1.25562 val accuracy 0.5538\n",
      "Epoch 79, CIFAR-10 Batch 1:  train Cost 0.752489 train accuracy 0.756757\n",
      "val Cost 1.26855 val accuracy 0.5436\n",
      "Epoch 80, CIFAR-10 Batch 1:  train Cost 0.760617 train accuracy 0.766892\n",
      "val Cost 1.27045 val accuracy 0.5476\n",
      "Epoch 81, CIFAR-10 Batch 1:  train Cost 0.732941 train accuracy 0.77027\n",
      "val Cost 1.26724 val accuracy 0.549\n",
      "Epoch 82, CIFAR-10 Batch 1:  train Cost 0.719609 train accuracy 0.780405\n",
      "val Cost 1.25692 val accuracy 0.5524\n",
      "Epoch 83, CIFAR-10 Batch 1:  train Cost 0.698632 train accuracy 0.766892\n",
      "val Cost 1.24569 val accuracy 0.5494\n",
      "Epoch 84, CIFAR-10 Batch 1:  train Cost 0.678428 train accuracy 0.780405\n",
      "val Cost 1.2353 val accuracy 0.5586\n",
      "Epoch 85, CIFAR-10 Batch 1:  train Cost 0.71999 train accuracy 0.763514\n",
      "val Cost 1.27102 val accuracy 0.5506\n",
      "Epoch 86, CIFAR-10 Batch 1:  train Cost 0.719808 train accuracy 0.766892\n",
      "val Cost 1.29428 val accuracy 0.5498\n",
      "Epoch 87, CIFAR-10 Batch 1:  train Cost 0.721337 train accuracy 0.773649\n",
      "val Cost 1.33009 val accuracy 0.5444\n",
      "Epoch 88, CIFAR-10 Batch 1:  train Cost 0.745124 train accuracy 0.753378\n",
      "val Cost 1.32918 val accuracy 0.5356\n",
      "Epoch 89, CIFAR-10 Batch 1:  train Cost 0.695639 train accuracy 0.777027\n",
      "val Cost 1.25976 val accuracy 0.5498\n",
      "Epoch 90, CIFAR-10 Batch 1:  train Cost 0.661301 train accuracy 0.793919\n",
      "val Cost 1.27415 val accuracy 0.5526\n",
      "Epoch 91, CIFAR-10 Batch 1:  train Cost 0.644285 train accuracy 0.800676\n",
      "val Cost 1.27738 val accuracy 0.5602\n",
      "Epoch 92, CIFAR-10 Batch 1:  train Cost 0.663828 train accuracy 0.777027\n",
      "val Cost 1.31865 val accuracy 0.5446\n",
      "Epoch 93, CIFAR-10 Batch 1:  train Cost 0.653095 train accuracy 0.787162\n",
      "val Cost 1.28187 val accuracy 0.5496\n",
      "Epoch 94, CIFAR-10 Batch 1:  train Cost 0.638566 train accuracy 0.793919\n",
      "val Cost 1.28742 val accuracy 0.5506\n",
      "Epoch 95, CIFAR-10 Batch 1:  train Cost 0.620671 train accuracy 0.800676\n",
      "val Cost 1.29927 val accuracy 0.5558\n",
      "Epoch 96, CIFAR-10 Batch 1:  train Cost 0.586804 train accuracy 0.824324\n",
      "val Cost 1.29108 val accuracy 0.5586\n",
      "Epoch 97, CIFAR-10 Batch 1:  train Cost 0.588899 train accuracy 0.804054\n",
      "val Cost 1.284 val accuracy 0.56\n",
      "Epoch 98, CIFAR-10 Batch 1:  train Cost 0.572387 train accuracy 0.807432\n",
      "val Cost 1.29702 val accuracy 0.5622\n",
      "Epoch 99, CIFAR-10 Batch 1:  train Cost 0.581084 train accuracy 0.814189\n",
      "val Cost 1.30988 val accuracy 0.5546\n",
      "Epoch 100, CIFAR-10 Batch 1:  train Cost 0.564258 train accuracy 0.817568\n",
      "val Cost 1.30487 val accuracy 0.5552\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment results\n",
    "* 100 epoch,512 batch size, 2 convolutions, 2 fully connected, no dropout:train Cost 3317.09 train accuracy 0.760135 val Cost 27927.9 val accuracy 0.4018\n",
    "* 20 epoch,96 batch size, 3 convolutions, 1 fully connected 512 unnits, no dropout:train Cost 225.305 train accuracy 0.958333\n",
    "val Cost 54559.4 val accuracy 0.4438\n",
    "* 20 epoch,96 batch size, 3 convolutions, 1 fully connected 512 unnits, 0.6 dropout before last layer(after fully connected): after 10 batches it plateaus to train Cost 2.24019 train accuracy 0.125\n",
    "val Cost 151.769 val accuracy 0.106\n",
    "* same parameters as previous experiment but with dropout after last convolution and before fully connected: started with high values (35 and 30 percent accuracy on first epoch but then dropped to las than 10% for the first 10 epochs)\n",
    "* 20 epoch,96 batch size, 3 convolutions, 2 fully connected 512 unnits: it overfits, in epoch 25 train acc is 94% but validation is 41%\n",
    "* same as the last one but with  2 dropouts, one after first convolution and one between fully connected (0.5 keep prob): after 10 epochs, it didnt improved (less than 0.7)\n",
    "--------------------------------------------------------------\n",
    "    512 batch, keep_prob = 0.5, 45 epochs\n",
    "    network = conv2d_maxpool(x,32,[3,3],[1,1],[2,2],[2,2])\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = conv2d_maxpool(network,32,[3,3],[2,2],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,64,[3,3],[1,1],[2,2],[2,2])\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = flatten(network)\n",
    "    #network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    didnt improved, of 0.1\n",
    "-----------------------------------------------------------------------------------\n",
    "    512 batch, keep_prob = 0.5, 45 epochs\n",
    "    network = conv2d_maxpool(x,32,[3,3],[1,1],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,32,[3,3],[2,2],[2,2],[2,2])\n",
    "    network = conv2d_maxpool(network,64,[3,3],[1,1],[2,2],[2,2])\n",
    "    network = flatten(network)\n",
    "    network = fully_conn(network,512)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    network = fully_conn(network,128)\n",
    "    network = tf.nn.dropout(network,keep_prob)\n",
    "    after giving a smaller stdv to weights initialization, it got good results! 0.8 on trian and 0.55 on validation!\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  train Cost 2.29219 train accuracy 0.108108\n",
      "val Cost 2.29091 val accuracy 0.1062\n",
      "Epoch  1, CIFAR-10 Batch 2:  train Cost 2.16285 train accuracy 0.165541\n",
      "val Cost 2.17297 val accuracy 0.1652\n",
      "Epoch  1, CIFAR-10 Batch 3:  train Cost 2.06651 train accuracy 0.226351\n",
      "val Cost 2.07703 val accuracy 0.2264\n",
      "Epoch  1, CIFAR-10 Batch 4:  train Cost 2.01147 train accuracy 0.293919\n",
      "val Cost 2.03802 val accuracy 0.24\n",
      "Epoch  1, CIFAR-10 Batch 5:  train Cost 1.99628 train accuracy 0.293919\n",
      "val Cost 1.99318 val accuracy 0.2638\n",
      "Epoch  2, CIFAR-10 Batch 1:  train Cost 2.0171 train accuracy 0.273649\n",
      "val Cost 1.96903 val accuracy 0.2704\n",
      "Epoch  2, CIFAR-10 Batch 2:  train Cost 1.93099 train accuracy 0.307432\n",
      "val Cost 1.93341 val accuracy 0.294\n",
      "Epoch  2, CIFAR-10 Batch 3:  train Cost 1.84621 train accuracy 0.320946\n",
      "val Cost 1.90365 val accuracy 0.2978\n",
      "Epoch  2, CIFAR-10 Batch 4:  train Cost 1.80448 train accuracy 0.358108\n",
      "val Cost 1.86384 val accuracy 0.3126\n",
      "Epoch  2, CIFAR-10 Batch 5:  train Cost 1.87338 train accuracy 0.314189\n",
      "val Cost 1.83441 val accuracy 0.3174\n",
      "Epoch  3, CIFAR-10 Batch 1:  train Cost 1.88542 train accuracy 0.324324\n",
      "val Cost 1.81761 val accuracy 0.3202\n",
      "Epoch  3, CIFAR-10 Batch 2:  train Cost 1.79253 train accuracy 0.375\n",
      "val Cost 1.77724 val accuracy 0.3506\n",
      "Epoch  3, CIFAR-10 Batch 3:  train Cost 1.66391 train accuracy 0.432432\n",
      "val Cost 1.75063 val accuracy 0.3648\n",
      "Epoch  3, CIFAR-10 Batch 4:  train Cost 1.62555 train accuracy 0.408784\n",
      "val Cost 1.70608 val accuracy 0.3792\n",
      "Epoch  3, CIFAR-10 Batch 5:  train Cost 1.73967 train accuracy 0.408784\n",
      "val Cost 1.6959 val accuracy 0.3772\n",
      "Epoch  4, CIFAR-10 Batch 1:  train Cost 1.76141 train accuracy 0.408784\n",
      "val Cost 1.68958 val accuracy 0.3886\n",
      "Epoch  4, CIFAR-10 Batch 2:  train Cost 1.69958 train accuracy 0.381757\n",
      "val Cost 1.65746 val accuracy 0.4052\n",
      "Epoch  4, CIFAR-10 Batch 3:  train Cost 1.48482 train accuracy 0.466216\n",
      "val Cost 1.60758 val accuracy 0.418\n",
      "Epoch  4, CIFAR-10 Batch 4:  train Cost 1.55491 train accuracy 0.472973\n",
      "val Cost 1.64808 val accuracy 0.4066\n",
      "Epoch  4, CIFAR-10 Batch 5:  train Cost 1.65598 train accuracy 0.425676\n",
      "val Cost 1.6334 val accuracy 0.4022\n",
      "Epoch  5, CIFAR-10 Batch 1:  train Cost 1.67196 train accuracy 0.408784\n",
      "val Cost 1.60133 val accuracy 0.421\n",
      "Epoch  5, CIFAR-10 Batch 2:  train Cost 1.63679 train accuracy 0.385135\n",
      "val Cost 1.58236 val accuracy 0.4262\n",
      "Epoch  5, CIFAR-10 Batch 3:  train Cost 1.39741 train accuracy 0.523649\n",
      "val Cost 1.53674 val accuracy 0.435\n",
      "Epoch  5, CIFAR-10 Batch 4:  train Cost 1.42766 train accuracy 0.456081\n",
      "val Cost 1.54553 val accuracy 0.4296\n",
      "Epoch  5, CIFAR-10 Batch 5:  train Cost 1.55034 train accuracy 0.456081\n",
      "val Cost 1.51696 val accuracy 0.4406\n",
      "Epoch  6, CIFAR-10 Batch 1:  train Cost 1.60137 train accuracy 0.445946\n",
      "val Cost 1.53983 val accuracy 0.4388\n",
      "Epoch  6, CIFAR-10 Batch 2:  train Cost 1.58557 train accuracy 0.405405\n",
      "val Cost 1.51935 val accuracy 0.4414\n",
      "Epoch  6, CIFAR-10 Batch 3:  train Cost 1.32075 train accuracy 0.550676\n",
      "val Cost 1.47801 val accuracy 0.4524\n",
      "Epoch  6, CIFAR-10 Batch 4:  train Cost 1.384 train accuracy 0.523649\n",
      "val Cost 1.48747 val accuracy 0.4586\n",
      "Epoch  6, CIFAR-10 Batch 5:  train Cost 1.46616 train accuracy 0.466216\n",
      "val Cost 1.46345 val accuracy 0.4602\n",
      "Epoch  7, CIFAR-10 Batch 1:  train Cost 1.52223 train accuracy 0.496622\n",
      "val Cost 1.48748 val accuracy 0.4558\n",
      "Epoch  7, CIFAR-10 Batch 2:  train Cost 1.52867 train accuracy 0.422297\n",
      "val Cost 1.4517 val accuracy 0.467\n",
      "Epoch  7, CIFAR-10 Batch 3:  train Cost 1.28028 train accuracy 0.570946\n",
      "val Cost 1.42903 val accuracy 0.4758\n",
      "Epoch  7, CIFAR-10 Batch 4:  train Cost 1.31865 train accuracy 0.527027\n",
      "val Cost 1.43326 val accuracy 0.4708\n",
      "Epoch  7, CIFAR-10 Batch 5:  train Cost 1.40429 train accuracy 0.486486\n",
      "val Cost 1.44461 val accuracy 0.4702\n",
      "Epoch  8, CIFAR-10 Batch 1:  train Cost 1.46728 train accuracy 0.5\n",
      "val Cost 1.43649 val accuracy 0.4762\n",
      "Epoch  8, CIFAR-10 Batch 2:  train Cost 1.44477 train accuracy 0.449324\n",
      "val Cost 1.39136 val accuracy 0.4962\n",
      "Epoch  8, CIFAR-10 Batch 3:  train Cost 1.23504 train accuracy 0.574324\n",
      "val Cost 1.38147 val accuracy 0.4966\n",
      "Epoch  8, CIFAR-10 Batch 4:  train Cost 1.27668 train accuracy 0.540541\n",
      "val Cost 1.39159 val accuracy 0.4868\n",
      "Epoch  8, CIFAR-10 Batch 5:  train Cost 1.3404 train accuracy 0.510135\n",
      "val Cost 1.39141 val accuracy 0.4896\n",
      "Epoch  9, CIFAR-10 Batch 1:  train Cost 1.4471 train accuracy 0.496622\n",
      "val Cost 1.39986 val accuracy 0.4966\n",
      "Epoch  9, CIFAR-10 Batch 2:  train Cost 1.39495 train accuracy 0.513514\n",
      "val Cost 1.39868 val accuracy 0.4868\n",
      "Epoch  9, CIFAR-10 Batch 3:  train Cost 1.22962 train accuracy 0.550676\n",
      "val Cost 1.37632 val accuracy 0.4984\n",
      "Epoch  9, CIFAR-10 Batch 4:  train Cost 1.21061 train accuracy 0.560811\n",
      "val Cost 1.33488 val accuracy 0.5182\n",
      "Epoch  9, CIFAR-10 Batch 5:  train Cost 1.27712 train accuracy 0.533784\n",
      "val Cost 1.34538 val accuracy 0.5166\n",
      "Epoch 10, CIFAR-10 Batch 1:  train Cost 1.36401 train accuracy 0.513514\n",
      "val Cost 1.33099 val accuracy 0.5156\n",
      "Epoch 10, CIFAR-10 Batch 2:  train Cost 1.35339 train accuracy 0.540541\n",
      "val Cost 1.36339 val accuracy 0.4978\n",
      "Epoch 10, CIFAR-10 Batch 3:  train Cost 1.18375 train accuracy 0.577703\n",
      "val Cost 1.32856 val accuracy 0.5164\n",
      "Epoch 10, CIFAR-10 Batch 4:  train Cost 1.15683 train accuracy 0.587838\n",
      "val Cost 1.29802 val accuracy 0.5286\n",
      "Epoch 10, CIFAR-10 Batch 5:  train Cost 1.22789 train accuracy 0.570946\n",
      "val Cost 1.29886 val accuracy 0.5296\n",
      "Epoch 11, CIFAR-10 Batch 1:  train Cost 1.32792 train accuracy 0.52027\n",
      "val Cost 1.29066 val accuracy 0.5306\n",
      "Epoch 11, CIFAR-10 Batch 2:  train Cost 1.30056 train accuracy 0.513514\n",
      "val Cost 1.29395 val accuracy 0.5366\n",
      "Epoch 11, CIFAR-10 Batch 3:  train Cost 1.12019 train accuracy 0.608108\n",
      "val Cost 1.28345 val accuracy 0.5334\n",
      "Epoch 11, CIFAR-10 Batch 4:  train Cost 1.12688 train accuracy 0.594595\n",
      "val Cost 1.27132 val accuracy 0.5436\n",
      "Epoch 11, CIFAR-10 Batch 5:  train Cost 1.20335 train accuracy 0.584459\n",
      "val Cost 1.2852 val accuracy 0.5382\n",
      "Epoch 12, CIFAR-10 Batch 1:  train Cost 1.26841 train accuracy 0.560811\n",
      "val Cost 1.26676 val accuracy 0.5424\n",
      "Epoch 12, CIFAR-10 Batch 2:  train Cost 1.2353 train accuracy 0.547297\n",
      "val Cost 1.27303 val accuracy 0.5458\n",
      "Epoch 12, CIFAR-10 Batch 3:  train Cost 1.10754 train accuracy 0.618243\n",
      "val Cost 1.25992 val accuracy 0.5514\n",
      "Epoch 12, CIFAR-10 Batch 4:  train Cost 1.07511 train accuracy 0.628378\n",
      "val Cost 1.24818 val accuracy 0.5522\n",
      "Epoch 12, CIFAR-10 Batch 5:  train Cost 1.16694 train accuracy 0.597973\n",
      "val Cost 1.24061 val accuracy 0.5534\n",
      "Epoch 13, CIFAR-10 Batch 1:  train Cost 1.24187 train accuracy 0.554054\n",
      "val Cost 1.23591 val accuracy 0.5562\n",
      "Epoch 13, CIFAR-10 Batch 2:  train Cost 1.1943 train accuracy 0.554054\n",
      "val Cost 1.23985 val accuracy 0.562\n",
      "Epoch 13, CIFAR-10 Batch 3:  train Cost 1.08098 train accuracy 0.60473\n",
      "val Cost 1.24037 val accuracy 0.5544\n",
      "Epoch 13, CIFAR-10 Batch 4:  train Cost 1.05191 train accuracy 0.608108\n",
      "val Cost 1.2282 val accuracy 0.5612\n",
      "Epoch 13, CIFAR-10 Batch 5:  train Cost 1.13333 train accuracy 0.594595\n",
      "val Cost 1.22395 val accuracy 0.5626\n",
      "Epoch 14, CIFAR-10 Batch 1:  train Cost 1.19311 train accuracy 0.594595\n",
      "val Cost 1.21702 val accuracy 0.5616\n",
      "Epoch 14, CIFAR-10 Batch 2:  train Cost 1.17735 train accuracy 0.560811\n",
      "val Cost 1.24266 val accuracy 0.5574\n",
      "Epoch 14, CIFAR-10 Batch 3:  train Cost 1.05784 train accuracy 0.64527\n",
      "val Cost 1.20344 val accuracy 0.5676\n",
      "Epoch 14, CIFAR-10 Batch 4:  train Cost 1.01602 train accuracy 0.64527\n",
      "val Cost 1.1927 val accuracy 0.5776\n",
      "Epoch 14, CIFAR-10 Batch 5:  train Cost 1.10196 train accuracy 0.635135\n",
      "val Cost 1.20412 val accuracy 0.562\n",
      "Epoch 15, CIFAR-10 Batch 1:  train Cost 1.17548 train accuracy 0.577703\n",
      "val Cost 1.20009 val accuracy 0.5708\n",
      "Epoch 15, CIFAR-10 Batch 2:  train Cost 1.12745 train accuracy 0.570946\n",
      "val Cost 1.20409 val accuracy 0.5742\n",
      "Epoch 15, CIFAR-10 Batch 3:  train Cost 1.07091 train accuracy 0.641892\n",
      "val Cost 1.21599 val accuracy 0.5724\n",
      "Epoch 15, CIFAR-10 Batch 4:  train Cost 1.01319 train accuracy 0.665541\n",
      "val Cost 1.21554 val accuracy 0.564\n",
      "Epoch 15, CIFAR-10 Batch 5:  train Cost 1.08147 train accuracy 0.618243\n",
      "val Cost 1.18369 val accuracy 0.5744\n",
      "Epoch 16, CIFAR-10 Batch 1:  train Cost 1.13218 train accuracy 0.601351\n",
      "val Cost 1.18153 val accuracy 0.5724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, CIFAR-10 Batch 2:  train Cost 1.12406 train accuracy 0.584459\n",
      "val Cost 1.19347 val accuracy 0.578\n",
      "Epoch 16, CIFAR-10 Batch 3:  train Cost 1.01225 train accuracy 0.641892\n",
      "val Cost 1.16958 val accuracy 0.582\n",
      "Epoch 16, CIFAR-10 Batch 4:  train Cost 0.975846 train accuracy 0.652027\n",
      "val Cost 1.18697 val accuracy 0.5774\n",
      "Epoch 16, CIFAR-10 Batch 5:  train Cost 1.03421 train accuracy 0.648649\n",
      "val Cost 1.15461 val accuracy 0.5874\n",
      "Epoch 17, CIFAR-10 Batch 1:  train Cost 1.13023 train accuracy 0.614865\n",
      "val Cost 1.15298 val accuracy 0.5916\n",
      "Epoch 17, CIFAR-10 Batch 2:  train Cost 1.09118 train accuracy 0.594595\n",
      "val Cost 1.1606 val accuracy 0.5856\n",
      "Epoch 17, CIFAR-10 Batch 3:  train Cost 1.02318 train accuracy 0.64527\n",
      "val Cost 1.17802 val accuracy 0.5814\n",
      "Epoch 17, CIFAR-10 Batch 4:  train Cost 0.965653 train accuracy 0.682432\n",
      "val Cost 1.17041 val accuracy 0.5818\n",
      "Epoch 17, CIFAR-10 Batch 5:  train Cost 1.01865 train accuracy 0.655405\n",
      "val Cost 1.14513 val accuracy 0.588\n",
      "Epoch 18, CIFAR-10 Batch 1:  train Cost 1.06982 train accuracy 0.638514\n",
      "val Cost 1.14055 val accuracy 0.5936\n",
      "Epoch 18, CIFAR-10 Batch 2:  train Cost 1.04204 train accuracy 0.614865\n",
      "val Cost 1.13892 val accuracy 0.5962\n",
      "Epoch 18, CIFAR-10 Batch 3:  train Cost 0.991652 train accuracy 0.658784\n",
      "val Cost 1.14367 val accuracy 0.589\n",
      "Epoch 18, CIFAR-10 Batch 4:  train Cost 0.924465 train accuracy 0.679054\n",
      "val Cost 1.15145 val accuracy 0.5844\n",
      "Epoch 18, CIFAR-10 Batch 5:  train Cost 0.980257 train accuracy 0.685811\n",
      "val Cost 1.11766 val accuracy 0.597\n",
      "Epoch 19, CIFAR-10 Batch 1:  train Cost 1.07113 train accuracy 0.625\n",
      "val Cost 1.12141 val accuracy 0.5978\n",
      "Epoch 19, CIFAR-10 Batch 2:  train Cost 1.04357 train accuracy 0.625\n",
      "val Cost 1.12111 val accuracy 0.5998\n",
      "Epoch 19, CIFAR-10 Batch 3:  train Cost 0.94666 train accuracy 0.668919\n",
      "val Cost 1.12417 val accuracy 0.5944\n",
      "Epoch 19, CIFAR-10 Batch 4:  train Cost 0.921303 train accuracy 0.675676\n",
      "val Cost 1.14616 val accuracy 0.5858\n",
      "Epoch 19, CIFAR-10 Batch 5:  train Cost 0.963601 train accuracy 0.689189\n",
      "val Cost 1.1159 val accuracy 0.5962\n",
      "Epoch 20, CIFAR-10 Batch 1:  train Cost 1.03818 train accuracy 0.631757\n",
      "val Cost 1.12644 val accuracy 0.5926\n",
      "Epoch 20, CIFAR-10 Batch 2:  train Cost 1.00411 train accuracy 0.638514\n",
      "val Cost 1.10637 val accuracy 0.6082\n",
      "Epoch 20, CIFAR-10 Batch 3:  train Cost 0.910132 train accuracy 0.689189\n",
      "val Cost 1.10117 val accuracy 0.606\n",
      "Epoch 20, CIFAR-10 Batch 4:  train Cost 0.893859 train accuracy 0.675676\n",
      "val Cost 1.10141 val accuracy 0.603\n",
      "Epoch 20, CIFAR-10 Batch 5:  train Cost 0.932874 train accuracy 0.695946\n",
      "val Cost 1.09393 val accuracy 0.6106\n",
      "Epoch 21, CIFAR-10 Batch 1:  train Cost 1.00309 train accuracy 0.648649\n",
      "val Cost 1.0971 val accuracy 0.6048\n",
      "Epoch 21, CIFAR-10 Batch 2:  train Cost 1.00328 train accuracy 0.638514\n",
      "val Cost 1.09309 val accuracy 0.612\n",
      "Epoch 21, CIFAR-10 Batch 3:  train Cost 0.907109 train accuracy 0.682432\n",
      "val Cost 1.09701 val accuracy 0.606\n",
      "Epoch 21, CIFAR-10 Batch 4:  train Cost 0.874945 train accuracy 0.689189\n",
      "val Cost 1.12044 val accuracy 0.5994\n",
      "Epoch 21, CIFAR-10 Batch 5:  train Cost 0.935917 train accuracy 0.682432\n",
      "val Cost 1.09661 val accuracy 0.6104\n",
      "Epoch 22, CIFAR-10 Batch 1:  train Cost 0.99167 train accuracy 0.665541\n",
      "val Cost 1.08354 val accuracy 0.611\n",
      "Epoch 22, CIFAR-10 Batch 2:  train Cost 0.995545 train accuracy 0.648649\n",
      "val Cost 1.08082 val accuracy 0.6126\n",
      "Epoch 22, CIFAR-10 Batch 3:  train Cost 0.882982 train accuracy 0.709459\n",
      "val Cost 1.07675 val accuracy 0.6118\n",
      "Epoch 22, CIFAR-10 Batch 4:  train Cost 0.851232 train accuracy 0.709459\n",
      "val Cost 1.09209 val accuracy 0.6034\n",
      "Epoch 22, CIFAR-10 Batch 5:  train Cost 0.898521 train accuracy 0.689189\n",
      "val Cost 1.0838 val accuracy 0.6098\n",
      "Epoch 23, CIFAR-10 Batch 1:  train Cost 0.959601 train accuracy 0.668919\n",
      "val Cost 1.07673 val accuracy 0.6084\n",
      "Epoch 23, CIFAR-10 Batch 2:  train Cost 0.97611 train accuracy 0.662162\n",
      "val Cost 1.06675 val accuracy 0.6138\n",
      "Epoch 23, CIFAR-10 Batch 3:  train Cost 0.848171 train accuracy 0.699324\n",
      "val Cost 1.06175 val accuracy 0.6216\n",
      "Epoch 23, CIFAR-10 Batch 4:  train Cost 0.838989 train accuracy 0.699324\n",
      "val Cost 1.09069 val accuracy 0.6108\n",
      "Epoch 23, CIFAR-10 Batch 5:  train Cost 0.886811 train accuracy 0.685811\n",
      "val Cost 1.07792 val accuracy 0.613\n",
      "Epoch 24, CIFAR-10 Batch 1:  train Cost 0.960297 train accuracy 0.665541\n",
      "val Cost 1.06697 val accuracy 0.6168\n",
      "Epoch 24, CIFAR-10 Batch 2:  train Cost 0.969227 train accuracy 0.672297\n",
      "val Cost 1.07631 val accuracy 0.6106\n",
      "Epoch 24, CIFAR-10 Batch 3:  train Cost 0.854495 train accuracy 0.706081\n",
      "val Cost 1.07782 val accuracy 0.6146\n",
      "Epoch 24, CIFAR-10 Batch 4:  train Cost 0.833322 train accuracy 0.719595\n",
      "val Cost 1.08402 val accuracy 0.6114\n",
      "Epoch 24, CIFAR-10 Batch 5:  train Cost 0.858693 train accuracy 0.692568\n",
      "val Cost 1.05831 val accuracy 0.6258\n",
      "Epoch 25, CIFAR-10 Batch 1:  train Cost 0.943587 train accuracy 0.695946\n",
      "val Cost 1.08154 val accuracy 0.611\n",
      "Epoch 25, CIFAR-10 Batch 2:  train Cost 0.923933 train accuracy 0.668919\n",
      "val Cost 1.06728 val accuracy 0.6154\n",
      "Epoch 25, CIFAR-10 Batch 3:  train Cost 0.853711 train accuracy 0.682432\n",
      "val Cost 1.0757 val accuracy 0.6144\n",
      "Epoch 25, CIFAR-10 Batch 4:  train Cost 0.806887 train accuracy 0.709459\n",
      "val Cost 1.07172 val accuracy 0.6188\n",
      "Epoch 25, CIFAR-10 Batch 5:  train Cost 0.888845 train accuracy 0.655405\n",
      "val Cost 1.08463 val accuracy 0.6106\n",
      "Epoch 26, CIFAR-10 Batch 1:  train Cost 0.939795 train accuracy 0.675676\n",
      "val Cost 1.07775 val accuracy 0.6114\n",
      "Epoch 26, CIFAR-10 Batch 2:  train Cost 0.940398 train accuracy 0.625\n",
      "val Cost 1.0903 val accuracy 0.6126\n",
      "Epoch 26, CIFAR-10 Batch 3:  train Cost 0.855765 train accuracy 0.692568\n",
      "val Cost 1.0885 val accuracy 0.6094\n",
      "Epoch 26, CIFAR-10 Batch 4:  train Cost 0.855549 train accuracy 0.685811\n",
      "val Cost 1.12452 val accuracy 0.5972\n",
      "Epoch 26, CIFAR-10 Batch 5:  train Cost 0.861218 train accuracy 0.692568\n",
      "val Cost 1.04734 val accuracy 0.6268\n",
      "Epoch 27, CIFAR-10 Batch 1:  train Cost 0.944041 train accuracy 0.662162\n",
      "val Cost 1.09566 val accuracy 0.6052\n",
      "Epoch 27, CIFAR-10 Batch 2:  train Cost 0.951398 train accuracy 0.662162\n",
      "val Cost 1.0787 val accuracy 0.6122\n",
      "Epoch 27, CIFAR-10 Batch 3:  train Cost 0.805084 train accuracy 0.712838\n",
      "val Cost 1.04681 val accuracy 0.6202\n",
      "Epoch 27, CIFAR-10 Batch 4:  train Cost 0.790176 train accuracy 0.716216\n",
      "val Cost 1.07443 val accuracy 0.6188\n",
      "Epoch 27, CIFAR-10 Batch 5:  train Cost 0.874348 train accuracy 0.692568\n",
      "val Cost 1.06534 val accuracy 0.6196\n",
      "Epoch 28, CIFAR-10 Batch 1:  train Cost 0.92087 train accuracy 0.675676\n",
      "val Cost 1.10837 val accuracy 0.5978\n",
      "Epoch 28, CIFAR-10 Batch 2:  train Cost 0.915966 train accuracy 0.672297\n",
      "val Cost 1.0468 val accuracy 0.6272\n",
      "Epoch 28, CIFAR-10 Batch 3:  train Cost 0.775112 train accuracy 0.709459\n",
      "val Cost 1.03763 val accuracy 0.6246\n",
      "Epoch 28, CIFAR-10 Batch 4:  train Cost 0.785078 train accuracy 0.712838\n",
      "val Cost 1.07668 val accuracy 0.6178\n",
      "Epoch 28, CIFAR-10 Batch 5:  train Cost 0.829891 train accuracy 0.692568\n",
      "val Cost 1.04922 val accuracy 0.622\n",
      "Epoch 29, CIFAR-10 Batch 1:  train Cost 0.90503 train accuracy 0.712838\n",
      "val Cost 1.0652 val accuracy 0.6134\n",
      "Epoch 29, CIFAR-10 Batch 2:  train Cost 0.911842 train accuracy 0.675676\n",
      "val Cost 1.07179 val accuracy 0.6194\n",
      "Epoch 29, CIFAR-10 Batch 3:  train Cost 0.76788 train accuracy 0.709459\n",
      "val Cost 1.04862 val accuracy 0.6198\n",
      "Epoch 29, CIFAR-10 Batch 4:  train Cost 0.75181 train accuracy 0.75\n",
      "val Cost 1.02973 val accuracy 0.628\n",
      "Epoch 29, CIFAR-10 Batch 5:  train Cost 0.835102 train accuracy 0.695946\n",
      "val Cost 1.07468 val accuracy 0.6254\n",
      "Epoch 30, CIFAR-10 Batch 1:  train Cost 0.882147 train accuracy 0.695946\n",
      "val Cost 1.06436 val accuracy 0.6132\n",
      "Epoch 30, CIFAR-10 Batch 2:  train Cost 0.899636 train accuracy 0.689189\n",
      "val Cost 1.03513 val accuracy 0.6326\n",
      "Epoch 30, CIFAR-10 Batch 3:  train Cost 0.758297 train accuracy 0.722973\n",
      "val Cost 1.04269 val accuracy 0.6288\n",
      "Epoch 30, CIFAR-10 Batch 4:  train Cost 0.741182 train accuracy 0.739865\n",
      "val Cost 1.04617 val accuracy 0.6304\n",
      "Epoch 30, CIFAR-10 Batch 5:  train Cost 0.792695 train accuracy 0.712838\n",
      "val Cost 1.01941 val accuracy 0.6402\n",
      "Epoch 31, CIFAR-10 Batch 1:  train Cost 0.876236 train accuracy 0.692568\n",
      "val Cost 1.0631 val accuracy 0.6234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, CIFAR-10 Batch 2:  train Cost 0.86709 train accuracy 0.692568\n",
      "val Cost 1.01714 val accuracy 0.643\n",
      "Epoch 31, CIFAR-10 Batch 3:  train Cost 0.733848 train accuracy 0.736486\n",
      "val Cost 1.03451 val accuracy 0.6282\n",
      "Epoch 31, CIFAR-10 Batch 4:  train Cost 0.754291 train accuracy 0.722973\n",
      "val Cost 1.06742 val accuracy 0.6208\n",
      "Epoch 31, CIFAR-10 Batch 5:  train Cost 0.783074 train accuracy 0.726351\n",
      "val Cost 1.01926 val accuracy 0.6402\n",
      "Epoch 32, CIFAR-10 Batch 1:  train Cost 0.838321 train accuracy 0.702703\n",
      "val Cost 1.04026 val accuracy 0.6278\n",
      "Epoch 32, CIFAR-10 Batch 2:  train Cost 0.862109 train accuracy 0.699324\n",
      "val Cost 1.01397 val accuracy 0.6444\n",
      "Epoch 32, CIFAR-10 Batch 3:  train Cost 0.748325 train accuracy 0.726351\n",
      "val Cost 1.04993 val accuracy 0.627\n",
      "Epoch 32, CIFAR-10 Batch 4:  train Cost 0.742509 train accuracy 0.719595\n",
      "val Cost 1.0418 val accuracy 0.626\n",
      "Epoch 32, CIFAR-10 Batch 5:  train Cost 0.786745 train accuracy 0.722973\n",
      "val Cost 1.02967 val accuracy 0.6366\n",
      "Epoch 33, CIFAR-10 Batch 1:  train Cost 0.823301 train accuracy 0.722973\n",
      "val Cost 1.02382 val accuracy 0.6298\n",
      "Epoch 33, CIFAR-10 Batch 2:  train Cost 0.873608 train accuracy 0.692568\n",
      "val Cost 1.05212 val accuracy 0.627\n",
      "Epoch 33, CIFAR-10 Batch 3:  train Cost 0.706748 train accuracy 0.743243\n",
      "val Cost 1.03227 val accuracy 0.6336\n",
      "Epoch 33, CIFAR-10 Batch 4:  train Cost 0.70871 train accuracy 0.746622\n",
      "val Cost 1.0124 val accuracy 0.6378\n",
      "Epoch 33, CIFAR-10 Batch 5:  train Cost 0.75109 train accuracy 0.746622\n",
      "val Cost 1.01502 val accuracy 0.6414\n",
      "Epoch 34, CIFAR-10 Batch 1:  train Cost 0.81594 train accuracy 0.733108\n",
      "val Cost 1.03693 val accuracy 0.625\n",
      "Epoch 34, CIFAR-10 Batch 2:  train Cost 0.839408 train accuracy 0.699324\n",
      "val Cost 1.0359 val accuracy 0.6354\n",
      "Epoch 34, CIFAR-10 Batch 3:  train Cost 0.689138 train accuracy 0.75\n",
      "val Cost 1.01024 val accuracy 0.6408\n",
      "Epoch 34, CIFAR-10 Batch 4:  train Cost 0.697351 train accuracy 0.736486\n",
      "val Cost 1.01476 val accuracy 0.6432\n",
      "Epoch 34, CIFAR-10 Batch 5:  train Cost 0.746903 train accuracy 0.756757\n",
      "val Cost 1.00564 val accuracy 0.6432\n",
      "Epoch 35, CIFAR-10 Batch 1:  train Cost 0.802009 train accuracy 0.719595\n",
      "val Cost 1.0241 val accuracy 0.6316\n",
      "Epoch 35, CIFAR-10 Batch 2:  train Cost 0.802802 train accuracy 0.695946\n",
      "val Cost 1.00832 val accuracy 0.6446\n",
      "Epoch 35, CIFAR-10 Batch 3:  train Cost 0.67917 train accuracy 0.743243\n",
      "val Cost 1.01238 val accuracy 0.6376\n",
      "Epoch 35, CIFAR-10 Batch 4:  train Cost 0.678465 train accuracy 0.75\n",
      "val Cost 1.01576 val accuracy 0.6394\n",
      "Epoch 35, CIFAR-10 Batch 5:  train Cost 0.746803 train accuracy 0.777027\n",
      "val Cost 1.00851 val accuracy 0.644\n",
      "Epoch 36, CIFAR-10 Batch 1:  train Cost 0.815612 train accuracy 0.726351\n",
      "val Cost 1.04832 val accuracy 0.6276\n",
      "Epoch 36, CIFAR-10 Batch 2:  train Cost 0.808418 train accuracy 0.722973\n",
      "val Cost 1.05051 val accuracy 0.6368\n",
      "Epoch 36, CIFAR-10 Batch 3:  train Cost 0.739848 train accuracy 0.726351\n",
      "val Cost 1.07241 val accuracy 0.6294\n",
      "Epoch 36, CIFAR-10 Batch 4:  train Cost 0.687186 train accuracy 0.75\n",
      "val Cost 1.01514 val accuracy 0.639\n",
      "Epoch 36, CIFAR-10 Batch 5:  train Cost 0.744701 train accuracy 0.760135\n",
      "val Cost 1.02357 val accuracy 0.6372\n",
      "Epoch 37, CIFAR-10 Batch 1:  train Cost 0.795887 train accuracy 0.726351\n",
      "val Cost 1.01968 val accuracy 0.6314\n",
      "Epoch 37, CIFAR-10 Batch 2:  train Cost 0.819582 train accuracy 0.699324\n",
      "val Cost 1.08372 val accuracy 0.6264\n",
      "Epoch 37, CIFAR-10 Batch 3:  train Cost 0.712659 train accuracy 0.75\n",
      "val Cost 1.04551 val accuracy 0.6332\n",
      "Epoch 37, CIFAR-10 Batch 4:  train Cost 0.682013 train accuracy 0.75\n",
      "val Cost 1.02486 val accuracy 0.6386\n",
      "Epoch 37, CIFAR-10 Batch 5:  train Cost 0.742049 train accuracy 0.766892\n",
      "val Cost 1.01251 val accuracy 0.6384\n",
      "Epoch 38, CIFAR-10 Batch 1:  train Cost 0.796761 train accuracy 0.736486\n",
      "val Cost 1.04189 val accuracy 0.627\n",
      "Epoch 38, CIFAR-10 Batch 2:  train Cost 0.774005 train accuracy 0.709459\n",
      "val Cost 1.023 val accuracy 0.648\n",
      "Epoch 38, CIFAR-10 Batch 3:  train Cost 0.669748 train accuracy 0.766892\n",
      "val Cost 0.994488 val accuracy 0.6508\n",
      "Epoch 38, CIFAR-10 Batch 4:  train Cost 0.643524 train accuracy 0.777027\n",
      "val Cost 1.00525 val accuracy 0.6468\n",
      "Epoch 38, CIFAR-10 Batch 5:  train Cost 0.734 train accuracy 0.777027\n",
      "val Cost 1.00993 val accuracy 0.6464\n",
      "Epoch 39, CIFAR-10 Batch 1:  train Cost 0.773511 train accuracy 0.75\n",
      "val Cost 1.00669 val accuracy 0.6436\n",
      "Epoch 39, CIFAR-10 Batch 2:  train Cost 0.796681 train accuracy 0.712838\n",
      "val Cost 1.04444 val accuracy 0.641\n",
      "Epoch 39, CIFAR-10 Batch 3:  train Cost 0.639248 train accuracy 0.787162\n",
      "val Cost 1.00255 val accuracy 0.6528\n",
      "Epoch 39, CIFAR-10 Batch 4:  train Cost 0.645503 train accuracy 0.783784\n",
      "val Cost 0.981349 val accuracy 0.6564\n",
      "Epoch 39, CIFAR-10 Batch 5:  train Cost 0.713739 train accuracy 0.787162\n",
      "val Cost 0.99937 val accuracy 0.6502\n",
      "Epoch 40, CIFAR-10 Batch 1:  train Cost 0.744577 train accuracy 0.760135\n",
      "val Cost 0.988104 val accuracy 0.6478\n",
      "Epoch 40, CIFAR-10 Batch 2:  train Cost 0.794751 train accuracy 0.709459\n",
      "val Cost 1.04581 val accuracy 0.6422\n",
      "Epoch 40, CIFAR-10 Batch 3:  train Cost 0.652569 train accuracy 0.77027\n",
      "val Cost 1.01581 val accuracy 0.6462\n",
      "Epoch 40, CIFAR-10 Batch 4:  train Cost 0.63984 train accuracy 0.780405\n",
      "val Cost 1.00788 val accuracy 0.6494\n",
      "Epoch 40, CIFAR-10 Batch 5:  train Cost 0.680026 train accuracy 0.790541\n",
      "val Cost 0.977712 val accuracy 0.655\n",
      "Epoch 41, CIFAR-10 Batch 1:  train Cost 0.748456 train accuracy 0.753378\n",
      "val Cost 1.01154 val accuracy 0.6456\n",
      "Epoch 41, CIFAR-10 Batch 2:  train Cost 0.75048 train accuracy 0.72973\n",
      "val Cost 1.02954 val accuracy 0.6478\n",
      "Epoch 41, CIFAR-10 Batch 3:  train Cost 0.646178 train accuracy 0.777027\n",
      "val Cost 1.00608 val accuracy 0.6518\n",
      "Epoch 41, CIFAR-10 Batch 4:  train Cost 0.64641 train accuracy 0.787162\n",
      "val Cost 1.01025 val accuracy 0.646\n",
      "Epoch 41, CIFAR-10 Batch 5:  train Cost 0.712357 train accuracy 0.780405\n",
      "val Cost 1.00103 val accuracy 0.6482\n",
      "Epoch 42, CIFAR-10 Batch 1:  train Cost 0.716052 train accuracy 0.75\n",
      "val Cost 0.985664 val accuracy 0.656\n",
      "Epoch 42, CIFAR-10 Batch 2:  train Cost 0.728501 train accuracy 0.739865\n",
      "val Cost 0.976359 val accuracy 0.6614\n",
      "Epoch 42, CIFAR-10 Batch 3:  train Cost 0.601386 train accuracy 0.777027\n",
      "val Cost 0.98549 val accuracy 0.6544\n",
      "Epoch 42, CIFAR-10 Batch 4:  train Cost 0.610682 train accuracy 0.793919\n",
      "val Cost 1.00001 val accuracy 0.6458\n",
      "Epoch 42, CIFAR-10 Batch 5:  train Cost 0.692309 train accuracy 0.777027\n",
      "val Cost 0.993948 val accuracy 0.6514\n",
      "Epoch 43, CIFAR-10 Batch 1:  train Cost 0.720692 train accuracy 0.756757\n",
      "val Cost 0.986438 val accuracy 0.6556\n",
      "Epoch 43, CIFAR-10 Batch 2:  train Cost 0.720065 train accuracy 0.722973\n",
      "val Cost 0.977621 val accuracy 0.6628\n",
      "Epoch 43, CIFAR-10 Batch 3:  train Cost 0.611692 train accuracy 0.77027\n",
      "val Cost 0.992824 val accuracy 0.6566\n",
      "Epoch 43, CIFAR-10 Batch 4:  train Cost 0.607789 train accuracy 0.787162\n",
      "val Cost 0.992002 val accuracy 0.6512\n",
      "Epoch 43, CIFAR-10 Batch 5:  train Cost 0.708986 train accuracy 0.787162\n",
      "val Cost 0.991871 val accuracy 0.6552\n",
      "Epoch 44, CIFAR-10 Batch 1:  train Cost 0.713503 train accuracy 0.743243\n",
      "val Cost 0.970202 val accuracy 0.6574\n",
      "Epoch 44, CIFAR-10 Batch 2:  train Cost 0.695474 train accuracy 0.756757\n",
      "val Cost 0.977346 val accuracy 0.6614\n",
      "Epoch 44, CIFAR-10 Batch 3:  train Cost 0.588654 train accuracy 0.790541\n",
      "val Cost 0.97895 val accuracy 0.6586\n",
      "Epoch 44, CIFAR-10 Batch 4:  train Cost 0.60895 train accuracy 0.777027\n",
      "val Cost 0.987159 val accuracy 0.654\n",
      "Epoch 44, CIFAR-10 Batch 5:  train Cost 0.651894 train accuracy 0.797297\n",
      "val Cost 0.972988 val accuracy 0.6658\n",
      "Epoch 45, CIFAR-10 Batch 1:  train Cost 0.708891 train accuracy 0.75\n",
      "val Cost 0.981922 val accuracy 0.6486\n",
      "Epoch 45, CIFAR-10 Batch 2:  train Cost 0.722252 train accuracy 0.743243\n",
      "val Cost 1.00339 val accuracy 0.654\n",
      "Epoch 45, CIFAR-10 Batch 3:  train Cost 0.58646 train accuracy 0.777027\n",
      "val Cost 0.98009 val accuracy 0.658\n",
      "Epoch 45, CIFAR-10 Batch 4:  train Cost 0.596532 train accuracy 0.793919\n",
      "val Cost 0.997581 val accuracy 0.6496\n",
      "Epoch 45, CIFAR-10 Batch 5:  train Cost 0.6758 train accuracy 0.800676\n",
      "val Cost 0.980559 val accuracy 0.6598\n",
      "Epoch 46, CIFAR-10 Batch 1:  train Cost 0.699216 train accuracy 0.763514\n",
      "val Cost 0.99613 val accuracy 0.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, CIFAR-10 Batch 2:  train Cost 0.729703 train accuracy 0.739865\n",
      "val Cost 1.03056 val accuracy 0.6478\n",
      "Epoch 46, CIFAR-10 Batch 3:  train Cost 0.578527 train accuracy 0.800676\n",
      "val Cost 0.96743 val accuracy 0.6588\n",
      "Epoch 46, CIFAR-10 Batch 4:  train Cost 0.566976 train accuracy 0.790541\n",
      "val Cost 0.967592 val accuracy 0.658\n",
      "Epoch 46, CIFAR-10 Batch 5:  train Cost 0.65953 train accuracy 0.793919\n",
      "val Cost 0.970518 val accuracy 0.6592\n",
      "Epoch 47, CIFAR-10 Batch 1:  train Cost 0.694378 train accuracy 0.773649\n",
      "val Cost 0.957011 val accuracy 0.6636\n",
      "Epoch 47, CIFAR-10 Batch 2:  train Cost 0.669136 train accuracy 0.756757\n",
      "val Cost 0.987677 val accuracy 0.6604\n",
      "Epoch 47, CIFAR-10 Batch 3:  train Cost 0.554776 train accuracy 0.827703\n",
      "val Cost 0.957493 val accuracy 0.6706\n",
      "Epoch 47, CIFAR-10 Batch 4:  train Cost 0.564052 train accuracy 0.807432\n",
      "val Cost 0.980006 val accuracy 0.6558\n",
      "Epoch 47, CIFAR-10 Batch 5:  train Cost 0.649117 train accuracy 0.797297\n",
      "val Cost 0.954658 val accuracy 0.668\n",
      "Epoch 48, CIFAR-10 Batch 1:  train Cost 0.663599 train accuracy 0.766892\n",
      "val Cost 0.975789 val accuracy 0.6602\n",
      "Epoch 48, CIFAR-10 Batch 2:  train Cost 0.663997 train accuracy 0.756757\n",
      "val Cost 0.973674 val accuracy 0.6632\n",
      "Epoch 48, CIFAR-10 Batch 3:  train Cost 0.543211 train accuracy 0.831081\n",
      "val Cost 0.953266 val accuracy 0.6674\n",
      "Epoch 48, CIFAR-10 Batch 4:  train Cost 0.540184 train accuracy 0.827703\n",
      "val Cost 0.97169 val accuracy 0.6582\n",
      "Epoch 48, CIFAR-10 Batch 5:  train Cost 0.655731 train accuracy 0.783784\n",
      "val Cost 0.969902 val accuracy 0.6592\n",
      "Epoch 49, CIFAR-10 Batch 1:  train Cost 0.690708 train accuracy 0.75\n",
      "val Cost 0.997006 val accuracy 0.646\n",
      "Epoch 49, CIFAR-10 Batch 2:  train Cost 0.700107 train accuracy 0.75\n",
      "val Cost 1.01102 val accuracy 0.6564\n",
      "Epoch 49, CIFAR-10 Batch 3:  train Cost 0.557376 train accuracy 0.800676\n",
      "val Cost 0.965668 val accuracy 0.6672\n",
      "Epoch 49, CIFAR-10 Batch 4:  train Cost 0.580357 train accuracy 0.804054\n",
      "val Cost 1.02217 val accuracy 0.643\n",
      "Epoch 49, CIFAR-10 Batch 5:  train Cost 0.694945 train accuracy 0.777027\n",
      "val Cost 0.996836 val accuracy 0.649\n",
      "Epoch 50, CIFAR-10 Batch 1:  train Cost 0.681593 train accuracy 0.746622\n",
      "val Cost 0.982187 val accuracy 0.6566\n",
      "Epoch 50, CIFAR-10 Batch 2:  train Cost 0.6381 train accuracy 0.777027\n",
      "val Cost 0.977779 val accuracy 0.6608\n",
      "Epoch 50, CIFAR-10 Batch 3:  train Cost 0.56002 train accuracy 0.827703\n",
      "val Cost 0.982978 val accuracy 0.6622\n",
      "Epoch 50, CIFAR-10 Batch 4:  train Cost 0.563289 train accuracy 0.814189\n",
      "val Cost 0.995656 val accuracy 0.6532\n",
      "Epoch 50, CIFAR-10 Batch 5:  train Cost 0.662565 train accuracy 0.793919\n",
      "val Cost 1.00692 val accuracy 0.6508\n",
      "Epoch 51, CIFAR-10 Batch 1:  train Cost 0.655027 train accuracy 0.773649\n",
      "val Cost 0.964978 val accuracy 0.658\n",
      "Epoch 51, CIFAR-10 Batch 2:  train Cost 0.656481 train accuracy 0.780405\n",
      "val Cost 0.988434 val accuracy 0.6586\n",
      "Epoch 51, CIFAR-10 Batch 3:  train Cost 0.539163 train accuracy 0.817568\n",
      "val Cost 0.982346 val accuracy 0.659\n",
      "Epoch 51, CIFAR-10 Batch 4:  train Cost 0.524886 train accuracy 0.831081\n",
      "val Cost 0.994149 val accuracy 0.6548\n",
      "Epoch 51, CIFAR-10 Batch 5:  train Cost 0.630456 train accuracy 0.820946\n",
      "val Cost 0.975157 val accuracy 0.663\n",
      "Epoch 52, CIFAR-10 Batch 1:  train Cost 0.648777 train accuracy 0.760135\n",
      "val Cost 0.970906 val accuracy 0.6586\n",
      "Epoch 52, CIFAR-10 Batch 2:  train Cost 0.631779 train accuracy 0.766892\n",
      "val Cost 0.982722 val accuracy 0.6604\n",
      "Epoch 52, CIFAR-10 Batch 3:  train Cost 0.538427 train accuracy 0.817568\n",
      "val Cost 0.968794 val accuracy 0.6648\n",
      "Epoch 52, CIFAR-10 Batch 4:  train Cost 0.5152 train accuracy 0.837838\n",
      "val Cost 0.959337 val accuracy 0.6688\n",
      "Epoch 52, CIFAR-10 Batch 5:  train Cost 0.674078 train accuracy 0.766892\n",
      "val Cost 1.00191 val accuracy 0.6508\n",
      "Epoch 53, CIFAR-10 Batch 1:  train Cost 0.636882 train accuracy 0.783784\n",
      "val Cost 0.96049 val accuracy 0.6634\n",
      "Epoch 53, CIFAR-10 Batch 2:  train Cost 0.662709 train accuracy 0.77027\n",
      "val Cost 1.00497 val accuracy 0.6618\n",
      "Epoch 53, CIFAR-10 Batch 3:  train Cost 0.534014 train accuracy 0.834459\n",
      "val Cost 0.957327 val accuracy 0.669\n",
      "Epoch 53, CIFAR-10 Batch 4:  train Cost 0.499078 train accuracy 0.841216\n",
      "val Cost 0.951942 val accuracy 0.6698\n",
      "Epoch 53, CIFAR-10 Batch 5:  train Cost 0.610054 train accuracy 0.814189\n",
      "val Cost 0.993981 val accuracy 0.6544\n",
      "Epoch 54, CIFAR-10 Batch 1:  train Cost 0.630619 train accuracy 0.800676\n",
      "val Cost 0.975838 val accuracy 0.661\n",
      "Epoch 54, CIFAR-10 Batch 2:  train Cost 0.634666 train accuracy 0.797297\n",
      "val Cost 0.999121 val accuracy 0.6648\n",
      "Epoch 54, CIFAR-10 Batch 3:  train Cost 0.522772 train accuracy 0.827703\n",
      "val Cost 0.952251 val accuracy 0.6674\n",
      "Epoch 54, CIFAR-10 Batch 4:  train Cost 0.499781 train accuracy 0.827703\n",
      "val Cost 0.973066 val accuracy 0.6624\n",
      "Epoch 54, CIFAR-10 Batch 5:  train Cost 0.588793 train accuracy 0.824324\n",
      "val Cost 0.968406 val accuracy 0.664\n",
      "Epoch 55, CIFAR-10 Batch 1:  train Cost 0.617654 train accuracy 0.790541\n",
      "val Cost 0.964093 val accuracy 0.6664\n",
      "Epoch 55, CIFAR-10 Batch 2:  train Cost 0.600905 train accuracy 0.797297\n",
      "val Cost 0.967274 val accuracy 0.6688\n",
      "Epoch 55, CIFAR-10 Batch 3:  train Cost 0.537356 train accuracy 0.837838\n",
      "val Cost 0.971992 val accuracy 0.6644\n",
      "Epoch 55, CIFAR-10 Batch 4:  train Cost 0.487399 train accuracy 0.844595\n",
      "val Cost 0.961076 val accuracy 0.6684\n",
      "Epoch 55, CIFAR-10 Batch 5:  train Cost 0.604045 train accuracy 0.810811\n",
      "val Cost 0.976458 val accuracy 0.6658\n",
      "Epoch 56, CIFAR-10 Batch 1:  train Cost 0.627551 train accuracy 0.790541\n",
      "val Cost 1.00777 val accuracy 0.6532\n",
      "Epoch 56, CIFAR-10 Batch 2:  train Cost 0.611808 train accuracy 0.790541\n",
      "val Cost 0.984328 val accuracy 0.6662\n",
      "Epoch 56, CIFAR-10 Batch 3:  train Cost 0.577307 train accuracy 0.807432\n",
      "val Cost 1.0208 val accuracy 0.6624\n",
      "Epoch 56, CIFAR-10 Batch 4:  train Cost 0.528285 train accuracy 0.827703\n",
      "val Cost 0.98193 val accuracy 0.6568\n",
      "Epoch 56, CIFAR-10 Batch 5:  train Cost 0.576796 train accuracy 0.834459\n",
      "val Cost 0.955717 val accuracy 0.672\n",
      "Epoch 57, CIFAR-10 Batch 1:  train Cost 0.615819 train accuracy 0.790541\n",
      "val Cost 0.978551 val accuracy 0.6664\n",
      "Epoch 57, CIFAR-10 Batch 2:  train Cost 0.59891 train accuracy 0.783784\n",
      "val Cost 0.943739 val accuracy 0.6712\n",
      "Epoch 57, CIFAR-10 Batch 3:  train Cost 0.568442 train accuracy 0.817568\n",
      "val Cost 0.998557 val accuracy 0.6556\n",
      "Epoch 57, CIFAR-10 Batch 4:  train Cost 0.547935 train accuracy 0.814189\n",
      "val Cost 0.965634 val accuracy 0.6668\n",
      "Epoch 57, CIFAR-10 Batch 5:  train Cost 0.600608 train accuracy 0.824324\n",
      "val Cost 0.950174 val accuracy 0.67\n",
      "Epoch 58, CIFAR-10 Batch 1:  train Cost 0.604231 train accuracy 0.783784\n",
      "val Cost 0.958874 val accuracy 0.672\n",
      "Epoch 58, CIFAR-10 Batch 2:  train Cost 0.590947 train accuracy 0.790541\n",
      "val Cost 0.952464 val accuracy 0.6746\n",
      "Epoch 58, CIFAR-10 Batch 3:  train Cost 0.516777 train accuracy 0.824324\n",
      "val Cost 0.960047 val accuracy 0.6684\n",
      "Epoch 58, CIFAR-10 Batch 4:  train Cost 0.482021 train accuracy 0.851351\n",
      "val Cost 0.970285 val accuracy 0.6656\n",
      "Epoch 58, CIFAR-10 Batch 5:  train Cost 0.584819 train accuracy 0.827703\n",
      "val Cost 0.933518 val accuracy 0.6786\n",
      "Epoch 59, CIFAR-10 Batch 1:  train Cost 0.598482 train accuracy 0.783784\n",
      "val Cost 0.956512 val accuracy 0.6704\n",
      "Epoch 59, CIFAR-10 Batch 2:  train Cost 0.588277 train accuracy 0.804054\n",
      "val Cost 0.956826 val accuracy 0.6692\n",
      "Epoch 59, CIFAR-10 Batch 3:  train Cost 0.51933 train accuracy 0.834459\n",
      "val Cost 0.962179 val accuracy 0.673\n",
      "Epoch 59, CIFAR-10 Batch 4:  train Cost 0.481489 train accuracy 0.841216\n",
      "val Cost 0.948044 val accuracy 0.6718\n",
      "Epoch 59, CIFAR-10 Batch 5:  train Cost 0.612156 train accuracy 0.804054\n",
      "val Cost 0.955786 val accuracy 0.6726\n",
      "Epoch 60, CIFAR-10 Batch 1:  train Cost 0.586433 train accuracy 0.793919\n",
      "val Cost 0.961126 val accuracy 0.6694\n",
      "Epoch 60, CIFAR-10 Batch 2:  train Cost 0.592186 train accuracy 0.797297\n",
      "val Cost 0.939454 val accuracy 0.6768\n",
      "Epoch 60, CIFAR-10 Batch 3:  train Cost 0.5078 train accuracy 0.844595\n",
      "val Cost 0.957669 val accuracy 0.674\n",
      "Epoch 60, CIFAR-10 Batch 4:  train Cost 0.468401 train accuracy 0.858108\n",
      "val Cost 0.946505 val accuracy 0.6758\n",
      "Epoch 60, CIFAR-10 Batch 5:  train Cost 0.561227 train accuracy 0.817568\n",
      "val Cost 0.946545 val accuracy 0.6732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, CIFAR-10 Batch 1:  train Cost 0.607146 train accuracy 0.790541\n",
      "val Cost 0.968051 val accuracy 0.6654\n",
      "Epoch 61, CIFAR-10 Batch 2:  train Cost 0.560773 train accuracy 0.817568\n",
      "val Cost 0.938019 val accuracy 0.678\n",
      "Epoch 61, CIFAR-10 Batch 3:  train Cost 0.499222 train accuracy 0.824324\n",
      "val Cost 0.965639 val accuracy 0.6712\n",
      "Epoch 61, CIFAR-10 Batch 4:  train Cost 0.478065 train accuracy 0.858108\n",
      "val Cost 0.959166 val accuracy 0.6668\n",
      "Epoch 61, CIFAR-10 Batch 5:  train Cost 0.569604 train accuracy 0.817568\n",
      "val Cost 0.953857 val accuracy 0.6724\n",
      "Epoch 62, CIFAR-10 Batch 1:  train Cost 0.597326 train accuracy 0.807432\n",
      "val Cost 0.974324 val accuracy 0.6732\n",
      "Epoch 62, CIFAR-10 Batch 2:  train Cost 0.643891 train accuracy 0.763514\n",
      "val Cost 0.973665 val accuracy 0.666\n",
      "Epoch 62, CIFAR-10 Batch 3:  train Cost 0.508808 train accuracy 0.831081\n",
      "val Cost 0.956915 val accuracy 0.6732\n",
      "Epoch 62, CIFAR-10 Batch 4:  train Cost 0.479416 train accuracy 0.85473\n",
      "val Cost 0.949373 val accuracy 0.6744\n",
      "Epoch 62, CIFAR-10 Batch 5:  train Cost 0.561123 train accuracy 0.824324\n",
      "val Cost 0.94601 val accuracy 0.6766\n",
      "Epoch 63, CIFAR-10 Batch 1:  train Cost 0.629007 train accuracy 0.787162\n",
      "val Cost 0.993628 val accuracy 0.6662\n",
      "Epoch 63, CIFAR-10 Batch 2:  train Cost 0.601246 train accuracy 0.780405\n",
      "val Cost 0.946519 val accuracy 0.675\n",
      "Epoch 63, CIFAR-10 Batch 3:  train Cost 0.497229 train accuracy 0.844595\n",
      "val Cost 0.966646 val accuracy 0.6726\n",
      "Epoch 63, CIFAR-10 Batch 4:  train Cost 0.48562 train accuracy 0.841216\n",
      "val Cost 0.954298 val accuracy 0.6734\n",
      "Epoch 63, CIFAR-10 Batch 5:  train Cost 0.52977 train accuracy 0.841216\n",
      "val Cost 0.951626 val accuracy 0.6736\n",
      "Epoch 64, CIFAR-10 Batch 1:  train Cost 0.577878 train accuracy 0.800676\n",
      "val Cost 0.960359 val accuracy 0.6726\n",
      "Epoch 64, CIFAR-10 Batch 2:  train Cost 0.575925 train accuracy 0.810811\n",
      "val Cost 0.950674 val accuracy 0.679\n",
      "Epoch 64, CIFAR-10 Batch 3:  train Cost 0.487532 train accuracy 0.841216\n",
      "val Cost 0.965868 val accuracy 0.677\n",
      "Epoch 64, CIFAR-10 Batch 4:  train Cost 0.46293 train accuracy 0.858108\n",
      "val Cost 0.934941 val accuracy 0.6794\n",
      "Epoch 64, CIFAR-10 Batch 5:  train Cost 0.540329 train accuracy 0.817568\n",
      "val Cost 0.955729 val accuracy 0.6776\n",
      "Epoch 65, CIFAR-10 Batch 1:  train Cost 0.599914 train accuracy 0.780405\n",
      "val Cost 0.983292 val accuracy 0.6688\n",
      "Epoch 65, CIFAR-10 Batch 2:  train Cost 0.555305 train accuracy 0.804054\n",
      "val Cost 0.976116 val accuracy 0.6702\n",
      "Epoch 65, CIFAR-10 Batch 3:  train Cost 0.46972 train accuracy 0.851351\n",
      "val Cost 0.957839 val accuracy 0.676\n",
      "Epoch 65, CIFAR-10 Batch 4:  train Cost 0.451908 train accuracy 0.851351\n",
      "val Cost 0.934461 val accuracy 0.6844\n",
      "Epoch 65, CIFAR-10 Batch 5:  train Cost 0.531335 train accuracy 0.827703\n",
      "val Cost 0.944098 val accuracy 0.68\n",
      "Epoch 66, CIFAR-10 Batch 1:  train Cost 0.590077 train accuracy 0.790541\n",
      "val Cost 0.979352 val accuracy 0.667\n",
      "Epoch 66, CIFAR-10 Batch 2:  train Cost 0.549255 train accuracy 0.814189\n",
      "val Cost 0.949323 val accuracy 0.6798\n",
      "Epoch 66, CIFAR-10 Batch 3:  train Cost 0.473884 train accuracy 0.847973\n",
      "val Cost 0.964717 val accuracy 0.675\n",
      "Epoch 66, CIFAR-10 Batch 4:  train Cost 0.464959 train accuracy 0.861486\n",
      "val Cost 0.959009 val accuracy 0.6766\n",
      "Epoch 66, CIFAR-10 Batch 5:  train Cost 0.51669 train accuracy 0.827703\n",
      "val Cost 0.94977 val accuracy 0.683\n",
      "Epoch 67, CIFAR-10 Batch 1:  train Cost 0.55283 train accuracy 0.810811\n",
      "val Cost 0.945488 val accuracy 0.682\n",
      "Epoch 67, CIFAR-10 Batch 2:  train Cost 0.562302 train accuracy 0.797297\n",
      "val Cost 0.965465 val accuracy 0.674\n",
      "Epoch 67, CIFAR-10 Batch 3:  train Cost 0.494444 train accuracy 0.844595\n",
      "val Cost 0.983465 val accuracy 0.6732\n",
      "Epoch 67, CIFAR-10 Batch 4:  train Cost 0.454153 train accuracy 0.837838\n",
      "val Cost 0.93798 val accuracy 0.6842\n",
      "Epoch 67, CIFAR-10 Batch 5:  train Cost 0.506708 train accuracy 0.837838\n",
      "val Cost 0.963955 val accuracy 0.679\n",
      "Epoch 68, CIFAR-10 Batch 1:  train Cost 0.547907 train accuracy 0.793919\n",
      "val Cost 0.967214 val accuracy 0.6752\n",
      "Epoch 68, CIFAR-10 Batch 2:  train Cost 0.557295 train accuracy 0.817568\n",
      "val Cost 0.976633 val accuracy 0.6674\n",
      "Epoch 68, CIFAR-10 Batch 3:  train Cost 0.463925 train accuracy 0.851351\n",
      "val Cost 0.958675 val accuracy 0.6806\n",
      "Epoch 68, CIFAR-10 Batch 4:  train Cost 0.456943 train accuracy 0.837838\n",
      "val Cost 0.938267 val accuracy 0.6802\n",
      "Epoch 68, CIFAR-10 Batch 5:  train Cost 0.508242 train accuracy 0.844595\n",
      "val Cost 0.956757 val accuracy 0.6816\n",
      "Epoch 69, CIFAR-10 Batch 1:  train Cost 0.545596 train accuracy 0.793919\n",
      "val Cost 0.948277 val accuracy 0.6784\n",
      "Epoch 69, CIFAR-10 Batch 2:  train Cost 0.518077 train accuracy 0.851351\n",
      "val Cost 0.937085 val accuracy 0.686\n",
      "Epoch 69, CIFAR-10 Batch 3:  train Cost 0.493817 train accuracy 0.844595\n",
      "val Cost 1.01341 val accuracy 0.6618\n",
      "Epoch 69, CIFAR-10 Batch 4:  train Cost 0.465576 train accuracy 0.847973\n",
      "val Cost 0.960033 val accuracy 0.676\n",
      "Epoch 69, CIFAR-10 Batch 5:  train Cost 0.515238 train accuracy 0.827703\n",
      "val Cost 0.966138 val accuracy 0.6732\n",
      "Epoch 70, CIFAR-10 Batch 1:  train Cost 0.516562 train accuracy 0.820946\n",
      "val Cost 0.936389 val accuracy 0.6836\n",
      "Epoch 70, CIFAR-10 Batch 2:  train Cost 0.519024 train accuracy 0.834459\n",
      "val Cost 0.934475 val accuracy 0.682\n",
      "Epoch 70, CIFAR-10 Batch 3:  train Cost 0.462466 train accuracy 0.847973\n",
      "val Cost 0.961259 val accuracy 0.6772\n",
      "Epoch 70, CIFAR-10 Batch 4:  train Cost 0.430203 train accuracy 0.868243\n",
      "val Cost 0.928543 val accuracy 0.6838\n",
      "Epoch 70, CIFAR-10 Batch 5:  train Cost 0.5001 train accuracy 0.837838\n",
      "val Cost 0.956902 val accuracy 0.682\n",
      "Epoch 71, CIFAR-10 Batch 1:  train Cost 0.523833 train accuracy 0.820946\n",
      "val Cost 0.937201 val accuracy 0.6856\n",
      "Epoch 71, CIFAR-10 Batch 2:  train Cost 0.51594 train accuracy 0.827703\n",
      "val Cost 0.927392 val accuracy 0.6812\n",
      "Epoch 71, CIFAR-10 Batch 3:  train Cost 0.462303 train accuracy 0.844595\n",
      "val Cost 0.966425 val accuracy 0.678\n",
      "Epoch 71, CIFAR-10 Batch 4:  train Cost 0.445341 train accuracy 0.847973\n",
      "val Cost 0.941534 val accuracy 0.6784\n",
      "Epoch 71, CIFAR-10 Batch 5:  train Cost 0.480468 train accuracy 0.851351\n",
      "val Cost 0.949592 val accuracy 0.6816\n",
      "Epoch 72, CIFAR-10 Batch 1:  train Cost 0.523683 train accuracy 0.810811\n",
      "val Cost 0.93751 val accuracy 0.6862\n",
      "Epoch 72, CIFAR-10 Batch 2:  train Cost 0.509506 train accuracy 0.831081\n",
      "val Cost 0.929681 val accuracy 0.6892\n",
      "Epoch 72, CIFAR-10 Batch 3:  train Cost 0.47319 train accuracy 0.841216\n",
      "val Cost 0.990296 val accuracy 0.6726\n",
      "Epoch 72, CIFAR-10 Batch 4:  train Cost 0.447609 train accuracy 0.847973\n",
      "val Cost 0.952299 val accuracy 0.6756\n",
      "Epoch 72, CIFAR-10 Batch 5:  train Cost 0.471234 train accuracy 0.841216\n",
      "val Cost 0.957249 val accuracy 0.6824\n",
      "Epoch 73, CIFAR-10 Batch 1:  train Cost 0.511634 train accuracy 0.820946\n",
      "val Cost 0.944829 val accuracy 0.6824\n",
      "Epoch 73, CIFAR-10 Batch 2:  train Cost 0.502167 train accuracy 0.831081\n",
      "val Cost 0.934944 val accuracy 0.6886\n",
      "Epoch 73, CIFAR-10 Batch 3:  train Cost 0.460956 train accuracy 0.851351\n",
      "val Cost 0.963466 val accuracy 0.6784\n",
      "Epoch 73, CIFAR-10 Batch 4:  train Cost 0.424306 train accuracy 0.871622\n",
      "val Cost 0.935115 val accuracy 0.6848\n",
      "Epoch 73, CIFAR-10 Batch 5:  train Cost 0.466912 train accuracy 0.847973\n",
      "val Cost 0.958218 val accuracy 0.6872\n",
      "Epoch 74, CIFAR-10 Batch 1:  train Cost 0.502032 train accuracy 0.807432\n",
      "val Cost 0.941621 val accuracy 0.6846\n",
      "Epoch 74, CIFAR-10 Batch 2:  train Cost 0.49022 train accuracy 0.851351\n",
      "val Cost 0.926607 val accuracy 0.6898\n",
      "Epoch 74, CIFAR-10 Batch 3:  train Cost 0.46087 train accuracy 0.851351\n",
      "val Cost 0.983225 val accuracy 0.681\n",
      "Epoch 74, CIFAR-10 Batch 4:  train Cost 0.438483 train accuracy 0.858108\n",
      "val Cost 0.960204 val accuracy 0.6782\n",
      "Epoch 74, CIFAR-10 Batch 5:  train Cost 0.453802 train accuracy 0.858108\n",
      "val Cost 0.952393 val accuracy 0.6804\n",
      "Epoch 75, CIFAR-10 Batch 1:  train Cost 0.50811 train accuracy 0.800676\n",
      "val Cost 0.954607 val accuracy 0.6802\n",
      "Epoch 75, CIFAR-10 Batch 2:  train Cost 0.482741 train accuracy 0.851351\n",
      "val Cost 0.92282 val accuracy 0.6942\n",
      "Epoch 75, CIFAR-10 Batch 3:  train Cost 0.440021 train accuracy 0.878378\n",
      "val Cost 0.985812 val accuracy 0.6808\n",
      "Epoch 75, CIFAR-10 Batch 4:  train Cost 0.428457 train accuracy 0.847973\n",
      "val Cost 0.943486 val accuracy 0.6814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, CIFAR-10 Batch 5:  train Cost 0.472421 train accuracy 0.851351\n",
      "val Cost 0.977736 val accuracy 0.6758\n",
      "Epoch 76, CIFAR-10 Batch 1:  train Cost 0.505109 train accuracy 0.807432\n",
      "val Cost 0.958744 val accuracy 0.6782\n",
      "Epoch 76, CIFAR-10 Batch 2:  train Cost 0.500364 train accuracy 0.834459\n",
      "val Cost 0.94973 val accuracy 0.6836\n",
      "Epoch 76, CIFAR-10 Batch 3:  train Cost 0.446777 train accuracy 0.847973\n",
      "val Cost 0.964592 val accuracy 0.6798\n",
      "Epoch 76, CIFAR-10 Batch 4:  train Cost 0.436412 train accuracy 0.847973\n",
      "val Cost 0.961774 val accuracy 0.675\n",
      "Epoch 76, CIFAR-10 Batch 5:  train Cost 0.448912 train accuracy 0.85473\n",
      "val Cost 0.978418 val accuracy 0.678\n",
      "Epoch 77, CIFAR-10 Batch 1:  train Cost 0.515885 train accuracy 0.837838\n",
      "val Cost 0.962907 val accuracy 0.6812\n",
      "Epoch 77, CIFAR-10 Batch 2:  train Cost 0.492524 train accuracy 0.837838\n",
      "val Cost 0.949051 val accuracy 0.6802\n",
      "Epoch 77, CIFAR-10 Batch 3:  train Cost 0.425687 train accuracy 0.861486\n",
      "val Cost 0.94321 val accuracy 0.6892\n",
      "Epoch 77, CIFAR-10 Batch 4:  train Cost 0.418371 train accuracy 0.858108\n",
      "val Cost 0.95424 val accuracy 0.6804\n",
      "Epoch 77, CIFAR-10 Batch 5:  train Cost 0.47587 train accuracy 0.844595\n",
      "val Cost 0.974072 val accuracy 0.6798\n",
      "Epoch 78, CIFAR-10 Batch 1:  train Cost 0.524153 train accuracy 0.810811\n",
      "val Cost 0.944465 val accuracy 0.6838\n",
      "Epoch 78, CIFAR-10 Batch 2:  train Cost 0.509669 train accuracy 0.834459\n",
      "val Cost 0.956339 val accuracy 0.685\n",
      "Epoch 78, CIFAR-10 Batch 3:  train Cost 0.419709 train accuracy 0.875\n",
      "val Cost 0.923892 val accuracy 0.6914\n",
      "Epoch 78, CIFAR-10 Batch 4:  train Cost 0.410401 train accuracy 0.868243\n",
      "val Cost 0.965131 val accuracy 0.6786\n",
      "Epoch 78, CIFAR-10 Batch 5:  train Cost 0.477672 train accuracy 0.851351\n",
      "val Cost 0.98455 val accuracy 0.6744\n",
      "Epoch 79, CIFAR-10 Batch 1:  train Cost 0.486738 train accuracy 0.820946\n",
      "val Cost 0.948119 val accuracy 0.6798\n",
      "Epoch 79, CIFAR-10 Batch 2:  train Cost 0.506719 train accuracy 0.817568\n",
      "val Cost 0.944728 val accuracy 0.6866\n",
      "Epoch 79, CIFAR-10 Batch 3:  train Cost 0.417189 train accuracy 0.875\n",
      "val Cost 0.939696 val accuracy 0.6898\n",
      "Epoch 79, CIFAR-10 Batch 4:  train Cost 0.412501 train accuracy 0.868243\n",
      "val Cost 0.965167 val accuracy 0.6836\n",
      "Epoch 79, CIFAR-10 Batch 5:  train Cost 0.471147 train accuracy 0.834459\n",
      "val Cost 0.969568 val accuracy 0.6752\n",
      "Epoch 80, CIFAR-10 Batch 1:  train Cost 0.49864 train accuracy 0.824324\n",
      "val Cost 0.965735 val accuracy 0.6818\n",
      "Epoch 80, CIFAR-10 Batch 2:  train Cost 0.530109 train accuracy 0.814189\n",
      "val Cost 0.967333 val accuracy 0.6792\n",
      "Epoch 80, CIFAR-10 Batch 3:  train Cost 0.430214 train accuracy 0.881757\n",
      "val Cost 0.944848 val accuracy 0.6856\n",
      "Epoch 80, CIFAR-10 Batch 4:  train Cost 0.411775 train accuracy 0.861486\n",
      "val Cost 0.950556 val accuracy 0.6826\n",
      "Epoch 80, CIFAR-10 Batch 5:  train Cost 0.473062 train accuracy 0.834459\n",
      "val Cost 0.982742 val accuracy 0.6758\n",
      "Epoch 81, CIFAR-10 Batch 1:  train Cost 0.526401 train accuracy 0.837838\n",
      "val Cost 0.986629 val accuracy 0.68\n",
      "Epoch 81, CIFAR-10 Batch 2:  train Cost 0.526147 train accuracy 0.817568\n",
      "val Cost 0.994533 val accuracy 0.67\n",
      "Epoch 81, CIFAR-10 Batch 3:  train Cost 0.434728 train accuracy 0.871622\n",
      "val Cost 0.954324 val accuracy 0.6842\n",
      "Epoch 81, CIFAR-10 Batch 4:  train Cost 0.413379 train accuracy 0.85473\n",
      "val Cost 0.950305 val accuracy 0.68\n",
      "Epoch 81, CIFAR-10 Batch 5:  train Cost 0.450462 train accuracy 0.841216\n",
      "val Cost 0.944101 val accuracy 0.6828\n",
      "Epoch 82, CIFAR-10 Batch 1:  train Cost 0.480507 train accuracy 0.824324\n",
      "val Cost 0.938551 val accuracy 0.687\n",
      "Epoch 82, CIFAR-10 Batch 2:  train Cost 0.495453 train accuracy 0.844595\n",
      "val Cost 0.955202 val accuracy 0.6888\n",
      "Epoch 82, CIFAR-10 Batch 3:  train Cost 0.436931 train accuracy 0.868243\n",
      "val Cost 0.978998 val accuracy 0.6822\n",
      "Epoch 82, CIFAR-10 Batch 4:  train Cost 0.400587 train accuracy 0.875\n",
      "val Cost 0.957057 val accuracy 0.6778\n",
      "Epoch 82, CIFAR-10 Batch 5:  train Cost 0.426752 train accuracy 0.858108\n",
      "val Cost 0.944512 val accuracy 0.688\n",
      "Epoch 83, CIFAR-10 Batch 1:  train Cost 0.473874 train accuracy 0.834459\n",
      "val Cost 0.944773 val accuracy 0.6876\n",
      "Epoch 83, CIFAR-10 Batch 2:  train Cost 0.488925 train accuracy 0.834459\n",
      "val Cost 0.962079 val accuracy 0.6814\n",
      "Epoch 83, CIFAR-10 Batch 3:  train Cost 0.440416 train accuracy 0.871622\n",
      "val Cost 0.968584 val accuracy 0.6866\n",
      "Epoch 83, CIFAR-10 Batch 4:  train Cost 0.434997 train accuracy 0.837838\n",
      "val Cost 0.985496 val accuracy 0.6694\n",
      "Epoch 83, CIFAR-10 Batch 5:  train Cost 0.432344 train accuracy 0.837838\n",
      "val Cost 0.950948 val accuracy 0.6838\n",
      "Epoch 84, CIFAR-10 Batch 1:  train Cost 0.507079 train accuracy 0.804054\n",
      "val Cost 0.957853 val accuracy 0.6836\n",
      "Epoch 84, CIFAR-10 Batch 2:  train Cost 0.508553 train accuracy 0.827703\n",
      "val Cost 0.994399 val accuracy 0.6706\n",
      "Epoch 84, CIFAR-10 Batch 3:  train Cost 0.426291 train accuracy 0.881757\n",
      "val Cost 0.952996 val accuracy 0.6868\n",
      "Epoch 84, CIFAR-10 Batch 4:  train Cost 0.380697 train accuracy 0.871622\n",
      "val Cost 0.95556 val accuracy 0.679\n",
      "Epoch 84, CIFAR-10 Batch 5:  train Cost 0.420505 train accuracy 0.861486\n",
      "val Cost 0.948486 val accuracy 0.6868\n",
      "Epoch 85, CIFAR-10 Batch 1:  train Cost 0.469528 train accuracy 0.831081\n",
      "val Cost 0.935935 val accuracy 0.6908\n",
      "Epoch 85, CIFAR-10 Batch 2:  train Cost 0.478415 train accuracy 0.831081\n",
      "val Cost 0.979908 val accuracy 0.6762\n",
      "Epoch 85, CIFAR-10 Batch 3:  train Cost 0.424114 train accuracy 0.868243\n",
      "val Cost 0.973974 val accuracy 0.6798\n",
      "Epoch 85, CIFAR-10 Batch 4:  train Cost 0.403313 train accuracy 0.858108\n",
      "val Cost 0.973064 val accuracy 0.675\n",
      "Epoch 85, CIFAR-10 Batch 5:  train Cost 0.428605 train accuracy 0.858108\n",
      "val Cost 0.959241 val accuracy 0.6798\n",
      "Epoch 86, CIFAR-10 Batch 1:  train Cost 0.461003 train accuracy 0.837838\n",
      "val Cost 0.942687 val accuracy 0.6838\n",
      "Epoch 86, CIFAR-10 Batch 2:  train Cost 0.496687 train accuracy 0.834459\n",
      "val Cost 0.962945 val accuracy 0.6834\n",
      "Epoch 86, CIFAR-10 Batch 3:  train Cost 0.421912 train accuracy 0.868243\n",
      "val Cost 0.986885 val accuracy 0.6808\n",
      "Epoch 86, CIFAR-10 Batch 4:  train Cost 0.404359 train accuracy 0.868243\n",
      "val Cost 0.975425 val accuracy 0.6772\n",
      "Epoch 86, CIFAR-10 Batch 5:  train Cost 0.438544 train accuracy 0.85473\n",
      "val Cost 0.967681 val accuracy 0.6798\n",
      "Epoch 87, CIFAR-10 Batch 1:  train Cost 0.477653 train accuracy 0.827703\n",
      "val Cost 0.932626 val accuracy 0.6908\n",
      "Epoch 87, CIFAR-10 Batch 2:  train Cost 0.451261 train accuracy 0.851351\n",
      "val Cost 0.934476 val accuracy 0.6892\n",
      "Epoch 87, CIFAR-10 Batch 3:  train Cost 0.394845 train accuracy 0.888514\n",
      "val Cost 0.967736 val accuracy 0.6826\n",
      "Epoch 87, CIFAR-10 Batch 4:  train Cost 0.407748 train accuracy 0.851351\n",
      "val Cost 0.976402 val accuracy 0.6802\n",
      "Epoch 87, CIFAR-10 Batch 5:  train Cost 0.419579 train accuracy 0.858108\n",
      "val Cost 0.964744 val accuracy 0.6782\n",
      "Epoch 88, CIFAR-10 Batch 1:  train Cost 0.443744 train accuracy 0.851351\n",
      "val Cost 0.949174 val accuracy 0.6884\n",
      "Epoch 88, CIFAR-10 Batch 2:  train Cost 0.473097 train accuracy 0.834459\n",
      "val Cost 0.973719 val accuracy 0.6812\n",
      "Epoch 88, CIFAR-10 Batch 3:  train Cost 0.403426 train accuracy 0.89527\n",
      "val Cost 0.977535 val accuracy 0.6836\n",
      "Epoch 88, CIFAR-10 Batch 4:  train Cost 0.366975 train accuracy 0.881757\n",
      "val Cost 0.952798 val accuracy 0.6872\n",
      "Epoch 88, CIFAR-10 Batch 5:  train Cost 0.438407 train accuracy 0.85473\n",
      "val Cost 0.991767 val accuracy 0.671\n",
      "Epoch 89, CIFAR-10 Batch 1:  train Cost 0.457484 train accuracy 0.841216\n",
      "val Cost 0.935466 val accuracy 0.692\n",
      "Epoch 89, CIFAR-10 Batch 2:  train Cost 0.468136 train accuracy 0.837838\n",
      "val Cost 0.98206 val accuracy 0.6798\n",
      "Epoch 89, CIFAR-10 Batch 3:  train Cost 0.419752 train accuracy 0.888514\n",
      "val Cost 0.991984 val accuracy 0.6784\n",
      "Epoch 89, CIFAR-10 Batch 4:  train Cost 0.387048 train accuracy 0.878378\n",
      "val Cost 0.9864 val accuracy 0.6734\n",
      "Epoch 89, CIFAR-10 Batch 5:  train Cost 0.440285 train accuracy 0.85473\n",
      "val Cost 0.996289 val accuracy 0.6724\n",
      "Epoch 90, CIFAR-10 Batch 1:  train Cost 0.458179 train accuracy 0.837838\n",
      "val Cost 0.955857 val accuracy 0.6816\n",
      "Epoch 90, CIFAR-10 Batch 2:  train Cost 0.443793 train accuracy 0.871622\n",
      "val Cost 0.945675 val accuracy 0.6874\n",
      "Epoch 90, CIFAR-10 Batch 3:  train Cost 0.407244 train accuracy 0.881757\n",
      "val Cost 0.984473 val accuracy 0.6774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, CIFAR-10 Batch 4:  train Cost 0.400029 train accuracy 0.871622\n",
      "val Cost 0.978212 val accuracy 0.6818\n",
      "Epoch 90, CIFAR-10 Batch 5:  train Cost 0.419889 train accuracy 0.861486\n",
      "val Cost 1.00922 val accuracy 0.6724\n",
      "Epoch 91, CIFAR-10 Batch 1:  train Cost 0.447625 train accuracy 0.837838\n",
      "val Cost 0.946294 val accuracy 0.6892\n",
      "Epoch 91, CIFAR-10 Batch 2:  train Cost 0.468823 train accuracy 0.834459\n",
      "val Cost 0.983425 val accuracy 0.6782\n",
      "Epoch 91, CIFAR-10 Batch 3:  train Cost 0.427217 train accuracy 0.871622\n",
      "val Cost 1.00633 val accuracy 0.674\n",
      "Epoch 91, CIFAR-10 Batch 4:  train Cost 0.383491 train accuracy 0.885135\n",
      "val Cost 0.967156 val accuracy 0.682\n",
      "Epoch 91, CIFAR-10 Batch 5:  train Cost 0.427386 train accuracy 0.851351\n",
      "val Cost 0.975371 val accuracy 0.6758\n",
      "Epoch 92, CIFAR-10 Batch 1:  train Cost 0.449662 train accuracy 0.827703\n",
      "val Cost 0.960115 val accuracy 0.6832\n",
      "Epoch 92, CIFAR-10 Batch 2:  train Cost 0.454788 train accuracy 0.844595\n",
      "val Cost 0.981198 val accuracy 0.6796\n",
      "Epoch 92, CIFAR-10 Batch 3:  train Cost 0.425693 train accuracy 0.861486\n",
      "val Cost 1.0137 val accuracy 0.6774\n",
      "Epoch 92, CIFAR-10 Batch 4:  train Cost 0.39697 train accuracy 0.875\n",
      "val Cost 0.978177 val accuracy 0.6782\n",
      "Epoch 92, CIFAR-10 Batch 5:  train Cost 0.421711 train accuracy 0.864865\n",
      "val Cost 0.996458 val accuracy 0.6672\n",
      "Epoch 93, CIFAR-10 Batch 1:  train Cost 0.465143 train accuracy 0.837838\n",
      "val Cost 0.951215 val accuracy 0.6822\n",
      "Epoch 93, CIFAR-10 Batch 2:  train Cost 0.473514 train accuracy 0.834459\n",
      "val Cost 0.990143 val accuracy 0.6826\n",
      "Epoch 93, CIFAR-10 Batch 3:  train Cost 0.418219 train accuracy 0.885135\n",
      "val Cost 1.02204 val accuracy 0.675\n",
      "Epoch 93, CIFAR-10 Batch 4:  train Cost 0.391397 train accuracy 0.875\n",
      "val Cost 0.955462 val accuracy 0.6836\n",
      "Epoch 93, CIFAR-10 Batch 5:  train Cost 0.449179 train accuracy 0.841216\n",
      "val Cost 1.04473 val accuracy 0.6562\n",
      "Epoch 94, CIFAR-10 Batch 1:  train Cost 0.463612 train accuracy 0.861486\n",
      "val Cost 0.962792 val accuracy 0.6886\n",
      "Epoch 94, CIFAR-10 Batch 2:  train Cost 0.464697 train accuracy 0.837838\n",
      "val Cost 0.968641 val accuracy 0.6842\n",
      "Epoch 94, CIFAR-10 Batch 3:  train Cost 0.447764 train accuracy 0.858108\n",
      "val Cost 1.07061 val accuracy 0.6626\n",
      "Epoch 94, CIFAR-10 Batch 4:  train Cost 0.410302 train accuracy 0.871622\n",
      "val Cost 0.966211 val accuracy 0.6796\n",
      "Epoch 94, CIFAR-10 Batch 5:  train Cost 0.424835 train accuracy 0.861486\n",
      "val Cost 0.98471 val accuracy 0.6766\n",
      "Epoch 95, CIFAR-10 Batch 1:  train Cost 0.440883 train accuracy 0.844595\n",
      "val Cost 0.952086 val accuracy 0.6852\n",
      "Epoch 95, CIFAR-10 Batch 2:  train Cost 0.47006 train accuracy 0.85473\n",
      "val Cost 0.939845 val accuracy 0.6954\n",
      "Epoch 95, CIFAR-10 Batch 3:  train Cost 0.487072 train accuracy 0.837838\n",
      "val Cost 1.09194 val accuracy 0.663\n",
      "Epoch 95, CIFAR-10 Batch 4:  train Cost 0.410353 train accuracy 0.875\n",
      "val Cost 0.977811 val accuracy 0.6772\n",
      "Epoch 95, CIFAR-10 Batch 5:  train Cost 0.452377 train accuracy 0.844595\n",
      "val Cost 1.00103 val accuracy 0.6694\n",
      "Epoch 96, CIFAR-10 Batch 1:  train Cost 0.457675 train accuracy 0.844595\n",
      "val Cost 0.948531 val accuracy 0.6798\n",
      "Epoch 96, CIFAR-10 Batch 2:  train Cost 0.453686 train accuracy 0.864865\n",
      "val Cost 0.950245 val accuracy 0.6884\n",
      "Epoch 96, CIFAR-10 Batch 3:  train Cost 0.475118 train accuracy 0.861486\n",
      "val Cost 1.09914 val accuracy 0.6592\n",
      "Epoch 96, CIFAR-10 Batch 4:  train Cost 0.46541 train accuracy 0.858108\n",
      "val Cost 1.00509 val accuracy 0.6704\n",
      "Epoch 96, CIFAR-10 Batch 5:  train Cost 0.446633 train accuracy 0.85473\n",
      "val Cost 0.957214 val accuracy 0.6822\n",
      "Epoch 97, CIFAR-10 Batch 1:  train Cost 0.491267 train accuracy 0.820946\n",
      "val Cost 0.973983 val accuracy 0.6684\n",
      "Epoch 97, CIFAR-10 Batch 2:  train Cost 0.436733 train accuracy 0.868243\n",
      "val Cost 0.946192 val accuracy 0.6884\n",
      "Epoch 97, CIFAR-10 Batch 3:  train Cost 0.415094 train accuracy 0.871622\n",
      "val Cost 1.03924 val accuracy 0.6724\n",
      "Epoch 97, CIFAR-10 Batch 4:  train Cost 0.438492 train accuracy 0.868243\n",
      "val Cost 1.02417 val accuracy 0.6626\n",
      "Epoch 97, CIFAR-10 Batch 5:  train Cost 0.45096 train accuracy 0.868243\n",
      "val Cost 0.963216 val accuracy 0.6848\n",
      "Epoch 98, CIFAR-10 Batch 1:  train Cost 0.461106 train accuracy 0.841216\n",
      "val Cost 0.964954 val accuracy 0.6758\n",
      "Epoch 98, CIFAR-10 Batch 2:  train Cost 0.436713 train accuracy 0.858108\n",
      "val Cost 0.938411 val accuracy 0.6928\n",
      "Epoch 98, CIFAR-10 Batch 3:  train Cost 0.410812 train accuracy 0.878378\n",
      "val Cost 1.00528 val accuracy 0.6768\n",
      "Epoch 98, CIFAR-10 Batch 4:  train Cost 0.42983 train accuracy 0.85473\n",
      "val Cost 0.966436 val accuracy 0.6814\n",
      "Epoch 98, CIFAR-10 Batch 5:  train Cost 0.415644 train accuracy 0.875\n",
      "val Cost 0.961293 val accuracy 0.6804\n",
      "Epoch 99, CIFAR-10 Batch 1:  train Cost 0.458752 train accuracy 0.834459\n",
      "val Cost 0.943189 val accuracy 0.6846\n",
      "Epoch 99, CIFAR-10 Batch 2:  train Cost 0.441919 train accuracy 0.864865\n",
      "val Cost 0.940983 val accuracy 0.694\n",
      "Epoch 99, CIFAR-10 Batch 3:  train Cost 0.423646 train accuracy 0.885135\n",
      "val Cost 1.00211 val accuracy 0.6794\n",
      "Epoch 99, CIFAR-10 Batch 4:  train Cost 0.403906 train accuracy 0.878378\n",
      "val Cost 0.959318 val accuracy 0.685\n",
      "Epoch 99, CIFAR-10 Batch 5:  train Cost 0.433732 train accuracy 0.868243\n",
      "val Cost 0.958468 val accuracy 0.685\n",
      "Epoch 100, CIFAR-10 Batch 1:  train Cost 0.440138 train accuracy 0.851351\n",
      "val Cost 0.947539 val accuracy 0.6816\n",
      "Epoch 100, CIFAR-10 Batch 2:  train Cost 0.455544 train accuracy 0.861486\n",
      "val Cost 0.960867 val accuracy 0.6818\n",
      "Epoch 100, CIFAR-10 Batch 3:  train Cost 0.397642 train accuracy 0.891892\n",
      "val Cost 0.961905 val accuracy 0.6836\n",
      "Epoch 100, CIFAR-10 Batch 4:  train Cost 0.380036 train accuracy 0.875\n",
      "val Cost 0.937876 val accuracy 0.69\n",
      "Epoch 100, CIFAR-10 Batch 5:  train Cost 0.422365 train accuracy 0.878378\n",
      "val Cost 0.967653 val accuracy 0.6812\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6736902564764022\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgXHIiIiIiKRgmMRERERkej/A06XO6rgpRytAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3d114ef60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
